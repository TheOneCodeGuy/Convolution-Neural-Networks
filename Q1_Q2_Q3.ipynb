{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Q1_Q2_Q3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7_qTq010ijWD"
      },
      "source": [
        "### Run this cell only once (the first ever time you run) to process data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v8PCDpQkAHwN",
        "colab": {}
      },
      "source": [
        "# # from google.colab import drive\n",
        "# # drive.mount('/content/drive')\n",
        "\n",
        "# # For extracting\n",
        "# !pip install pyunpack\n",
        "# !pip install patool\n",
        "\n",
        "# from pyunpack import Archive\n",
        "# Archive('CUB_200_2011.tgz').extractall('Assignment3_Data')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fjjzGKEwFmsN",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Z5GNnFsDPna",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "30ffb66f-8c0d-4bb2-a05a-42f850598a0d"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "from io import StringIO\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
        "import pickle\n",
        "import torchvision.models as models\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J6JD3ycyPuYi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "09b5598c-0150-4d14-ba1c-969a6fb97966"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DP8iACDGDSWD",
        "colab": {}
      },
      "source": [
        "class DatasetClass(Dataset):\n",
        "    \n",
        "    def __init__(self, directory, img_size):\n",
        "        \n",
        "        self.directory = directory\n",
        "        self.classes = ['026.Bronzed_Cowbird',\t'084.Red_legged_Kittiwake',\t'131.Vesper_Sparrow',\t'085.Horned_Lark',\t'015.Lazuli_Bunting',\t'041.Scissor_tailed_Flycatcher',\t'114.Black_throated_Sparrow']\n",
        "        print('Number of Classes =', len(self.classes))\n",
        "        self.files = []\n",
        "        for class_name in self.classes:\n",
        "            images = os.listdir(directory + '/' + class_name)\n",
        "            images = [class_name + '/' + image for image in images]\n",
        "            self.files.extend(images)\n",
        "        \n",
        "        self.img_size = img_size\n",
        "        self.size = len(self.files)\n",
        "        \n",
        "    def __getitem__(self, idx):     \n",
        "        \n",
        "        image_name = self.files[idx]\n",
        "        y = self.classes.index(re.split('/', image_name)[0])\n",
        "        img = Image.open(self.directory + '/' + image_name).convert(mode='RGB').resize(self.img_size)\n",
        "        \n",
        "        trans = transforms.ToTensor()\n",
        "        # return trans(img), torch.Tensor(y, dtype=torch.long)\n",
        "        \n",
        "        return trans(img), y        # Multiplying by pixel value\n",
        "      \n",
        "    def __len__(self):\n",
        "        \n",
        "        return self.size"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TKVGwDDOKAsM",
        "colab": {}
      },
      "source": [
        "def train_test_loader(directory, img_size, train_fraction=0.7, cv_fraction=0.2, num_workers=0, batch_size=32):\n",
        "\n",
        "    dataset = DatasetClass(directory, img_size)\n",
        "    \n",
        "    N = dataset.size\n",
        "    train_size = int(N*train_fraction)\n",
        "    cv_size = int(N*cv_fraction)\n",
        "    test_size = N - train_size - cv_size\n",
        "\n",
        "    train_data, cv_data, test_data = torch.utils.data.random_split(dataset, [train_size, cv_size, test_size])\n",
        "\n",
        "    trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    cvloader = DataLoader(cv_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    testloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    \n",
        "    return trainloader, cvloader, testloader, train_size, cv_size, test_size"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fT66UkOpUDtv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f94cfbfb-27a4-4e21-8781-b7e2e0c16aad"
      },
      "source": [
        "# trainloader, cvloader, testloader, train_size, cv_size, test_size = train_test_loader('D:/_SEM8/DL/Assignment 3/Assignment3_Data/CUB_200_2011/images/', (224, 224))\n",
        "trainloader, cvloader, testloader, train_size, cv_size, test_size = train_test_loader('/content/drive/My Drive/Assignment3_Data/CUB_200_2011/images', (224, 224), batch_size=32)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "  # trainloader, cvloader, testloader, train_size, cv_size, test_size = train_test_loader('Assignment3_Data/CUB_200_2011/images/', (224, 224))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Classes = 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "81_AjruYX-U6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2236bc5e-f3ff-42e6-8a9e-90c814091e40"
      },
      "source": [
        "RGB_mean = torch.zeros(3)\n",
        "i = 0\n",
        "for X, y in trainloader:\n",
        "    i += 1\n",
        "    RGB_mean += (X.sum(0).sum(1).sum(1)/(X.shape[2]*X.shape[2]))/train_size\n",
        "    print(i, '/', len(trainloader), end=', ')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 / 9, 2 / 9, 3 / 9, 4 / 9, 5 / 9, 6 / 9, 7 / 9, 8 / 9, 9 / 9, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vc8N6oxxiwx0"
      },
      "source": [
        "### Question 1. a\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xzGC9uLa9xum",
        "colab": {}
      },
      "source": [
        "class VGGNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, RGB_mean, num_classes):\n",
        "        super(VGGNet, self).__init__()\n",
        "        \n",
        "        self.RGB_mean = RGB_mean.to(device)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.c11 = nn.Conv2d(3, 64, 3, stride=1, padding=1)\n",
        "        self.c12 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
        "        self.p1 = nn.MaxPool2d(2, stride=2)\n",
        "\n",
        "        self.c21 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
        "        self.c22 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
        "        self.p2 = nn.MaxPool2d(2, stride=2)\n",
        "\n",
        "        self.c31 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
        "        self.c32 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
        "        self.c33 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
        "        self.p3 = nn.MaxPool2d(2, stride=2)\n",
        "\n",
        "        self.c41 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n",
        "        self.c42 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.c43 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.p4 = nn.MaxPool2d(2, stride=2)\n",
        "\n",
        "        self.c51 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.c52 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.c53 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.p5 = nn.MaxPool2d(2, stride=2)\n",
        "\n",
        "        self.flat = nn.Flatten(1, -1)\n",
        "        self.fc1 = nn.Linear(7*7*512, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.out = nn.Linear(4096, self.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x - self.RGB_mean[None, :, None, None]\n",
        "        x = self.p1(F.relu(self.c12(F.relu(self.c11(x)))))\n",
        "        x = self.p1(F.relu(self.c22(F.relu(self.c21(x)))))\n",
        "        x = self.p3(F.relu(self.c33(F.relu(self.c32(F.relu(self.c31(x)))))))\n",
        "        x = self.p4(F.relu(self.c43(F.relu(self.c42(F.relu(self.c41(x)))))))\n",
        "        x = self.p5(F.relu(self.c53(F.relu(self.c52(F.relu(self.c51(x)))))))\n",
        "        x = F.relu(self.fc2(F.relu(self.fc1(self.flat(x)))))\n",
        "        Z = self.out(x)\n",
        "\n",
        "\n",
        "        return Z"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EwuOkncsWXVy",
        "colab": {}
      },
      "source": [
        "VGG_model = VGGNet(RGB_mean, 7)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(VGG_model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZHF1Sn-Dhl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vgg16 = models.vgg16(pretrained=True)\n",
        "# pickle.dump(vgg16, open('/content/drive/My Drive/vgg_init.sav', 'wb'))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bms2NosWGTC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg16 = pickle.load(open('/content/drive/My Drive/vgg_init.sav', 'rb'))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX8N4KctEfc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = list(vgg16.parameters())\n",
        "\n",
        "with torch.no_grad():\n",
        "    \n",
        "    VGG_model.c11.weight = params[0]\n",
        "    VGG_model.c11.bias = params[1]\n",
        "    VGG_model.c12.weight = params[2]\n",
        "    VGG_model.c12.bias = params[3]\n",
        "    \n",
        "    VGG_model.c21.weight = params[4]\n",
        "    VGG_model.c21.bias = params[5]\n",
        "    VGG_model.c22.weight = params[6]\n",
        "    VGG_model.c22.bias = params[7]\n",
        "\n",
        "    VGG_model.c31.weight = params[8]\n",
        "    VGG_model.c31.bias = params[9]\n",
        "    VGG_model.c32.weight = params[10]\n",
        "    VGG_model.c32.bias = params[11]\n",
        "    VGG_model.c33.weight = params[12]\n",
        "    VGG_model.c33.bias = params[13]\n",
        "\n",
        "    VGG_model.c41.weight = params[14]\n",
        "    VGG_model.c41.bias = params[15]\n",
        "    VGG_model.c42.weight = params[16]\n",
        "    VGG_model.c42.bias = params[17]\n",
        "    VGG_model.c43.weight = params[18]\n",
        "    VGG_model.c43.bias = params[19]\n",
        "\n",
        "    VGG_model.c51.weight = params[20]\n",
        "    VGG_model.c51.bias = params[21]\n",
        "    VGG_model.c52.weight = params[22]\n",
        "    VGG_model.c52.bias = params[23]\n",
        "    VGG_model.c53.weight = params[24]\n",
        "    VGG_model.c53.bias = params[25]\n",
        "\n",
        "    VGG_model.fc1.weight = params[26]\n",
        "    VGG_model.fc1.bias = params[27]\n",
        "\n",
        "    VGG_model.fc2.weight = params[28]\n",
        "    VGG_model.fc2.bias = params[29]\n",
        "\n",
        "VGG_model = VGG_model.to(device)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VUTJaWtqSqQj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a0d51b7e-e628-4667-d04c-a7749d160401"
      },
      "source": [
        "old_loss = np.inf\n",
        "\n",
        "max_epoch = 100\n",
        "losses = []\n",
        "for epoch in range(max_epoch):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for data in trainloader:\n",
        "        \n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward\n",
        "        y_hat = VGG_model(X)\n",
        "        \n",
        "        # Calculate Loss (Cross Entropy)\n",
        "        loss = criterion(y_hat, y)\n",
        "        \n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update Parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()*len(X)/train_size\n",
        "    \n",
        "    print('Epoch', epoch+1, ': Loss =', running_loss, abs(running_loss-old_loss)/running_loss)\n",
        "    losses.append(running_loss)\n",
        "\n",
        "    if abs(running_loss-old_loss)/running_loss < 1e-2 and running_loss<0.05:\n",
        "        print('Converged')\n",
        "        break\n",
        "    \n",
        "    old_loss = running_loss\n",
        "\n",
        "print('Finished Training')\n",
        "plt.plot(losses)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Iter Number')\n",
        "plt.title('Convergence monitor plot')\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 : Loss = 1.8460866445448338 inf\n",
            "Epoch 2 : Loss = 1.0807606449110583 0.70813644375139\n",
            "Epoch 3 : Loss = 0.6524373722824071 0.6564971456651195\n",
            "Epoch 4 : Loss = 0.4732703884304193 0.3785721402223944\n",
            "Epoch 5 : Loss = 0.38688288800392423 0.22329108654094523\n",
            "Epoch 6 : Loss = 0.32553923711544136 0.18843704197392783\n",
            "Epoch 7 : Loss = 0.2902372030846333 0.12163166422367284\n",
            "Epoch 8 : Loss = 0.26113165608681865 0.11145928239408057\n",
            "Epoch 9 : Loss = 0.24088090819141178 0.08406954310930682\n",
            "Epoch 10 : Loss = 0.2186906454558987 0.10146873310128844\n",
            "Epoch 11 : Loss = 0.20416518692770902 0.0711456186373873\n",
            "Epoch 12 : Loss = 0.19065284765348203 0.07087404904009678\n",
            "Epoch 13 : Loss = 0.17924669883392416 0.06363380131271432\n",
            "Epoch 14 : Loss = 0.16815741092797357 0.06594587681122446\n",
            "Epoch 15 : Loss = 0.15963291021174253 0.05340064717810287\n",
            "Epoch 16 : Loss = 0.150935411323446 0.057623978442396605\n",
            "Epoch 17 : Loss = 0.14390422232475014 0.04886019940977488\n",
            "Epoch 18 : Loss = 0.13576473467026023 0.05995288595568472\n",
            "Epoch 19 : Loss = 0.13111437348330893 0.03546797397878964\n",
            "Epoch 20 : Loss = 0.12481511732115563 0.05046869559834637\n",
            "Epoch 21 : Loss = 0.12068857107220626 0.03419169033396307\n",
            "Epoch 22 : Loss = 0.11453054061334723 0.053767583963900197\n",
            "Epoch 23 : Loss = 0.10944787412881851 0.046439152199032205\n",
            "Epoch 24 : Loss = 0.1047580350670665 0.04476829924071747\n",
            "Epoch 25 : Loss = 0.1018730155630394 0.028319761499961092\n",
            "Epoch 26 : Loss = 0.09735744597085261 0.046381348104989235\n",
            "Epoch 27 : Loss = 0.09501377973406988 0.024666593028319857\n",
            "Epoch 28 : Loss = 0.09170677777678293 0.03606060574210001\n",
            "Epoch 29 : Loss = 0.08798246116796021 0.04233021626563648\n",
            "Epoch 30 : Loss = 0.08541762031848422 0.03002706982368322\n",
            "Epoch 31 : Loss = 0.08222835934120604 0.038785414215117324\n",
            "Epoch 32 : Loss = 0.07978247544802856 0.03065690653796216\n",
            "Epoch 33 : Loss = 0.07776238979570542 0.025977669380149476\n",
            "Epoch 34 : Loss = 0.07549703074456923 0.03000593571422201\n",
            "Epoch 35 : Loss = 0.07324027630925595 0.030813024595704338\n",
            "Epoch 36 : Loss = 0.07152482140354993 0.0239840501806674\n",
            "Epoch 37 : Loss = 0.06925546142957352 0.03276795688213187\n",
            "Epoch 38 : Loss = 0.06776119784195664 0.02205190632996245\n",
            "Epoch 39 : Loss = 0.06567215021271323 0.031810251719746385\n",
            "Epoch 40 : Loss = 0.0639694510125117 0.026617380222139303\n",
            "Epoch 41 : Loss = 0.06277076733859989 0.019096208708837877\n",
            "Epoch 42 : Loss = 0.061094235992286264 0.027441727015381678\n",
            "Epoch 43 : Loss = 0.05988659718778076 0.020165427010635145\n",
            "Epoch 44 : Loss = 0.0582114722618658 0.028776542850168268\n",
            "Epoch 45 : Loss = 0.057171784211534245 0.01818533503318235\n",
            "Epoch 46 : Loss = 0.05600896389434562 0.020761325265401257\n",
            "Epoch 47 : Loss = 0.054567586474821544 0.02641453493987961\n",
            "Epoch 48 : Loss = 0.05360048113695835 0.018042848074293828\n",
            "Epoch 49 : Loss = 0.05233452714434485 0.024189651874026757\n",
            "Epoch 50 : Loss = 0.05155399158752754 0.015140157585899545\n",
            "Epoch 51 : Loss = 0.05021944300690179 0.026574340548586796\n",
            "Epoch 52 : Loss = 0.04925371824604709 0.01960714429782578\n",
            "Epoch 53 : Loss = 0.048683343721287596 0.011716009648492686\n",
            "Epoch 54 : Loss = 0.047545176051323426 0.023938657178081663\n",
            "Epoch 55 : Loss = 0.04658432125733704 0.02062614132936525\n",
            "Epoch 56 : Loss = 0.0456821639285478 0.019748568176418225\n",
            "Epoch 57 : Loss = 0.044909997320756685 0.017193646267135952\n",
            "Epoch 58 : Loss = 0.044255546485609296 0.014787995790769396\n",
            "Epoch 59 : Loss = 0.04355728190119673 0.016030949451723692\n",
            "Epoch 60 : Loss = 0.04270058528938775 0.02006287749929027\n",
            "Epoch 61 : Loss = 0.041986414844669945 0.017009560053171967\n",
            "Epoch 62 : Loss = 0.04130131683802355 0.016587800561740605\n",
            "Epoch 63 : Loss = 0.04073164322598471 0.013986020865355617\n",
            "Epoch 64 : Loss = 0.040063708060086814 0.016671825905284085\n",
            "Epoch 65 : Loss = 0.039332595111912554 0.018587966191756063\n",
            "Epoch 66 : Loss = 0.039036850000403904 0.007576049591747018\n",
            "Converged\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5xcdX3/8dd7d2Z3k73kQi5AgiRc5KaAGOMF1HjDaCtYa1vQWrQqP620Wq0ttv7UYu/Wiv6qVX6VUvtTKGKpqUIBQcUbmgUh3IoECCYhIRtyv+xt9vP743wnOdnMJrvJzs5s5v18POYxc67zmc1m3nu+33POVxGBmZnZcE21LsDMzOqTA8LMzCpyQJiZWUUOCDMzq8gBYWZmFTkgzMysIgeEWQOQ9FZJt9bw/T8h6f/V6v3t0Dgg7JBIeoukbkk7JK2TdLOk82pdl1UWEV+NiPPL05JC0km1rGkkklZJenWt6zAHhB0CSR8ErgT+CpgLPAv4AnBhLevKk1SodQ1HKv9sG4cDwsZE0jTgCuB9EfEfEbEzIgYi4r8i4sNpnVZJV0p6Kj2ulNSali2RtEbShyRtSEcf70jLXihpvaTm3Pv9mqQV6XWTpMslPSbpGUnXS5qZli1IfxW/U9IvgTskNUv6tKSNkp6QdFlap1D+LJK+nGpYK+kvyu8t6e2Sfijp7yVtTtu/LlfXTEn/kj7fZkn/mVv2q5LulbRF0o8lnXmAn2dI+j1Jj0raLumTkk5M221Ln7Elt/67Ja2UtEnSMknHDtvXe9K+tkj6vCTlP096fWfa5L50BPhbo9z3+yQ9Cjxa4XOUf/6Xpp/JOkl/dIDPfYGkB1Od35N0Wpr/b2R/cPxXqu2PR9qHTYCI8MOPUT+ApcAgUDjAOlcAdwFzgNnAj4FPpmVL0vZXAEXg9cAuYEZa/hjwmty+vg5cnl6/P+13PtAKfAm4Ni1bAATwFaAdmAK8B3gorT8D+E5ap5C2uTHtoz3V+jPgf6VlbwcGgHcDzcB7gacApeXfBv497bcIvDzNfx6wAXhh2u4SYBXQOsLPKoBvAl3AGUAfcDtwAjAt1X9JWveVwEbgnPT5/w9w57B9fQuYTvYl2wMszX2eHw5b96Tc9Gj2fRswE5hS4XOUf/7Xpp/nc9P7vzot/wTw/9LrZwM7gdekn90fAyuBlrR8VXk7P2r8/73WBfgxuR7AW4H1B1nnMeD1uenXAqvS6yXAbnIBk75QX5Re/wVwdXrdmb5Ijk/TDwOvym13TPoSL+S+oE7ILb+D9IWfpl+d1imQNY315b/sgIuB76bXbwdW5pZNTdsend53iBRqwz77P5HCMDfvEVKAVFg/gHNz03cDf5Kb/jRwZXr9ZeDvcss60udfkNvXebnl17M3XN/OgQNiNPt+5QH+zcs//1Nz8/4O+HJ6nQ+I/w1cn1uvCVgLLEnTDog6ebiJycbqGWDWQdqhjwWezE0/mebt2UdEDOamd5F9IQF8DXhTapJ6E3BPRJT3dTxwY2qW2EIWGCWyL/uy1cPqWD3CsuPJ/npdl9vfl8iOJMrWl19ExK70sgM4DtgUEZsrfPbjgQ+V95n2e9ywzz/c07nXuytMl382+/xcI2IH2b/HvEo1s+/P9WBGs+/VwzeqIL/O8H/3kd5rKG03r8K6VkMOCBurn5D95f3GA6zzFNkXZdmz0ryDioiHyL48Xge8hSwwylYDr4uI6blHW0Ssze8i93odWfNS2XHD9tUHzMrtqysizhhFmauBmZKmj7DsL4fVODUirh3Ffg9mn5+rpHbgKLK/vidi36O59XP+ZzzSv/vw91LarvxevsV0nXBA2JhExFbgY8DnJb1R0lRJRUmvk/R3abVrgY9Kmi1pVlp/LOfAf42sv+FlZH0QZV8E/lLS8QBp/wc6c+p64P2S5qUv8z/JfY51wK3ApyV1pQ7wEyW9/GDFpW1vBr4gaUb6/C9Li/8v8J7U4S5J7ZJ+RVLn6D/+iK4F3iHp7HSE9VfATyNi1SHs62myfo7x3vf/Tr8TZwDvIOunGe564FckvUpSEfgQWVj/eITarEYcEDZmEfFp4IPAR8k6IlcDlwHlM3n+AugGVgD3A/ekeaN1LfBy4I6I2Jib/1lgGXCrpO1kHdYvPMB+/i9ZCKwAfg7cRNZBXkrLfwdoIesI3gzcQNa/MBpvI2uj/x+yPpQPAEREN1nH9j+mfa4ka/8/bBHxHbL2+2+QHR2dCFx0iLv7BPCvqRnsN8dx398n+8y3A38fEftdnBcRjwC/TdYRvhF4A/CGiOhPq/w12R8YWw50JpRVX/mMDLMjXjpN9YsRcfxBV7YxkbQAeAIoDutfsknMRxB2xJI0RdLrJRUkzQM+TnZqq5mNggPCjmQC/pysqefnZGc9faymFZlNIm5iMjOzinwEYWZmFR1RN92aNWtWLFiwoNZlmJlNGnfffffGiJhdadkRFRALFiygu7u71mWYmU0akp4caZmbmMzMrCIHhJmZVeSAMDOzihwQZmZWkQPCzMwqckCYmVlFDggzM6vIAQF87vZH+f4vempdhplZXXFAAFfd+Tjff8QBYWaW54AAOloL7OgbqHUZZmZ1xQEBdLYV2NHnMU7MzPIcEEBHW4HtvQ4IM7M8BwRZE5MDwsxsXw4IoKut6CYmM7NhHBCUjyDcSW1mlueAIOuD2OEmJjOzfVQtICRdLWmDpAdGWP5hSfemxwOSSpJmpmWrJN2fllV9BKDOtgI7+0uUhjw+t5lZWTWPIK4Blo60MCI+FRFnR8TZwEeA70fEptwqr0jLF1WxRiBrYgLY2e+jCDOzsqoFRETcCWw66IqZi4Frq1XLwXS2ZQHhM5nMzPaqeR+EpKlkRxrfyM0O4FZJd0u69CDbXyqpW1J3T8+h3S6js60I4H4IM7OcmgcE8AbgR8Oal86LiHOA1wHvk/SykTaOiKsiYlFELJo9e/YhFVBuYvLtNszM9qqHgLiIYc1LEbE2PW8AbgQWV7OAjtTEtM1HEGZme9Q0ICRNA14OfDM3r11SZ/k1cD5Q8Uyo8dJZPoJwQJiZ7VGo1o4lXQssAWZJWgN8HCgCRMQX02q/BtwaETtzm84FbpRUru9rEfHf1aoTcn0QvprazGyPqgVERFw8inWuITsdNj/vceCs6lRVWcees5jcB2FmVlYPfRA1N7XYjOQmJjOzPAcE0NSk7H5MbmIyM9vDAZF0+pbfZmb7cEAkvmGfmdm+HBBJp8eEMDPbhwMi8ZgQZmb7ckAkHW3upDYzy3NAJF3ugzAz24cDIunwWUxmZvtwQCQdrUV2D5QYLA3VuhQzs7rggEjKgwbt7CvVuBIzs/rggEj23vLbZzKZmYEDYo89t/z2mUxmZoADYo/yEYQDwsws44BIPC61mdm+HBBJeVxq90GYmWUcEEmnm5jMzPbhgEj2BISbmMzMgCoGhKSrJW2Q9MAIy5dI2irp3vT4WG7ZUkmPSFop6fJq1Zg3pdhMk/DV1GZmSTWPIK4Blh5knR9ExNnpcQWApGbg88DrgNOBiyWdXsU6Se9LR2vBTUxmZknVAiIi7gQ2HcKmi4GVEfF4RPQD1wEXjmtxI+hsK/oIwswsqXUfxIsl3SfpZklnpHnzgNW5ddakeRVJulRSt6Tunp6ewyqms81jQpiZldUyIO4Bjo+Is4D/A/znoewkIq6KiEURsWj27NmHVZCbmMzM9qpZQETEtojYkV7fBBQlzQLWAsflVp2f5lVdZ5sDwsysrGYBIeloSUqvF6dangGWAydLWiipBbgIWDYRNXW4D8LMbI9CtXYs6VpgCTBL0hrg40ARICK+CLwZeK+kQWA3cFFEBDAo6TLgFqAZuDoiHqxWnXkeNMjMbK+qBUREXHyQ5f8I/OMIy24CbqpGXQeSNTG5k9rMDGp/FlNd6Wwt0DswxIBHlTMzc0Dkdfh2G2Zmezggcjo8aJCZ2R4OiJzymBC+5beZmQNiH76jq5nZXg6IHDcxmZnt5YDI8aBBZmZ7OSByymcxbXMTk5mZAyKvszXrpHYfhJmZA2IfbcUmCk3y1dRmZjgg9iGJjjbfj8nMDBwQ++loLbiJycwMB8R+OtuKbPdZTGZmDojhOls97KiZGTgg9tPhUeXMzAAHxH7cB2FmlnFADNPps5jMzAAHxH462grupDYzo4oBIelqSRskPTDC8rdKWiHpfkk/lnRWbtmqNP9eSd3VqrGSztYC/YND9A2WJvJtzczqTjWPIK4Blh5g+RPAyyPiucAngauGLX9FRJwdEYuqVF9F5TEh3A9hZo2uagEREXcCmw6w/McRsTlN3gXMr1YtY+FbfpuZZeqlD+KdwM256QBulXS3pEsPtKGkSyV1S+ru6ek57ELKd3R1R7WZNbpCrQuQ9AqygDgvN/u8iFgraQ5wm6T/SUck+4mIq0jNU4sWLYrDrafTAWFmBtT4CELSmcA/AxdGxDPl+RGxNj1vAG4EFk9UTXtu+e0mJjNrcDULCEnPAv4DeFtE/CI3v11SZ/k1cD5Q8UyoaujYM6qcb7dhZo2tak1Mkq4FlgCzJK0BPg4UASLii8DHgKOAL0gCGExnLM0FbkzzCsDXIuK/q1XncHuGHXUTk5k1uKoFRERcfJDl7wLeVWH+48BZ+28xMcpnMXnYUTNrdPVyFlPdaC00UWyW+yDMrOE5IIaR5Bv2mZnhgKios63oMSHMrOE5ICroaPWYEGZmDogKOnzLbzMzB0QlXQ4IMzMHRCVuYjIzc0BU5HGpzcwcEBWVz2KKOOx7/5mZTVoOiAo6WgsMlIK+waFal2JmVjMOiAr23I/JzUxm1sAcEBV4TAgzMwdERR1pTAhfTW1mjcwBUcHM9iwgNu3sr3ElZma144CoYE5nGwAbtvXVuBIzs9pxQFQwu7MVgA3be2tciZlZ7TggKmgrNjNtSpGnfQRhZg3MATGCuV2tPoIws4bmgBjBnM42H0GYWUOrakBIulrSBkkPjLBckj4naaWkFZLOyS27RNKj6XFJNeusZE5XKz3bHRBm1rhGFRCS2iU1pdfPlnSBpOIoNr0GWHqA5a8DTk6PS4F/Su8xE/g48EJgMfBxSTNGU+t4mdvVxobtvQwN+X5MZtaYRnsEcSfQJmkecCvwNrIv/wOKiDuBTQdY5ULgK5G5C5gu6RjgtcBtEbEpIjYDt3HgoBl3czpbGSgFm3f5Wggza0yjDQhFxC7gTcAXIuI3gDPG4f3nAatz02vSvJHm71+YdKmkbkndPT0941BSZm5XuhbCzUxm1qBGHRCSXgy8Ffh2mtdcnZLGJiKuiohFEbFo9uzZ47bfOelaiKe3+UwmM2tMow2IDwAfAW6MiAclnQB8dxzefy1wXG56fpo30vwJ4yMIM2t0owqIiPh+RFwQEX+bOqs3RsQfjMP7LwN+J53N9CJga0SsA24Bzpc0I3VOn5/mTZg9V1P7CMLMGlRhNCtJ+hrwHqAELAe6JH02Ij51kO2uBZYAsyStITszqQgQEV8EbgJeD6wEdgHvSMs2Sfpkei+AKyLiQJ3d485XU5tZoxtVQACnR8Q2SW8FbgYuB+4GDhgQEXHxQZYH8L4Rll0NXD3K+qrCV1ObWSMbbR9EMV338EZgWUQMAEf8BQK+mtrMGtloA+JLwCqgHbhT0vHAtmoVVS98NbWZNbLRdlJ/LiLmRcTr00VtTwKvqHJtNTenM7uaOmsJMzNrLKO91cY0Sf9QviBN0qfJjiaOaHO7yldTe+hRM2s8o21iuhrYDvxmemwD/qVaRdWL8rUQvljOzBrRaM9iOjEifj03/eeS7q1GQfUkfzX1acd01bgaM7OJNdojiN2SzitPSDoX2F2dkuqHr6Y2s0Y22iOI9wBfkTQtTW8GJnyMhonmq6nNrJGNKiAi4j7gLEldaXqbpA8AK6pZXK2Vr6b2EYSZNaIxjSgXEdsionz9wwerUE/dmdvV6k5qM2tIhzPkqMatijrmq6nNrFEdTkA0xNVjvprazBrVAfsgJG2nchAImFKViupM/mpqqSEOmszMgIMERER0TlQh9Sp/NfXM9pZal2NmNmEOp4mpIczp9NXUZtaYHBAHMbcrXQvhfggzazAOiIPw/ZjMrFE5IA7CV1ObWaOqakBIWirpEUkrJV1eYflnJN2bHr+QtCW3rJRbtqyadR6Ir6Y2s0Y12nsxjZmkZuDzwGuANcByScsi4qHyOhHxh7n1fx94Xm4XuyPi7GrVNxZzOn01tZk1nmoeQSwGVkbE4xHRD1wHXHiA9S8Grq1iPYdsblebjyDMrOFUMyDmAatz02vSvP2kMa4XAnfkZrel0evukvTGkd5E0qXlke56enrGo+79zOlsZYNvt2FmDaZeOqkvAm6IiFJu3vERsQh4C3ClpBMrbRgRV0XEoohYNHv27KoUN6fLY1ObWeOpZkCsBY7LTc9P8yq5iGHNSxGxNj0/DnyPffsnJpTHpjazRlTNgFgOnCxpoaQWshDY72wkSacCM4Cf5ObNkNSaXs8CzgUeGr7tRPHV1GbWiKoWEBExCFwG3AI8DFwfEQ9KukLSBblVLwKui33bb04DuiXdB3wX+Jv82U8TzVdTm1kjqtpprgARcRNw07B5Hxs2/YkK2/0YeG41axsLH0GYWSOql07qujYnHUF4XAgzayQOiFEoX03tIwgzayQOiFHy1dRm1mgcEKPkq6nNrNE4IEZpTlcr67f6CMLMGocDYpROPbqTdVt72bDdIWFmjcEBMUovWDATgOVPbK5xJWZmE8MBMUrPmTeNKcVmlq/aVOtSzMwmhANilIrNTTzvWdP52RMOCDNrDA6IMVi8cCYPr9/Gtl7ftM/MjnwOiDFYvGAmEXD3k+6HMLMjnwNiDJ73rBkUmsRyNzOZWQNwQIzBlJZmnjNvmvshzKwhOCDGaPHCmaxYs5XegdLBVzYzm8QcEGP0ggUz6S8Ncd/qLbUuxcysqhwQY7To+BkAvh7CzI54DogxmtHewilzO/nZKp/JZGZHNgfEIXjBwhnc8+RmBktDtS7FzKxqqhoQkpZKekTSSkmXV1j+dkk9ku5Nj3flll0i6dH0uKSadY7VCxbMZEffIA+v217rUszMqqZqY1JLagY+D7wGWAMsl7QsIh4atuq/R8Rlw7adCXwcWAQEcHfati7adRYvzG7c97NVm3ju/Gk1rsbMrDqqeQSxGFgZEY9HRD9wHXDhKLd9LXBbRGxKoXAbsLRKdY7ZMdOmMH/GFF8wZ2ZHtGoGxDxgdW56TZo33K9LWiHpBknHjXFbJF0qqVtSd09Pz3jUPSqLF8xk+apNRMSEvaeZ2USqdSf1fwELIuJMsqOEfx3rDiLiqohYFBGLZs+ePe4FjuQFC2fyzM5+Ht+4c8Le08xsIlUzINYCx+Wm56d5e0TEMxFRHuj5n4Hnj3bbWisPIOTbbpjZkaqaAbEcOFnSQkktwEXAsvwKko7JTV4APJxe3wKcL2mGpBnA+Wle3ThxdjtzOlu56f51tS7FzKwqqhYQETEIXEb2xf4wcH1EPCjpCkkXpNX+QNKDku4D/gB4e9p2E/BJspBZDlyR5tUNSbzj3IX84NGN/PyXdXFylZnZuNKR1Mm6aNGi6O7unrD329k3yHl/ewdnHTeda96xeMLe18xsvEi6OyIWVVpW607qSa29tcC7XnoC33ukh3t98z4zO8I4IA7TJS9ZwPSpRT53+6O1LsXMbFw5IA5TR2uBd523kDv+ZwMr1vgowsyOHA6IcXDJSxYwbYqPIszsyOKAGAedbUXeed5CvvPwBh5Yu7XW5ZiZjQsHxDi55CUL6Gwr8FkfRZjZEcIBMU6mTSnyu+cu5LaHnvZ1EWZ2RHBAjKN3vnQhx0xr40Nfv4/d/aVal2NmdlgcEOOoq63I3//GWTzes5O/vvnhg29gZlbHHBDj7NyTZvG75y7kKz95ku89sqHW5ZiZHTIHRBX88dJTePbcDj58wwo27+yvdTlmZofEAVEFbcVmPvNbZ7NlVz9/euP9HlTIzCYlB0SVnHHsND74mlO4+YH1/Mc9dTWUhZnZqDggqujSl53A4gUz+bP/vJ9vrXiq1uWYmY2JA6KKmpvEF377HM44dhqXfe3n/MOtjzA05OYmM5scHBBVNqujla+9+4X8xvPn87k7VvJ7X72HnX2DtS7LzOygHBAToLXQzN+9+Uw++iuncetD6/n1f/oxqzftqnVZZmYH5ICYIJJ410tP4Oq3v4C1W3bz+s/+gK93r/YZTmZWtxwQE2zJKXP49u+/lNOO6eLDN6zg3V/pZsP23lqXZWa2n6oGhKSlkh6RtFLS5RWWf1DSQ5JWSLpd0vG5ZSVJ96bHsmrWOdGeddRUrrv0RXz0V07jzkc3cv5n7uRbK57y0YSZ1RVV60tJUjPwC+A1wBpgOXBxRDyUW+cVwE8jYpek9wJLIuK30rIdEdExlvdctGhRdHd3j9tnmAgrN2znQ9ffx31rtvL842fw3pefyKtOm4OkWpdmZg1A0t0RsajSsmoeQSwGVkbE4xHRD1wHXJhfISK+GxHl3tq7gPlVrKcunTSnk2+89yV88sIzWL+1l3d9pZulV/6Ab967lsHSUK3LM7MGVs2AmAeszk2vSfNG8k7g5tx0m6RuSXdJeuNIG0m6NK3X3dPTc3gV10ihuYm3vXgB3/vwEv7hN89iKIL3X3cvr/z09/nG3Wso+doJM6uBuuiklvTbwCLgU7nZx6fDnrcAV0o6sdK2EXFVRCyKiEWzZ8+egGqrp9jcxJvOmc8tH3gZV73t+XS2FfjQ1+/jtVfeyU33r/NFdmY2oaoZEGuB43LT89O8fUh6NfBnwAUR0VeeHxFr0/PjwPeA51Wx1rrS1CTOP+No/uuy8/jCW88B4Pe+eg9v+Mcfsuy+p9jV7wvtzKz6qtlJXSDrpH4VWTAsB94SEQ/m1nkecAOwNCIezc2fAeyKiD5Js4CfABfmO7grmYyd1KNRGgq+ee9arvzOo/xy0y6mFJt55WlzeMOZx7DklDm0FZtrXaKZTVIH6qQuVOtNI2JQ0mXALUAzcHVEPCjpCqA7IpaRNSl1AF9PZ+38MiIuAE4DviRpiOwo528OFg5HsuYm8aZz5nPh2fNYvmoT31rxFDffv55vr1hHe0szS06dw2vPOJpXnDKbzrZircs1syNE1Y4gauFIPYKoZLA0xF2Pb+Lb9z/FbQ89zcYd/bQ0N/GSk47iNafP5WUnz+a4mVNrXaaZ1bkDHUE4II4ApaHgnl9u5pYH1nPLQ+tZvWk3AM+aOZVzT5rFeSfN4iUnHsWM9pYaV2pm9cYB0UAigsd6dvDDRzfyw5XPcNfjz7CjbxAJTju6i3NPOoqXnDiLFyycSUdr1VoYzWyScEA0sIHSECvWbOFHK5/hx49t5J4nt9BfGqK5STx7bifPndfFc+dN4znzpnHaMV3u8DZrMA4I26N3oMTdT27mJ489w31rtvDA2q1s3jUAQCGFxpnzp/Hc+dM4c950nn10B60Fh4bZkcoBYSOKCNZu2c0Da7eyYs1W7l+bPbak0GhuEscfNZWT53Rw8pxOTp7bwalHd3Hi7HYKzXVxnaWZHYaanOZqk4Mk5s+YyvwZU1n6nGOALDTWbN7NfWu28Mj67Tz69A4e3bCd7zy8Yc9tP1oLTZx6dCenH9vFqUd3MW/6FI6e1sYx09qY2d7imw2aHQF8BGGj1j84xBMbd/LQuq089NQ2HkyPrbsH9lmvpbmJeTOmsOCoqSyc1cHCWdnzMdPbmNPZSkdrwQFiVid8BGHjoqXQxClHd3LK0Z38WrrxSUTQs72PdVt7Wbe1l/Vbd7Nuay9rNu/m8Y07uevxTeweKO2zn7ZiE3M625jb1cqzZrazcNZUFsxqZ8FR7Rx/1FRf7GdWJxwQdlgkMaerjTldbZx13P7LI4Knt/XxxMadPL2tlw3be9mwrY8N2/tYv62XH67s4Rv39O2zTWdbgXnTpzBv+hSOnT6FY6a3cey0vU1Yc7vafLaV2QRwQFhVSeLoaW0cPa1txHV29Q+yauMunnxmJ09u2sVTW3bz1JbdrN3Sy/JVm9jWu//NCduKTUyf0sK0KUWmTSkys72FuV2tzJ3WxtFd2WNGewsdrYXs0Vag6E51szFxQFjNTW0pcPqxXZx+bFfF5Tv7Blm/rZf1qRnr6W29bN09wJZd/WzdPcDW3QM81rODHz22ke0VwqSstdDEjKktzGhvYWZ7kRlTW5jZ3sJR7a3M6kzPHS1Mn9pCV1uBzrYibcUm95dYw3JAWN1rby1w4uwOTpx98BFod/UPsn5rL+u39bJt9wDbewfZ3jvIjr5BtvcOsHlXFiybdvbz1JZtPLOjr+IRSlmxWXS2FZk+pcj0qVmoTJ/awoypRTrbinS0FehsK9DVVqCjtcjU1mamtjTT3lJgakszHW0FX0dik5YDwo4oU1sKnDC7gxNGESZl/YNDbNrZz8YdfWzc0cfWFCzbetPz7gG27B5g885+ntray0PrtrFl18B+ne8jKTZrTzNXe0sWKO3lpq/WAlNbCkxpaWJqS4G2YjNTis20tzZn67Vk23W0ZsvaCs20FptoLfjIxqrPAWENr6XQdNB+kkoGS0PpyCR77OwfZGffILv6S+zsy17v7C+xo2+QHb3Z9PY0f9POfn75zC629w2yu7/Erv5BxjpgYDlI2lPIdLQ2M6WlwJRiE1OKzUxpaaat2ExLoYnW5qbsOQVMOYjK67UW9i7Pnpv2WeYwakwOCLNDVGhuYnpqcjpcEUF/aYje/iF2DQyysy8Llp0pgHb1D9I7METvQInewRK9A0Ps7h9kR18WLjv7sma0rbsHeHprid0D2aO3v0RfaYj+waHDqm9KMQuWluYmiilsis1Kz9n8crC0FrNQyR/xZOuIQm771uamPftsLTZRaGqi0JStU2gWxaYm2op7Q60cYoWm7L0dWtXngDCrA5KyL8JCM9MY/+tAIoKBUtCXwqV3oERvCpFd/SX6B4foG8yCpL9Uoi+ts3tgKAuagRK7+0sMDg3RP5iFWf9giYFSpG2G2N47yMbBIfoGs+3L79WX1htvxWbtCZWmJtGcHoUm7QmhltxzsZDNzx7Z60JTEy2Fva8Lzdn25aAq7wGZRQYAAAhrSURBVG/Pc3PTPtOFNF1+z0KqqdicrVMcto/8dNPwZ+1dr17CzwFh1gAk0VLI/uLvHFtL2riICAaHgoHSEAODQV9p31DqGxyiNDTEQCkYLEUKoix4+gay596BUrZ9CqXyOqUhGIpsm9JQ1vQ3UBpKIZbte6A0RO/AEDt6B+kvZXUMpn0NlIay2gaz58FURy3lw65S0DRJNAmaJCQ4qr2V69/z4nGvwwFhZlUnac9f7bQAVThKGk8RQWkoC7WhFG6lUjAwNMTQEAwODaUgiz2v9wRNKZsub18Ovvz04FDsWWco8sv2PrJ1hvbML0+XhoIgC8WhyJ47qzS2S1UDQtJS4LNkY1L/c0T8zbDlrcBXgOcDzwC/FRGr0rKPAO8ESsAfRMQt1azVzKxMUtZc1OBnKFft0lJJzcDngdcBpwMXSzp92GrvBDZHxEnAZ4C/TdueDlwEnAEsBb6Q9mdmZhOkmvceWAysjIjHI6IfuA64cNg6FwL/ml7fALxKWe/MhcB1EdEXEU8AK9P+zMxsglQzIOYBq3PTa9K8iutExCCwFThqlNsCIOlSSd2Sunt6esapdDMzm/R3L4uIqyJiUUQsmj17dq3LMTM7YlQzINYC+RtAz0/zKq4jqQBMI+usHs22ZmZWRdUMiOXAyZIWSmoh63ReNmydZcAl6fWbgTsiG+JuGXCRpFZJC4GTgZ9VsVYzMxumaqe5RsSgpMuAW8hOc706Ih6UdAXQHRHLgC8D/yZpJbCJLERI610PPAQMAu+LiNHdGc3MzMaFx6Q2M2tgBxqT+ogKCEk9wJOHuPksYOM4ljORJmvtk7VucO214trH3/ERUfEMnyMqIA6HpO6RUrTeTdbaJ2vd4NprxbVPrEl/mquZmVWHA8LMzCpyQOx1Va0LOAyTtfbJWje49lpx7RPIfRBmZlaRjyDMzKwiB4SZmVXU8AEhaamkRyStlHR5res5EElXS9og6YHcvJmSbpP0aHqeUcsaRyLpOEnflfSQpAclvT/Nr/v6JbVJ+pmk+1Ltf57mL5T00/S78+/pljJ1R1KzpJ9L+laanix1r5J0v6R7JXWneXX/+wIgabqkGyT9j6SHJb14stSe19ABMcpBjerJNWQDKOVdDtweEScDt6fpejQIfCgiTgdeBLwv/awnQ/19wCsj4izgbGCppBeRDXD1mTTg1WayAbDq0fuBh3PTk6VugFdExNm56wcmw+8LZCNp/ndEnAqcRfbznyy17xURDfsAXgzckpv+CPCRWtd1kJoXAA/kph8BjkmvjwEeqXWNo/wc3wReM9nqB6YC9wAvJLsqtlDpd6leHmR3Qr4deCXwLUCToe5U2ypg1rB5df/7QnZX6idIJwFNptqHPxr6CIIxDExUx+ZGxLr0ej0wt5bFjIakBcDzgJ8ySepPzTT3AhuA24DHgC2RDXQF9fu7cyXwx8BQmj6KyVE3QAC3Srpb0qVp3mT4fVkI9AD/kpr2/llSO5Oj9n00ekAcUSL706Suz1uW1AF8A/hARGzLL6vn+iOiFBFnk/1Fvhg4tcYlHZSkXwU2RMTdta7lEJ0XEeeQNQG/T9LL8gvr+PelAJwD/FNEPA/YybDmpDqufR+NHhBHwsBET0s6BiA9b6hxPSOSVCQLh69GxH+k2ZOmfoCI2AJ8l6xpZnoa6Arq83fnXOACSavIxoR/JVnbeL3XDUBErE3PG4AbyYJ5Mvy+rAHWRMRP0/QNZIExGWrfR6MHxGgGNap3+UGXLiFr2687kkQ2/sfDEfEPuUV1X7+k2ZKmp9dTyPpOHiYLijen1equ9oj4SETMj4gFZL/bd0TEW6nzugEktUvqLL8GzgceYBL8vkTEemC1pFPSrFeRjW1T97Xvp9adILV+AK8HfkHWpvxnta7nILVeC6wDBsj+SnknWZvy7cCjwHeAmbWuc4TazyM7pF4B3Jser58M9QNnAj9PtT8AfCzNP4FspMOVwNeB1lrXeoDPsAT41mSpO9V4X3o8WP6/ORl+X1KdZwPd6XfmP4EZk6X2/MO32jAzs4oavYnJzMxG4IAwM7OKHBBmZlaRA8LMzCpyQJiZWUUOCGs4knak5wWS3jIO+1sl6Ru56TdLuuZw95v29QlJfzQe+zIbKweENbIFwJgCIncF8nDPr7c7ASvj/+N2yPzLY43sb4CXpvEG/jDdkO9TkpZLWiHpfwFIWiLpB5KWkV0RW8mngT8bPnP4EYCkB9KRy4I0VsA1kn4h6auSXi3pR2m8gMW53Zwl6Sdp/rtz+/pwrtbyGBULlI1v8hWyi/ryt5IxG5OR/hoyawSXA38UEb8KkO4YujUiXiCpFfiRpFvTuucAz4mIJ0bY1/XA70k6aQzvfxLwG8Dvkt325S1kV5xfAPwp8Ma03plkY2i0Az+X9G3gOcDJZPcnErAs3czul2n+JRFx1xhqMduPA8Jsr/OBMyWV71M0jezLth/42QHCAaAEfIpsTJGbR/l+T0TE/QCSHiQbTCYk3U/W/FX2zYjYDeyW9F2yUDgv1fvztE5HqvWXwJMOBxsPDgizvQT8fkTcss9MaQnZLZsP5t/IAuKB3LxB9m3Kbcu97su9HspND7Hv/83h98OJVOtfR8SXhtW6YJS1mh2U+yCskW0HOnPTtwDvTbclR9Kz051ERyUiBoDPAH+Ym72KrHkKSeeQDSYzVhcqGxf7KLKb7i1Ptf5uGl8DSfMkzTmEfZuNyEcQ1shWACVJ95GN9/1Zsqade9LtyXvY2w8wWl8GPpqb/gbwO6kJ6adkdw4+lDq/C8wCPhkRTwFPSToN+ElWKjuA3yZr6jIbF76bq5mZVeQmJjMzq8gBYWZmFTkgzMysIgeEmZlV5IAwM7OKHBBmZlaRA8LMzCr6/7ysf9SFfjjhAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuguuhIInaRW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "a1909fab-8721-47a8-ac70-ba7291903fc3"
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    train_loss = 0.0\n",
        "    y_train = []\n",
        "    y_train_pred = []\n",
        "\n",
        "    for data in trainloader:\n",
        "\n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        y_hat = VGG_model(X)\n",
        "        train_loss += criterion(y_hat, y)*len(X)/train_size\n",
        "        \n",
        "        y_train.extend(list(y.detach().cpu().numpy()))\n",
        "        y_train_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
        "\n",
        "print('Train Loss =', train_loss.item())\n",
        "pd.DataFrame(confusion_matrix(y_train, y_train_pred))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Loss = 0.03823063522577286\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>46</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    0   1   2   3   4   5   6\n",
              "0  43   0   0   0   0   0   0\n",
              "1   0  36   0   0   0   0   0\n",
              "2   0   0  46   0   0   0   0\n",
              "3   0   0   0  37   0   0   0\n",
              "4   0   0   0   0  40   0   0\n",
              "5   0   0   0   0   0  42   0\n",
              "6   0   0   0   0   0   0  43"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHtBQ8zvtwUa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b90f3559-c9ef-4f46-9c86-ae944d34cca9"
      },
      "source": [
        "acc_tr = accuracy_score(y_train, y_train_pred)\n",
        "prec_tr = precision_score(y_train, y_train_pred, average='weighted')\n",
        "f1_tr = f1_score(y_train, y_train_pred, average='weighted')\n",
        "\n",
        "print('Train Accuracy =', acc_tr, 'Train Precision =', prec_tr, 'Train F1 =', f1_tr)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy = 1.0 Train Precision = 1.0 Train F1 = 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ok3viz8oX4nv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "b346bedf-acf7-4585-f24b-5b43b13fa75b"
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    test_loss = 0.0\n",
        "    y_test = []\n",
        "    y_test_pred = []\n",
        "\n",
        "    for data in testloader:\n",
        "\n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        y_hat = VGG_model(X)      \n",
        "        test_loss += criterion(y_hat, y)*len(X)/test_size\n",
        "        \n",
        "        y_test.extend(list(y.detach().cpu().numpy()))\n",
        "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
        "\n",
        "print('Test Loss =', test_loss.item())\n",
        "pd.DataFrame(confusion_matrix(y_test, y_test_pred))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss = 0.27743086218833923\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0  1  2  3  4  5  6\n",
              "0  8  0  0  0  0  0  0\n",
              "1  0  8  0  0  0  0  0\n",
              "2  0  0  2  1  0  0  0\n",
              "3  0  0  1  8  0  0  1\n",
              "4  0  0  0  1  4  0  0\n",
              "5  0  0  0  0  0  4  0\n",
              "6  0  0  0  0  0  0  4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbtJA8AXtybh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af6312a8-721e-43f4-d25a-f827a7001f87"
      },
      "source": [
        "acc = accuracy_score(y_test, y_test_pred)\n",
        "prec = precision_score(y_test, y_test_pred, average='macro')\n",
        "f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "print('Test Accuracy =', acc, 'Test Precision =', prec, 'Test F1 =', f1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy = 0.9047619047619048 Test Precision = 0.8952380952380953 Test F1 = 0.8920634920634922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r_MY7HHT7GYv"
      },
      "source": [
        "### Question 1. b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZEvZrYhn7JuX",
        "colab": {}
      },
      "source": [
        "class GoogLeNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_classes, RGB_mean):\n",
        "    \n",
        "        super(GoogLeNet, self).__init__()\n",
        "        self.RGB_mean = RGB_mean.to(device)\n",
        "        self.n_classes = n_classes\n",
        "        # Convolution\n",
        "        # 3x224x224\n",
        "        self.c1 = nn.Conv2d(3, 64, 7, stride=2, padding=3)\n",
        "        # 64x112x112\n",
        "        self.mp1 = nn.MaxPool2d(3, stride=2, padding=1)\n",
        "\n",
        "        # Deep Convolution\n",
        "        # 64x56x56\n",
        "        self.c21 = nn.Conv2d(64, 64, 1, stride=1, padding=0)\n",
        "        # 64x56x56\n",
        "        self.c22 = nn.Conv2d(64, 192, 3, stride=1, padding=1)\n",
        "        # 192x56x56\n",
        "        self.mp2 = nn.MaxPool2d(3, stride=2)\n",
        "\n",
        "        # Inception 3a\n",
        "        # 192x28x28\n",
        "        # P1\n",
        "        self.c3a1 = nn.Conv2d(192, 64, 1, stride=1, padding=0)\n",
        "        # P2\n",
        "        self.c3a21 = nn.Conv2d(192, 96, 1, stride=1, padding=0)\n",
        "        self.c3a22 = nn.Conv2d(96, 128, 3, stride=1, padding=1)\n",
        "        # P3\n",
        "        self.c3a31 = nn.Conv2d(192, 16, 1, stride=1, padding=0)\n",
        "        self.c3a32 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n",
        "        # P4\n",
        "        self.mp3a4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
        "        self.c3a4 = nn.Conv2d(192, 32, 1, stride=1, padding=0)\n",
        "\n",
        "        # Inception 3b\n",
        "        # 256x28x28\n",
        "        # P1\n",
        "        self.c3b1 = nn.Conv2d(256, 128, 1, stride=1, padding=0)\n",
        "        # P2\n",
        "        self.c3b21 = nn.Conv2d(256, 128, 1, stride=1, padding=0)\n",
        "        self.c3b22 = nn.Conv2d(128, 192, 3, stride=1, padding=1)\n",
        "        # P3\n",
        "        self.c3b31 = nn.Conv2d(256, 32, 1, stride=1, padding=0)\n",
        "        self.c3b32 = nn.Conv2d(32, 96, 3, stride=1, padding=1)\n",
        "        # P4\n",
        "        self.mp3b4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
        "        self.c3b4 = nn.Conv2d(256, 64, 1, stride=1, padding=0)\n",
        "\n",
        "        # 480x28x28\n",
        "        # MP\n",
        "        self.mp3 = nn.MaxPool2d(3, stride=2, padding=1)\n",
        "\n",
        "        # Inception 4a\n",
        "        # 480x14x14\n",
        "        # P1\n",
        "        self.c4a1 = nn.Conv2d(480, 192, 1, stride=1, padding=0)\n",
        "        # P2\n",
        "        self.c4a21 = nn.Conv2d(480, 96, 1, stride=1, padding=0)\n",
        "        self.c4a22 = nn.Conv2d(96, 208, 3, stride=1, padding=1)\n",
        "        # P3\n",
        "        self.c4a31 = nn.Conv2d(480, 16, 1, stride=1, padding=0)\n",
        "        self.c4a32 = nn.Conv2d(16, 48, 3, stride=1, padding=1)\n",
        "        # P4\n",
        "        self.mp4a4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
        "        self.c4a4 = nn.Conv2d(480, 64, 1, stride=1, padding=0)\n",
        "\n",
        "        # Auxiliary 1\n",
        "        # 512x14x14\n",
        "        self.apa1 = nn.AvgPool2d(5, stride=3, padding=0)\n",
        "        # 512x4x4\n",
        "        self.ca1 = nn.Conv2d(512, 128, 1, stride=1)\n",
        "        # 128x4x4\n",
        "        self.flat1 = nn.Flatten(1, -1)\n",
        "        # (128x4x4)x1\n",
        "        self.fca1 = nn.Linear(2048, 1024)\n",
        "        self.a1drop = nn.Dropout(0.7)\n",
        "        self.a1out = nn.Linear(1024, self.n_classes)\n",
        "\n",
        "        # Inception 4b\n",
        "        # 512x14x14\n",
        "        # P1\n",
        "        self.c4b1 = nn.Conv2d(512, 160, 1, stride=1, padding=0)\n",
        "        # P2\n",
        "        self.c4b21 = nn.Conv2d(512, 112, 1, stride=1, padding=0)\n",
        "        self.c4b22 = nn.Conv2d(112, 224, 3, stride=1, padding=1)\n",
        "        # P3\n",
        "        self.c4b31 = nn.Conv2d(512, 24, 1, stride=1, padding=0)\n",
        "        self.c4b32 = nn.Conv2d(24, 64, 3, stride=1, padding=1)\n",
        "        # P4\n",
        "        self.mp4b4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
        "        self.c4b4 = nn.Conv2d(512, 64, 1, stride=1, padding=0)\n",
        "\n",
        "        # Inception 4c\n",
        "        # 512x14x14\n",
        "        # P1\n",
        "        self.c4c1 = nn.Conv2d(512, 128, 1, stride=1, padding=0)\n",
        "        # P2\n",
        "        self.c4c21 = nn.Conv2d(512, 128, 1, stride=1, padding=0)\n",
        "        self.c4c22 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
        "        # P3\n",
        "        self.c4c31 = nn.Conv2d(512, 24, 1, stride=1, padding=0)\n",
        "        self.c4c32 = nn.Conv2d(24, 64, 3, stride=1, padding=1)\n",
        "        # P4\n",
        "        self.mp4c4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
        "        self.c4c4 = nn.Conv2d(512, 64, 1, stride=1, padding=0)\n",
        "\n",
        "        # Inception 4d\n",
        "        # 512x14x14\n",
        "        # P1\n",
        "        self.c4d1 = nn.Conv2d(512, 112, 1, stride=1, padding=0)\n",
        "        # P2\n",
        "        self.c4d21 = nn.Conv2d(512, 144, 1, stride=1, padding=0)\n",
        "        self.c4d22 = nn.Conv2d(144, 288, 3, stride=1, padding=1)\n",
        "        # P3\n",
        "        self.c4d31 = nn.Conv2d(512, 32, 1, stride=1, padding=0)\n",
        "        self.c4d32 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
        "        # P4\n",
        "        self.mp4d4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
        "        self.c4d4 = nn.Conv2d(512, 64, 1, stride=1, padding=0)\n",
        "\n",
        "        # Auxiliary 2\n",
        "        # 528x14x14\n",
        "        self.apa2 = nn.AvgPool2d(5, stride=3, padding=0)\n",
        "        # 528x4x4\n",
        "        self.ca2 = nn.Conv2d(528, 128, 1, stride=1)\n",
        "        # 128x4x4\n",
        "        self.flat1 = nn.Flatten(1, -1)\n",
        "        # (128x4x4)x1\n",
        "        self.fca2 = nn.Linear(2048, 1024)\n",
        "        self.a2drop = nn.Dropout(0.7)\n",
        "        self.a2out = nn.Linear(1024, self.n_classes)\n",
        "\n",
        "        # Inception 4e\n",
        "        # 528x14x14\n",
        "        # P1\n",
        "        self.c4e1 = nn.Conv2d(528, 256, 1, stride=1, padding=0)\n",
        "        # P2\n",
        "        self.c4e21 = nn.Conv2d(528, 160, 1, stride=1, padding=0)\n",
        "        self.c4e22 = nn.Conv2d(160, 320, 3, stride=1, padding=1)\n",
        "        # P3\n",
        "        self.c4e31 = nn.Conv2d(528, 32, 1, stride=1, padding=0)\n",
        "        self.c4e32 = nn.Conv2d(32, 128, 3, stride=1, padding=1)\n",
        "        # P4\n",
        "        self.mp4e4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
        "        self.c4e4 = nn.Conv2d(528, 128, 1, stride=1, padding=0)\n",
        "\n",
        "        # 832x14x14\n",
        "        # MP\n",
        "        self.mp4 = nn.MaxPool2d(3, stride=2, padding=1)\n",
        "\n",
        "        # Inception 5a\n",
        "        # 832x7x7\n",
        "        # P1\n",
        "        self.c5a1 = nn.Conv2d(832, 256, 1, stride=1, padding=0)\n",
        "        # P2\n",
        "        self.c5a21 = nn.Conv2d(832, 160, 1, stride=1, padding=0)\n",
        "        self.c5a22 = nn.Conv2d(160, 320, 3, stride=1, padding=1)\n",
        "        # P3\n",
        "        self.c5a31 = nn.Conv2d(832, 32, 1, stride=1, padding=0)\n",
        "        self.c5a32 = nn.Conv2d(32, 128, 3, stride=1, padding=1)\n",
        "        # P4\n",
        "        self.mp5a4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
        "        self.c5a4 = nn.Conv2d(832, 128, 1, stride=1, padding=0)\n",
        "\n",
        "        # Inception 5b\n",
        "        # 832x7x7\n",
        "        # P1\n",
        "        self.c5b1 = nn.Conv2d(832, 384, 1, stride=1, padding=0)\n",
        "        # P2\n",
        "        self.c5b21 = nn.Conv2d(832, 192, 1, stride=1, padding=0)\n",
        "        self.c5b22 = nn.Conv2d(192, 384, 3, stride=1, padding=1)\n",
        "        # P3\n",
        "        self.c5b31 = nn.Conv2d(832, 48, 1, stride=1, padding=0)\n",
        "        self.c5b32 = nn.Conv2d(48, 128, 3, stride=1, padding=1)\n",
        "        # P4\n",
        "        self.mp5b4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
        "        self.c5b4 = nn.Conv2d(832, 128, 1, stride=1, padding=0)\n",
        "\n",
        "        # 1024x7x7\n",
        "        self.ap = nn.AvgPool2d(7, stride=1)\n",
        "        # 1024x1x1\n",
        "        self.drop = nn.Dropout(0.4)\n",
        "        self.flat = nn.Flatten(1, -1)\n",
        "        self.out = nn.Linear(1024, self.n_classes)\n",
        "\n",
        "    def forward(self, x, auxiliary=True):\n",
        "\n",
        "        x = x - self.RGB_mean[None, :, None, None]\n",
        "\n",
        "        # Layer 1\n",
        "        x = self.mp1(F.relu(self.c1(x)))\n",
        "\n",
        "        # Layer 2\n",
        "        x = self.mp2(F.relu(self.c22(F.relu(self.c21(x)))))\n",
        "\n",
        "        # Layer 3a\n",
        "        x1 = F.relu(self.c3a1(x))\n",
        "        x2 = F.relu(self.c3a22(F.relu(self.c3a21(x))))\n",
        "        x3 = F.relu(self.c3a32(F.relu(self.c3a31(x))))\n",
        "        x4 = F.relu(self.c3a4(self.mp3a4(x)))\n",
        "        x = torch.cat((x1, x2, x3, x4), 1)\n",
        "\n",
        "        # Layer 3b\n",
        "        x1 = F.relu(self.c3b1(x))\n",
        "        x2 = F.relu(self.c3b22(F.relu(self.c3b21(x))))\n",
        "        x3 = F.relu(self.c3b32(F.relu(self.c3b31(x))))\n",
        "        x4 = F.relu(self.c3b4(self.mp3b4(x)))\n",
        "        x = self.mp3(torch.cat((x1, x2, x3, x4), 1))\n",
        "\n",
        "        # Layer 4a\n",
        "        x1 = F.relu(self.c4a1(x))\n",
        "        x2 = F.relu(self.c4a22(F.relu(self.c4a21(x))))\n",
        "        x3 = F.relu(self.c4a32(F.relu(self.c4a31(x))))\n",
        "        x4 = F.relu(self.c4a4(self.mp4a4(x)))\n",
        "        x = torch.cat((x1, x2, x3, x4), 1)\n",
        "\n",
        "        # Auxiliary 1\n",
        "        if auxiliary == True:\n",
        "            z1 = self.flat1(F.relu(self.ca1(self.apa1(x))))\n",
        "            z1 = self.a1out(self.a1drop(F.relu(self.fca1(z1))))\n",
        "        else:\n",
        "            z1 = None\n",
        "\n",
        "        # Layer 4b\n",
        "        x1 = F.relu(self.c4b1(x))\n",
        "        x2 = F.relu(self.c4b22(F.relu(self.c4b21(x))))\n",
        "        x3 = F.relu(self.c4b32(F.relu(self.c4b31(x))))\n",
        "        x4 = F.relu(self.c4b4(self.mp4b4(x)))\n",
        "        x = torch.cat((x1, x2, x3, x4), 1)\n",
        "\n",
        "        # Layer 4c\n",
        "        x1 = F.relu(self.c4c1(x))\n",
        "        x2 = F.relu(self.c4c22(F.relu(self.c4c21(x))))\n",
        "        x3 = F.relu(self.c4c32(F.relu(self.c4c31(x))))\n",
        "        x4 = F.relu(self.c4c4(self.mp4c4(x)))\n",
        "        x = torch.cat((x1, x2, x3, x4), 1)\n",
        "\n",
        "        # Layer 4d\n",
        "        x1 = F.relu(self.c4d1(x))\n",
        "        x2 = F.relu(self.c4d22(F.relu(self.c4d21(x))))\n",
        "        x3 = F.relu(self.c4d32(F.relu(self.c4d31(x))))\n",
        "        x4 = F.relu(self.c4d4(self.mp4d4(x)))\n",
        "        x = torch.cat((x1, x2, x3, x4), 1)\n",
        "\n",
        "        # Auxiliary 2\n",
        "        if auxiliary == True:\n",
        "            z2 = self.flat(F.relu(self.ca2(self.apa2(x))))\n",
        "            z2 = self.a2out(self.a2drop(F.relu(self.fca2(z2))))\n",
        "        else:\n",
        "            z2 = None\n",
        "\n",
        "        # Layer 4e\n",
        "        x1 = F.relu(self.c4e1(x))\n",
        "        x2 = F.relu(self.c4e22(F.relu(self.c4e21(x))))\n",
        "        x3 = F.relu(self.c4e32(F.relu(self.c4e31(x))))\n",
        "        x4 = F.relu(self.c4e4(self.mp4e4(x)))\n",
        "        x = self.mp4(torch.cat((x1, x2, x3, x4), 1))\n",
        "\n",
        "        # Layer 5a\n",
        "        x1 = F.relu(self.c5a1(x))\n",
        "        x2 = F.relu(self.c5a22(F.relu(self.c5a21(x))))\n",
        "        x3 = F.relu(self.c5a32(F.relu(self.c5a31(x))))\n",
        "        x4 = F.relu(self.c5a4(self.mp5a4(x)))\n",
        "        x = torch.cat((x1, x2, x3, x4), 1)\n",
        "\n",
        "        # Layer 5b\n",
        "        x1 = F.relu(self.c5b1(x))\n",
        "        x2 = F.relu(self.c5b22(F.relu(self.c5b21(x))))\n",
        "        x3 = F.relu(self.c5b32(F.relu(self.c5b31(x))))\n",
        "        x4 = F.relu(self.c5b4(self.mp5b4(x)))\n",
        "        x = torch.cat((x1, x2, x3, x4), 1)\n",
        "\n",
        "        # Final Output\n",
        "        x = self.out(self.flat(self.drop(self.ap(x))))\n",
        "\n",
        "        return x, z1, z2\n",
        "\n",
        "    \n",
        "    def predict(self, x):\n",
        "\n",
        "        y_hat = self.forward(x)\n",
        "        y_hat = torch.argmax(y_hat, axis=1)\n",
        "        return y_hat"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOTtnofE32ZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# googlenet = models.googlenet(pretrained=True)\n",
        "# pickle.dump(googlenet, open('/content/drive/My Drive/google_init.sav', 'wb'))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnTY_tSQ5Zt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "googlenet = pickle.load(open('/content/drive/My Drive/google_init.sav', 'rb'))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k-5ltA0Q2kdX",
        "colab": {}
      },
      "source": [
        "classifier = GoogLeNet(7, RGB_mean)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(classifier.parameters(), lr=0.01, momentum=0.9)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yrvxmk_5pNL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "754a2b53-ae5a-426a-8a0a-6a9516920b24"
      },
      "source": [
        "params = list(googlenet.parameters())\n",
        "len(params)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "173"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo6APIXm53j9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = list(googlenet.parameters())\n",
        "\n",
        "with torch.no_grad():\n",
        "    \n",
        "    classifier.c1.weight = params[0]\n",
        "    # classifier.c1.bias = params[1]\n",
        "\n",
        "    classifier.c21.weight = params[3]\n",
        "    # classifier.c21.bias = params[4]\n",
        "    classifier.c22.weight = params[6]\n",
        "    # classifier.c22.bias = params[7]\n",
        "\n",
        "    classifier.c3a1.weight = params[9]\n",
        "    # classifier.c3a1.bias = params[10]\n",
        "    classifier.c3a21.weight = params[12]\n",
        "    # classifier.c3a21.bias = params[13]\n",
        "    classifier.c3a22.weight = params[15]\n",
        "    # classifier.c3a22.bias = params[16]\n",
        "    classifier.c3a31.weight = params[18]\n",
        "    # classifier.c3a31.bias = params[19]\n",
        "    classifier.c3a32.weight = params[21]\n",
        "    # classifier.c3a32.bias = params[22]\n",
        "    classifier.c3a4.weight = params[24]\n",
        "    # classifier.c3a4.bias = params[25]\n",
        "    \n",
        "    classifier.c3b1.weight = params[27]\n",
        "    # classifier.c3b1.bias = params[28]\n",
        "    classifier.c3b21.weight = params[30]\n",
        "    # classifier.c3b21.bias = params[31]\n",
        "    classifier.c3b22.weight = params[33]\n",
        "    # classifier.c3b22.bias = params[34]\n",
        "    classifier.c3b31.weight = params[36]\n",
        "    # classifier.c3b31.bias = params[37]\n",
        "    classifier.c3b32.weight = params[39]\n",
        "    # classifier.c3b32.bias = params[40]\n",
        "    classifier.c3b4.weight = params[42]\n",
        "    # classifier.c3b4.bias = params[43]\n",
        "    \n",
        "    classifier.c4a1.weight = params[45]\n",
        "    # classifier.c4a1.bias = params[46]\n",
        "    classifier.c4a21.weight = params[48]\n",
        "    # classifier.c4a21.bias = params[49]\n",
        "    classifier.c4a22.weight = params[51]\n",
        "    # classifier.c4a22.bias = params[52]\n",
        "    classifier.c4a31.weight = params[54]\n",
        "    # classifier.c4a31.bias = params[55]\n",
        "    classifier.c4a32.weight = params[57]\n",
        "    # classifier.c4a32.bias = params[58]\n",
        "    classifier.c4a4.weight = params[60]\n",
        "    # classifier.c4a4.bias = params[61]\n",
        "\n",
        "    classifier.c4b1.weight = params[63]\n",
        "    # classifier.c4b1.bias = params[64]\n",
        "    classifier.c4b21.weight = params[66]\n",
        "    # classifier.c4b21.bias = params[67]\n",
        "    classifier.c4b22.weight = params[69]\n",
        "    # classifier.c4b22.bias = params[70]\n",
        "    classifier.c4b31.weight = params[72]\n",
        "    # classifier.c4b31.bias = params[73]\n",
        "    classifier.c4b32.weight = params[75]\n",
        "    # classifier.c4b32.bias = params[76]\n",
        "    classifier.c4b4.weight = params[78]\n",
        "    # classifier.c4b4.bias = params[79]\n",
        "\n",
        "    classifier.c4c1.weight = params[81]\n",
        "    # classifier.c4c1.bias = params[82]\n",
        "    classifier.c4c21.weight = params[84]\n",
        "    # classifier.c4c21.bias = params[85]\n",
        "    classifier.c4c22.weight = params[87]\n",
        "    # classifier.c4c22.bias = params[88]\n",
        "    classifier.c4c31.weight = params[90]\n",
        "    # classifier.c4c31.bias = params[91]\n",
        "    classifier.c4c32.weight = params[93]\n",
        "    # classifier.c4c32.bias = params[94]\n",
        "    classifier.c4c4.weight = params[96]\n",
        "    # classifier.c4c4.bias = params[97]\n",
        "\n",
        "    classifier.c4d1.weight = params[99]\n",
        "    # classifier.c4d1.bias = params[100]\n",
        "    classifier.c4d21.weight = params[102]\n",
        "    # classifier.c4d21.bias = params[103]\n",
        "    classifier.c4d22.weight = params[105]\n",
        "    # classifier.c4d22.bias = params[106]\n",
        "    classifier.c4d31.weight = params[108]\n",
        "    # classifier.c4d31.bias = params[109]\n",
        "    classifier.c4d32.weight = params[111]\n",
        "    # classifier.c4d32.bias = params[112]\n",
        "    classifier.c4d4.weight = params[114]\n",
        "    # classifier.c4d4.bias = params[115]\n",
        "\n",
        "    classifier.c4e1.weight = params[117]\n",
        "    # classifier.c4e1.bias = params[118]\n",
        "    classifier.c4e21.weight = params[120]\n",
        "    # classifier.c4e21.bias = params[121]\n",
        "    classifier.c4e22.weight = params[123]\n",
        "    # classifier.c4e22.bias = params[124]\n",
        "    classifier.c4e31.weight = params[126]\n",
        "    # classifier.c4e31.bias = params[127]\n",
        "    classifier.c4e32.weight = params[129]\n",
        "    # classifier.c4e32.bias = params[130]\n",
        "    classifier.c4e4.weight = params[132]\n",
        "    # classifier.c4e4.bias = params[133]\n",
        "\n",
        "    classifier.c5a1.weight = params[135]\n",
        "    # classifier.c5a1.bias = params[136]\n",
        "    classifier.c5a21.weight = params[138]\n",
        "    # classifier.c5a21.bias = params[139]\n",
        "    classifier.c5a22.weight = params[141]\n",
        "    # classifier.c5a22.bias = params[142]\n",
        "    classifier.c5a31.weight = params[144]\n",
        "    # classifier.c5a31.bias = params[145]\n",
        "    classifier.c5a32.weight = params[147]\n",
        "    # classifier.c5a32.bias = params[148]\n",
        "    classifier.c5a4.weight = params[150]\n",
        "    # classifier.c5a4.bias = params[151]\n",
        "\n",
        "    classifier.c5b1.weight = params[153]\n",
        "    # classifier.c5b1.bias = params[154]\n",
        "    classifier.c5b21.weight = params[156]\n",
        "    # classifier.c5b21.bias = params[157]\n",
        "    classifier.c5b22.weight = params[159]\n",
        "    # classifier.c5b22.bias = params[160]\n",
        "    classifier.c5b31.weight = params[162]\n",
        "    # classifier.c5b31.bias = params[163]\n",
        "    classifier.c5b32.weight = params[165]\n",
        "    # classifier.c5b32.bias = params[166]\n",
        "    classifier.c5b4.weight = params[168]\n",
        "    # classifier.c5b4.bias = params[169]\n",
        "\n",
        "classifier = classifier.to(device)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QOH2pEviO6nE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e9a52b1a-2ad4-41a8-ab8f-17fa81111887"
      },
      "source": [
        "old_loss = np.inf\n",
        "\n",
        "max_epoch = 500\n",
        "losses = []\n",
        "for epoch in range(max_epoch):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for data in trainloader:\n",
        "        \n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward\n",
        "        y_hat, y_hat1, y_hat2 = classifier(X)\n",
        "        \n",
        "        # Calculate Loss (Cross Entropy)\n",
        "        loss_main = criterion(y_hat, y)\n",
        "        loss1 = criterion(y_hat1, y)\n",
        "        loss2 = criterion(y_hat2, y)\n",
        "\n",
        "        # Weighted Loss\n",
        "        loss = loss_main + 0.3*loss1 + 0.3*loss2\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update Parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()*len(X)/train_size\n",
        "    \n",
        "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
        "    losses.append(running_loss)\n",
        "    if abs(running_loss-old_loss)/running_loss < 0.05 and running_loss<0.2:\n",
        "        print('Converged')\n",
        "        break\n",
        "    \n",
        "    old_loss = running_loss\n",
        "\n",
        "print('Finished Training')\n",
        "plt.plot(losses)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Iter Number')\n",
        "plt.title('Convergence monitor plot')\n",
        "plt.show()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 : Loss = 3.1164643523585087\n",
            "Epoch 2 : Loss = 3.1080164286317724\n",
            "Epoch 3 : Loss = 3.1039295188225937\n",
            "Epoch 4 : Loss = 3.0909779603472987\n",
            "Epoch 5 : Loss = 3.079920948174772\n",
            "Epoch 6 : Loss = 3.074922722806499\n",
            "Epoch 7 : Loss = 3.0583348348996364\n",
            "Epoch 8 : Loss = 3.042222909395703\n",
            "Epoch 9 : Loss = 3.030142420260333\n",
            "Epoch 10 : Loss = 3.018599609880082\n",
            "Epoch 11 : Loss = 2.995203823162704\n",
            "Epoch 12 : Loss = 2.9746299263492277\n",
            "Epoch 13 : Loss = 2.969064879500492\n",
            "Epoch 14 : Loss = 2.954428048914733\n",
            "Epoch 15 : Loss = 2.9299787469857246\n",
            "Epoch 16 : Loss = 2.9042656371817768\n",
            "Epoch 17 : Loss = 2.8814437322915643\n",
            "Epoch 18 : Loss = 2.8726502810621093\n",
            "Epoch 19 : Loss = 2.843930604981213\n",
            "Epoch 20 : Loss = 2.830714169279623\n",
            "Epoch 21 : Loss = 2.81688010152624\n",
            "Epoch 22 : Loss = 2.8105200848928313\n",
            "Epoch 23 : Loss = 2.772459214572707\n",
            "Epoch 24 : Loss = 2.7631686007934997\n",
            "Epoch 25 : Loss = 2.731285630202875\n",
            "Epoch 26 : Loss = 2.7153428780492583\n",
            "Epoch 27 : Loss = 2.7001286308939867\n",
            "Epoch 28 : Loss = 2.680428172653145\n",
            "Epoch 29 : Loss = 2.6620733787789157\n",
            "Epoch 30 : Loss = 2.629120916439681\n",
            "Epoch 31 : Loss = 2.59954516646754\n",
            "Epoch 32 : Loss = 2.5935385476421393\n",
            "Epoch 33 : Loss = 2.573152171610124\n",
            "Epoch 34 : Loss = 2.537383905271204\n",
            "Epoch 35 : Loss = 2.487561160263699\n",
            "Epoch 36 : Loss = 2.4717251606519217\n",
            "Epoch 37 : Loss = 2.460934495261322\n",
            "Epoch 38 : Loss = 2.448342680515728\n",
            "Epoch 39 : Loss = 2.409612201232113\n",
            "Epoch 40 : Loss = 2.3862181573794694\n",
            "Epoch 41 : Loss = 2.3411622404636816\n",
            "Epoch 42 : Loss = 2.3273347778187397\n",
            "Epoch 43 : Loss = 2.2961612204641417\n",
            "Epoch 44 : Loss = 2.264931165382837\n",
            "Epoch 45 : Loss = 2.2322875298689464\n",
            "Epoch 46 : Loss = 2.2001799415628254\n",
            "Epoch 47 : Loss = 2.1949157316094907\n",
            "Epoch 48 : Loss = 2.1491749544176906\n",
            "Epoch 49 : Loss = 2.1365894951471467\n",
            "Epoch 50 : Loss = 2.093997426149322\n",
            "Epoch 51 : Loss = 2.0988535789662537\n",
            "Epoch 52 : Loss = 2.0408085838012164\n",
            "Epoch 53 : Loss = 2.042130647220678\n",
            "Epoch 54 : Loss = 2.0288310698931227\n",
            "Epoch 55 : Loss = 2.0000562618003075\n",
            "Epoch 56 : Loss = 2.004658138295084\n",
            "Epoch 57 : Loss = 1.964362982673512\n",
            "Epoch 58 : Loss = 1.9363177146645787\n",
            "Epoch 59 : Loss = 1.9020193769541354\n",
            "Epoch 60 : Loss = 1.883198903412769\n",
            "Epoch 61 : Loss = 1.8746817867930343\n",
            "Epoch 62 : Loss = 1.8115928945641067\n",
            "Epoch 63 : Loss = 1.7895298153681207\n",
            "Epoch 64 : Loss = 1.8032350108183222\n",
            "Epoch 65 : Loss = 1.7999982185895436\n",
            "Epoch 66 : Loss = 1.7538099122795079\n",
            "Epoch 67 : Loss = 1.7258447533285162\n",
            "Epoch 68 : Loss = 1.6855091068387447\n",
            "Epoch 69 : Loss = 1.6889954207250883\n",
            "Epoch 70 : Loss = 1.6807742804185022\n",
            "Epoch 71 : Loss = 1.6525196086238902\n",
            "Epoch 72 : Loss = 1.625275865249102\n",
            "Epoch 73 : Loss = 1.6441801474900197\n",
            "Epoch 74 : Loss = 1.5655246121542794\n",
            "Epoch 75 : Loss = 1.5648616485894764\n",
            "Epoch 76 : Loss = 1.5355822699410575\n",
            "Epoch 77 : Loss = 1.5345824117859899\n",
            "Epoch 78 : Loss = 1.4906051644880183\n",
            "Epoch 79 : Loss = 1.480891108097515\n",
            "Epoch 80 : Loss = 1.4855533422908718\n",
            "Epoch 81 : Loss = 1.4977823840616473\n",
            "Epoch 82 : Loss = 1.5784425000280455\n",
            "Epoch 83 : Loss = 1.5266290250970926\n",
            "Epoch 84 : Loss = 1.4604506799983645\n",
            "Epoch 85 : Loss = 1.5021279680604303\n",
            "Epoch 86 : Loss = 1.4683538143643102\n",
            "Epoch 87 : Loss = 1.3623258893082781\n",
            "Epoch 88 : Loss = 1.3828772435204908\n",
            "Epoch 89 : Loss = 1.3407759990426307\n",
            "Epoch 90 : Loss = 1.4406187015128054\n",
            "Epoch 91 : Loss = 1.3580200161252705\n",
            "Epoch 92 : Loss = 1.3075857195704654\n",
            "Epoch 93 : Loss = 1.3042631693418019\n",
            "Epoch 94 : Loss = 1.343013994785136\n",
            "Epoch 95 : Loss = 1.2713578279010096\n",
            "Epoch 96 : Loss = 1.2519336062441304\n",
            "Epoch 97 : Loss = 1.2083330702698605\n",
            "Epoch 98 : Loss = 1.2199730814957037\n",
            "Epoch 99 : Loss = 1.2124654823063972\n",
            "Epoch 100 : Loss = 1.2069885788894281\n",
            "Epoch 101 : Loss = 1.1474231568778432\n",
            "Epoch 102 : Loss = 1.1511623236360449\n",
            "Epoch 103 : Loss = 1.1297185624518045\n",
            "Epoch 104 : Loss = 1.1637620477310873\n",
            "Epoch 105 : Loss = 1.1707056565567175\n",
            "Epoch 106 : Loss = 1.1510210519052966\n",
            "Epoch 107 : Loss = 1.1079020820012908\n",
            "Epoch 108 : Loss = 1.1566080546545234\n",
            "Epoch 109 : Loss = 1.120653673539178\n",
            "Epoch 110 : Loss = 1.114382345086606\n",
            "Epoch 111 : Loss = 1.1117765648024425\n",
            "Epoch 112 : Loss = 1.0749041984305563\n",
            "Epoch 113 : Loss = 1.0796003690580043\n",
            "Epoch 114 : Loss = 1.0588786427567645\n",
            "Epoch 115 : Loss = 1.0254857340756194\n",
            "Epoch 116 : Loss = 0.9643304701050814\n",
            "Epoch 117 : Loss = 1.0878056707282513\n",
            "Epoch 118 : Loss = 1.0322667842127304\n",
            "Epoch 119 : Loss = 1.023813051213786\n",
            "Epoch 120 : Loss = 0.956730095143933\n",
            "Epoch 121 : Loss = 0.9446437363009835\n",
            "Epoch 122 : Loss = 0.9754893655145626\n",
            "Epoch 123 : Loss = 0.9668953593184308\n",
            "Epoch 124 : Loss = 0.9074483420375332\n",
            "Epoch 125 : Loss = 0.909691588388503\n",
            "Epoch 126 : Loss = 0.8790516103601621\n",
            "Epoch 127 : Loss = 0.9188961995187952\n",
            "Epoch 128 : Loss = 0.8569847345352173\n",
            "Epoch 129 : Loss = 0.8422218041968262\n",
            "Epoch 130 : Loss = 0.8294931292949238\n",
            "Epoch 131 : Loss = 0.9176054000854493\n",
            "Epoch 132 : Loss = 0.888840364662197\n",
            "Epoch 133 : Loss = 0.8181203931881575\n",
            "Epoch 134 : Loss = 0.8379562104620585\n",
            "Epoch 135 : Loss = 0.8196652317711699\n",
            "Epoch 136 : Loss = 0.8069336970924085\n",
            "Epoch 137 : Loss = 0.7971903179165378\n",
            "Epoch 138 : Loss = 0.773512315874731\n",
            "Epoch 139 : Loss = 0.8121031246534208\n",
            "Epoch 140 : Loss = 0.8067812900925346\n",
            "Epoch 141 : Loss = 0.7336451839071533\n",
            "Epoch 142 : Loss = 0.7791260541108428\n",
            "Epoch 143 : Loss = 0.8345085110398536\n",
            "Epoch 144 : Loss = 0.7669404722256943\n",
            "Epoch 145 : Loss = 0.7987365101688\n",
            "Epoch 146 : Loss = 0.7588242961139213\n",
            "Epoch 147 : Loss = 0.6774972338709682\n",
            "Epoch 148 : Loss = 0.6688622699381997\n",
            "Epoch 149 : Loss = 0.7250256663000127\n",
            "Epoch 150 : Loss = 0.7385079594854694\n",
            "Epoch 151 : Loss = 0.6693039962223598\n",
            "Epoch 152 : Loss = 0.7559901691895329\n",
            "Epoch 153 : Loss = 0.72333309708572\n",
            "Epoch 154 : Loss = 0.6906226632487068\n",
            "Epoch 155 : Loss = 0.7085359476584592\n",
            "Epoch 156 : Loss = 0.7184402953041555\n",
            "Epoch 157 : Loss = 0.7064598080173186\n",
            "Epoch 158 : Loss = 0.6974967022390731\n",
            "Epoch 159 : Loss = 0.6328254229931051\n",
            "Epoch 160 : Loss = 0.5661107233177078\n",
            "Epoch 161 : Loss = 0.6180301337707333\n",
            "Epoch 162 : Loss = 0.5809034119499684\n",
            "Epoch 163 : Loss = 0.5892519059912252\n",
            "Epoch 164 : Loss = 0.5495418895827768\n",
            "Epoch 165 : Loss = 0.5904932138396471\n",
            "Epoch 166 : Loss = 0.5539504277166174\n",
            "Epoch 167 : Loss = 0.5859607871401186\n",
            "Epoch 168 : Loss = 0.5763342110537486\n",
            "Epoch 169 : Loss = 0.5520988191877092\n",
            "Epoch 170 : Loss = 0.5403221112510469\n",
            "Epoch 171 : Loss = 0.5470358158237842\n",
            "Epoch 172 : Loss = 0.554989672288662\n",
            "Epoch 173 : Loss = 0.5548646051293882\n",
            "Epoch 174 : Loss = 0.5562522168358859\n",
            "Epoch 175 : Loss = 0.6254936242768159\n",
            "Epoch 176 : Loss = 0.5268375944177448\n",
            "Epoch 177 : Loss = 0.5134970678685019\n",
            "Epoch 178 : Loss = 0.5430855626428585\n",
            "Epoch 179 : Loss = 0.4816392298774852\n",
            "Epoch 180 : Loss = 0.465033165669192\n",
            "Epoch 181 : Loss = 0.4958517765334259\n",
            "Epoch 182 : Loss = 0.5044961844794841\n",
            "Epoch 183 : Loss = 0.5577077815756981\n",
            "Epoch 184 : Loss = 0.534906770501818\n",
            "Epoch 185 : Loss = 0.5268823965501287\n",
            "Epoch 186 : Loss = 0.4924856850079127\n",
            "Epoch 187 : Loss = 0.4283699450384864\n",
            "Epoch 188 : Loss = 0.4643519434571682\n",
            "Epoch 189 : Loss = 0.4382304794281617\n",
            "Epoch 190 : Loss = 0.3999456213949449\n",
            "Epoch 191 : Loss = 0.4577478613795304\n",
            "Epoch 192 : Loss = 0.4046095603434467\n",
            "Epoch 193 : Loss = 0.4898030255314365\n",
            "Epoch 194 : Loss = 0.4075241809522649\n",
            "Epoch 195 : Loss = 0.39570861203329905\n",
            "Epoch 196 : Loss = 0.3716666893676598\n",
            "Epoch 197 : Loss = 0.38553521192863016\n",
            "Epoch 198 : Loss = 0.3853354320085838\n",
            "Epoch 199 : Loss = 0.3595144410166591\n",
            "Epoch 200 : Loss = 0.3634327386314446\n",
            "Epoch 201 : Loss = 0.36350710612140885\n",
            "Epoch 202 : Loss = 0.48339115451852616\n",
            "Epoch 203 : Loss = 0.4591069485252327\n",
            "Epoch 204 : Loss = 0.3718421274361295\n",
            "Epoch 205 : Loss = 0.3383572993585872\n",
            "Epoch 206 : Loss = 0.40039859800388594\n",
            "Epoch 207 : Loss = 0.39124068981263693\n",
            "Epoch 208 : Loss = 0.3170797188315242\n",
            "Epoch 209 : Loss = 0.3299677629504054\n",
            "Epoch 210 : Loss = 0.3670393466326418\n",
            "Epoch 211 : Loss = 0.3141403726792087\n",
            "Epoch 212 : Loss = 0.319730313189769\n",
            "Epoch 213 : Loss = 0.29571320339777746\n",
            "Epoch 214 : Loss = 0.37880368619015\n",
            "Epoch 215 : Loss = 0.3774803923605211\n",
            "Epoch 216 : Loss = 0.3057971958083973\n",
            "Epoch 217 : Loss = 0.3425570798044836\n",
            "Epoch 218 : Loss = 0.43189506736366595\n",
            "Epoch 219 : Loss = 0.43959582412699794\n",
            "Epoch 220 : Loss = 0.33111773748015694\n",
            "Epoch 221 : Loss = 0.3629328742467568\n",
            "Epoch 222 : Loss = 0.2890415485519981\n",
            "Epoch 223 : Loss = 0.3106636209770362\n",
            "Epoch 224 : Loss = 0.26687205424499844\n",
            "Epoch 225 : Loss = 0.27874119896506605\n",
            "Epoch 226 : Loss = 0.32317326677385527\n",
            "Epoch 227 : Loss = 0.26500896294357884\n",
            "Epoch 228 : Loss = 0.27656540580950545\n",
            "Epoch 229 : Loss = 0.3248411317320235\n",
            "Epoch 230 : Loss = 0.2834338465841805\n",
            "Epoch 231 : Loss = 0.2466039099967438\n",
            "Epoch 232 : Loss = 0.24777269031112617\n",
            "Converged\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVvSW-o33fn7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "6f19d91d-23f5-4e4c-fbab-e2d8ea2ac4e9"
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    train_loss = 0.0\n",
        "    y_train = []\n",
        "    y_train_pred = []\n",
        "\n",
        "    for data in trainloader:\n",
        "\n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        y_hat, _, _ = classifier(X)      \n",
        "        train_loss += criterion(y_hat, y)*len(X)/train_size\n",
        "        \n",
        "        y_train.extend(list(y.detach().cpu().numpy()))\n",
        "        y_train_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
        "\n",
        "print('Train Loss =', train_loss.item())\n",
        "pd.DataFrame(confusion_matrix(y_train, y_train_pred))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Loss = 0.27260008454322815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    0   1   2   3   4   5   6\n",
              "0  38   0   0   0   0   0   5\n",
              "1   0  35   0   0   0   0   1\n",
              "2   1   0  39   3   0   0   3\n",
              "3   0   0   0  35   0   0   2\n",
              "4   0   0   1   0  39   0   0\n",
              "5   0   2   1   0   1  38   0\n",
              "6   1   0   0   0   0   0  42"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylNoy_9_3fA4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e789c95-0948-4b3a-e14f-66c1e280bbca"
      },
      "source": [
        "acc = accuracy_score(y_train, y_train_pred)\n",
        "prec = precision_score(y_train, y_train_pred, average='macro')\n",
        "f1 = f1_score(y_train, y_train_pred, average='macro')\n",
        "\n",
        "print('Train Accuracy =', acc, 'Train Precision =', prec, 'Train F1 =', f1)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy = 0.926829268292683 Train Precision = 0.9336672742726707 Train F1 = 0.929207402523245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ATw2OTqQiJCq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "faebbb70-f2fa-4aa3-9dd9-48154848ef87"
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    test_loss = 0.0\n",
        "    y_test = []\n",
        "    y_test_pred = []\n",
        "\n",
        "    for data in testloader:\n",
        "\n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        y_hat, _, _ = classifier(X)      \n",
        "        test_loss += criterion(y_hat, y)*len(X)/test_size\n",
        "        \n",
        "        y_test.extend(list(y.detach().cpu().numpy()))\n",
        "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
        "\n",
        "print('Test Loss =', test_loss.item())\n",
        "pd.DataFrame(confusion_matrix(y_test, y_test_pred))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss = 1.4933983087539673\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0  1  2  3  4  5  6\n",
              "0  3  0  0  0  0  0  5\n",
              "1  0  4  0  2  0  0  2\n",
              "2  1  0  1  0  0  0  1\n",
              "3  1  0  3  6  0  0  0\n",
              "4  1  0  0  0  2  1  1\n",
              "5  1  0  0  0  1  1  1\n",
              "6  0  0  1  0  0  0  3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgq5cyyZ4m15",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0a33ef9-57f7-461d-ed72-0934c4bf55b1"
      },
      "source": [
        "acc = accuracy_score(y_test, y_test_pred)\n",
        "prec = precision_score(y_test, y_test_pred, average='macro')\n",
        "f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "print('Test Accuracy =', acc, 'Test Precision =', prec, 'Test F1 =', f1)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy = 0.47619047619047616 Test Precision = 0.5394296180010466 Test F1 = 0.45280112044817933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s1RPoXsreDy4"
      },
      "source": [
        "### Question 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NuqwHPPlQavd",
        "colab": {}
      },
      "source": [
        "class CNN2(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_classes):\n",
        "        super(CNN2, self).__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        # 3x224x224\n",
        "        self.cl1 = nn.Conv2d(3, 4, 3, stride=1, padding=1)\n",
        "        # 4x224x224\n",
        "        self.pl1 = nn.AvgPool2d(2, stride=2)\n",
        "        # 4x112x112\n",
        "        self.cl2 = nn.Conv2d(4, 16, 3, stride=1, padding=1)\n",
        "        # 16x112x112\n",
        "        self.pl2 = nn.AvgPool2d(2, stride=2)\n",
        "\n",
        "        # 16x56x56\n",
        "        self.flat = nn.Flatten(1, -1)\n",
        "        # (16x56x56)x1\n",
        "        self.out = nn.Linear(16*56*56, self.n_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.pl1(F.relu(self.cl1(x)))\n",
        "        x = self.pl2(F.relu(self.cl2(x)))\n",
        "        x = self.out(self.flat(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "\n",
        "        y_hat = self.forward(x)\n",
        "        y_hat = torch.argmax(y_hat, axis=1)\n",
        "        return y_hat"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y4dEykLIhv7s",
        "colab": {}
      },
      "source": [
        "classifier = CNN2(7)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(classifier.parameters(), lr=0.00005, momentum=0.9)\n",
        "classifier = classifier.to(device)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_3RX6TtEhMFp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "449f89a5-c6fb-4d6d-9080-f8259f1440d8"
      },
      "source": [
        "old_loss = np.inf\n",
        "losses = []\n",
        "max_epoch = 500\n",
        "for epoch in range(max_epoch):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for data in trainloader:\n",
        "            \n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward\n",
        "        y_hat = classifier(X)\n",
        "        \n",
        "        # Calculate Loss (Cross Entropy)\n",
        "        loss = criterion(y_hat, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update Parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()*len(X)/train_size\n",
        "\n",
        "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
        "    losses.append(running_loss)\n",
        "\n",
        "    if (abs(running_loss-old_loss)/running_loss < 0.2) and running_loss<0.3:\n",
        "        print('Converged')\n",
        "        break\n",
        "    \n",
        "    old_loss = running_loss\n",
        "\n",
        "print('Finished Training')\n",
        "plt.plot(losses)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Iter Number')\n",
        "plt.title('Convergence monitor plot')\n",
        "plt.show()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 : Loss = 1.9484334253683324\n",
            "Epoch 2 : Loss = 1.947373752809983\n",
            "Epoch 3 : Loss = 1.9462925332763885\n",
            "Epoch 4 : Loss = 1.9449584060429694\n",
            "Epoch 5 : Loss = 1.9439449260459127\n",
            "Epoch 6 : Loss = 1.9427635395568421\n",
            "Epoch 7 : Loss = 1.9418055026788745\n",
            "Epoch 8 : Loss = 1.940997310631782\n",
            "Epoch 9 : Loss = 1.9404067469806208\n",
            "Epoch 10 : Loss = 1.9397513779198252\n",
            "Epoch 11 : Loss = 1.9390972383346292\n",
            "Epoch 12 : Loss = 1.938540056607449\n",
            "Epoch 13 : Loss = 1.9379960842664232\n",
            "Epoch 14 : Loss = 1.9372729299791183\n",
            "Epoch 15 : Loss = 1.9368470446157953\n",
            "Epoch 16 : Loss = 1.9365565810054022\n",
            "Epoch 17 : Loss = 1.9359457754507299\n",
            "Epoch 18 : Loss = 1.935442887532171\n",
            "Epoch 19 : Loss = 1.9347893161640763\n",
            "Epoch 20 : Loss = 1.93448837168956\n",
            "Epoch 21 : Loss = 1.9339445463871707\n",
            "Epoch 22 : Loss = 1.9335623850805834\n",
            "Epoch 23 : Loss = 1.9328854491071004\n",
            "Epoch 24 : Loss = 1.932512533373949\n",
            "Epoch 25 : Loss = 1.9318959422227813\n",
            "Epoch 26 : Loss = 1.9314164147559776\n",
            "Epoch 27 : Loss = 1.931144365450231\n",
            "Epoch 28 : Loss = 1.9304854313255604\n",
            "Epoch 29 : Loss = 1.929869052840442\n",
            "Epoch 30 : Loss = 1.929652035028677\n",
            "Epoch 31 : Loss = 1.9288988561995768\n",
            "Epoch 32 : Loss = 1.9284731153827095\n",
            "Epoch 33 : Loss = 1.927922998986593\n",
            "Epoch 34 : Loss = 1.927448688899183\n",
            "Epoch 35 : Loss = 1.9267448344712472\n",
            "Epoch 36 : Loss = 1.9265186043151163\n",
            "Epoch 37 : Loss = 1.9256972332033961\n",
            "Epoch 38 : Loss = 1.9251209194236516\n",
            "Epoch 39 : Loss = 1.9244895193219598\n",
            "Epoch 40 : Loss = 1.9239521574890988\n",
            "Epoch 41 : Loss = 1.9232810831236093\n",
            "Epoch 42 : Loss = 1.9228265750698927\n",
            "Epoch 43 : Loss = 1.9221636925009484\n",
            "Epoch 44 : Loss = 1.9213716900722486\n",
            "Epoch 45 : Loss = 1.9207152688960165\n",
            "Epoch 46 : Loss = 1.9202305917540492\n",
            "Epoch 47 : Loss = 1.9194150290838101\n",
            "Epoch 48 : Loss = 1.9187590162097783\n",
            "Epoch 49 : Loss = 1.9180176631914196\n",
            "Epoch 50 : Loss = 1.917649082605847\n",
            "Epoch 51 : Loss = 1.916613891980374\n",
            "Epoch 52 : Loss = 1.9160641348735796\n",
            "Epoch 53 : Loss = 1.915375442039676\n",
            "Epoch 54 : Loss = 1.9145038331427224\n",
            "Epoch 55 : Loss = 1.9139695674284825\n",
            "Epoch 56 : Loss = 1.9129308881659957\n",
            "Epoch 57 : Loss = 1.9120563268661501\n",
            "Epoch 58 : Loss = 1.9113547266152677\n",
            "Epoch 59 : Loss = 1.9103483634543337\n",
            "Epoch 60 : Loss = 1.909640578858113\n",
            "Epoch 61 : Loss = 1.9090816310058487\n",
            "Epoch 62 : Loss = 1.9079128894241013\n",
            "Epoch 63 : Loss = 1.9069307623009233\n",
            "Epoch 64 : Loss = 1.9059749077421448\n",
            "Epoch 65 : Loss = 1.9053806602331822\n",
            "Epoch 66 : Loss = 1.904361585706784\n",
            "Epoch 67 : Loss = 1.9033612154086708\n",
            "Epoch 68 : Loss = 1.9022744885720442\n",
            "Epoch 69 : Loss = 1.9014995779309953\n",
            "Epoch 70 : Loss = 1.9002110916563029\n",
            "Epoch 71 : Loss = 1.8994283493387574\n",
            "Epoch 72 : Loss = 1.8981249560877835\n",
            "Epoch 73 : Loss = 1.8969933293006975\n",
            "Epoch 74 : Loss = 1.8957710718859362\n",
            "Epoch 75 : Loss = 1.894872873917689\n",
            "Epoch 76 : Loss = 1.8935659916143384\n",
            "Epoch 77 : Loss = 1.8924650842719788\n",
            "Epoch 78 : Loss = 1.8913409406715154\n",
            "Epoch 79 : Loss = 1.8901387357545647\n",
            "Epoch 80 : Loss = 1.8892231328146798\n",
            "Epoch 81 : Loss = 1.8874746199684276\n",
            "Epoch 82 : Loss = 1.8858791490049729\n",
            "Epoch 83 : Loss = 1.884718432659056\n",
            "Epoch 84 : Loss = 1.883460458978128\n",
            "Epoch 85 : Loss = 1.881971483861943\n",
            "Epoch 86 : Loss = 1.8804235549753967\n",
            "Epoch 87 : Loss = 1.8789370919769237\n",
            "Epoch 88 : Loss = 1.877619432655361\n",
            "Epoch 89 : Loss = 1.8765157361479172\n",
            "Epoch 90 : Loss = 1.8741211293051054\n",
            "Epoch 91 : Loss = 1.8730310288871204\n",
            "Epoch 92 : Loss = 1.8714256078939406\n",
            "Epoch 93 : Loss = 1.8694362532386384\n",
            "Epoch 94 : Loss = 1.8680261751500573\n",
            "Epoch 95 : Loss = 1.8661254199955106\n",
            "Epoch 96 : Loss = 1.8644354966459376\n",
            "Epoch 97 : Loss = 1.8624358114880553\n",
            "Epoch 98 : Loss = 1.8602975025409605\n",
            "Epoch 99 : Loss = 1.858435933182879\n",
            "Epoch 100 : Loss = 1.8568378105396177\n",
            "Epoch 101 : Loss = 1.8547404302537234\n",
            "Epoch 102 : Loss = 1.8521631025686498\n",
            "Epoch 103 : Loss = 1.8509790324167925\n",
            "Epoch 104 : Loss = 1.8487435903283362\n",
            "Epoch 105 : Loss = 1.8459609763547518\n",
            "Epoch 106 : Loss = 1.8438414210641842\n",
            "Epoch 107 : Loss = 1.8414342959998793\n",
            "Epoch 108 : Loss = 1.8391415985619148\n",
            "Epoch 109 : Loss = 1.8370874036064546\n",
            "Epoch 110 : Loss = 1.8344429860131664\n",
            "Epoch 111 : Loss = 1.8317405051885998\n",
            "Epoch 112 : Loss = 1.828751387496443\n",
            "Epoch 113 : Loss = 1.8262698924499938\n",
            "Epoch 114 : Loss = 1.8237245332073253\n",
            "Epoch 115 : Loss = 1.8209417929632739\n",
            "Epoch 116 : Loss = 1.8177170383805596\n",
            "Epoch 117 : Loss = 1.814727600858602\n",
            "Epoch 118 : Loss = 1.8126589022447006\n",
            "Epoch 119 : Loss = 1.8095792822721526\n",
            "Epoch 120 : Loss = 1.8059389740747858\n",
            "Epoch 121 : Loss = 1.803112215696727\n",
            "Epoch 122 : Loss = 1.8001132746606754\n",
            "Epoch 123 : Loss = 1.7969251671734585\n",
            "Epoch 124 : Loss = 1.7932925220150566\n",
            "Epoch 125 : Loss = 1.7893641094712844\n",
            "Epoch 126 : Loss = 1.7862413423817334\n",
            "Epoch 127 : Loss = 1.7823951182049742\n",
            "Epoch 128 : Loss = 1.7784557774507213\n",
            "Epoch 129 : Loss = 1.7749311861676205\n",
            "Epoch 130 : Loss = 1.7709770568156489\n",
            "Epoch 131 : Loss = 1.7669505300422164\n",
            "Epoch 132 : Loss = 1.7645847917849178\n",
            "Epoch 133 : Loss = 1.760190910993968\n",
            "Epoch 134 : Loss = 1.7560418823454855\n",
            "Epoch 135 : Loss = 1.751572580287681\n",
            "Epoch 136 : Loss = 1.7465850829247396\n",
            "Epoch 137 : Loss = 1.7445571015520793\n",
            "Epoch 138 : Loss = 1.740903426130474\n",
            "Epoch 139 : Loss = 1.7352485249682172\n",
            "Epoch 140 : Loss = 1.7308010215958651\n",
            "Epoch 141 : Loss = 1.7274742292610197\n",
            "Epoch 142 : Loss = 1.7203743287495206\n",
            "Epoch 143 : Loss = 1.7178441676528613\n",
            "Epoch 144 : Loss = 1.7151629866623295\n",
            "Epoch 145 : Loss = 1.707442556939474\n",
            "Epoch 146 : Loss = 1.7042793301339765\n",
            "Epoch 147 : Loss = 1.6975591942823725\n",
            "Epoch 148 : Loss = 1.6928629721498658\n",
            "Epoch 149 : Loss = 1.687849085920779\n",
            "Epoch 150 : Loss = 1.6833556250828068\n",
            "Epoch 151 : Loss = 1.679483095112578\n",
            "Epoch 152 : Loss = 1.6738587239893472\n",
            "Epoch 153 : Loss = 1.6674563170310097\n",
            "Epoch 154 : Loss = 1.6634196404380663\n",
            "Epoch 155 : Loss = 1.6624023241448485\n",
            "Epoch 156 : Loss = 1.65202645225392\n",
            "Epoch 157 : Loss = 1.649968918192262\n",
            "Epoch 158 : Loss = 1.6402907076613\n",
            "Epoch 159 : Loss = 1.6372030259009438\n",
            "Epoch 160 : Loss = 1.6337670025509825\n",
            "Epoch 161 : Loss = 1.6284710156377598\n",
            "Epoch 162 : Loss = 1.6208588192271856\n",
            "Epoch 163 : Loss = 1.6210817484905498\n",
            "Epoch 164 : Loss = 1.613494554878527\n",
            "Epoch 165 : Loss = 1.6075170177200526\n",
            "Epoch 166 : Loss = 1.6014824823220015\n",
            "Epoch 167 : Loss = 1.5992170131995702\n",
            "Epoch 168 : Loss = 1.5899677363837639\n",
            "Epoch 169 : Loss = 1.584151668831031\n",
            "Epoch 170 : Loss = 1.579569311092124\n",
            "Epoch 171 : Loss = 1.5774732537385894\n",
            "Epoch 172 : Loss = 1.5700023224129496\n",
            "Epoch 173 : Loss = 1.5664709577992404\n",
            "Epoch 174 : Loss = 1.5606142174491482\n",
            "Epoch 175 : Loss = 1.5581509493784622\n",
            "Epoch 176 : Loss = 1.551198738377269\n",
            "Epoch 177 : Loss = 1.5409033859232992\n",
            "Epoch 178 : Loss = 1.53887057636673\n",
            "Epoch 179 : Loss = 1.5323508820051932\n",
            "Epoch 180 : Loss = 1.529034399820122\n",
            "Epoch 181 : Loss = 1.5246286122225716\n",
            "Epoch 182 : Loss = 1.5159315128359645\n",
            "Epoch 183 : Loss = 1.5199778727953444\n",
            "Epoch 184 : Loss = 1.506733967452099\n",
            "Epoch 185 : Loss = 1.5007071137843646\n",
            "Epoch 186 : Loss = 1.4956993537497436\n",
            "Epoch 187 : Loss = 1.4915352619483497\n",
            "Epoch 188 : Loss = 1.4879117988127866\n",
            "Epoch 189 : Loss = 1.4813738886902972\n",
            "Epoch 190 : Loss = 1.47741304415859\n",
            "Epoch 191 : Loss = 1.4707226960916553\n",
            "Epoch 192 : Loss = 1.463084547361846\n",
            "Epoch 193 : Loss = 1.467499514074691\n",
            "Epoch 194 : Loss = 1.4528035025148027\n",
            "Epoch 195 : Loss = 1.452091575499611\n",
            "Epoch 196 : Loss = 1.4418052235546845\n",
            "Epoch 197 : Loss = 1.4408822013941376\n",
            "Epoch 198 : Loss = 1.4342955948999119\n",
            "Epoch 199 : Loss = 1.4263192917827117\n",
            "Epoch 200 : Loss = 1.4210830777364325\n",
            "Epoch 201 : Loss = 1.4218337319869196\n",
            "Epoch 202 : Loss = 1.4083437014124534\n",
            "Epoch 203 : Loss = 1.4105852269129469\n",
            "Epoch 204 : Loss = 1.413748165040897\n",
            "Epoch 205 : Loss = 1.3995864345636932\n",
            "Epoch 206 : Loss = 1.400226497484001\n",
            "Epoch 207 : Loss = 1.3787788356222757\n",
            "Epoch 208 : Loss = 1.3803562312176003\n",
            "Epoch 209 : Loss = 1.3714518364298218\n",
            "Epoch 210 : Loss = 1.3692589130135777\n",
            "Epoch 211 : Loss = 1.35675529818917\n",
            "Epoch 212 : Loss = 1.3605072303100747\n",
            "Epoch 213 : Loss = 1.351082197049769\n",
            "Epoch 214 : Loss = 1.3394809059980437\n",
            "Epoch 215 : Loss = 1.3351450755621082\n",
            "Epoch 216 : Loss = 1.3436840510949855\n",
            "Epoch 217 : Loss = 1.3296017592792313\n",
            "Epoch 218 : Loss = 1.3194917589945245\n",
            "Epoch 219 : Loss = 1.323899618424605\n",
            "Epoch 220 : Loss = 1.3122343356601038\n",
            "Epoch 221 : Loss = 1.3041860355317385\n",
            "Epoch 222 : Loss = 1.3037693928343077\n",
            "Epoch 223 : Loss = 1.323867414886528\n",
            "Epoch 224 : Loss = 1.293704618975676\n",
            "Epoch 225 : Loss = 1.288493911563727\n",
            "Epoch 226 : Loss = 1.2761711812600858\n",
            "Epoch 227 : Loss = 1.2742859536347073\n",
            "Epoch 228 : Loss = 1.2722517771172606\n",
            "Epoch 229 : Loss = 1.2540094644765822\n",
            "Epoch 230 : Loss = 1.2506482783925659\n",
            "Epoch 231 : Loss = 1.2496351776222734\n",
            "Epoch 232 : Loss = 1.2347703553242966\n",
            "Epoch 233 : Loss = 1.2524244664853459\n",
            "Epoch 234 : Loss = 1.2360219033337636\n",
            "Epoch 235 : Loss = 1.2356645944641855\n",
            "Epoch 236 : Loss = 1.2173446711762856\n",
            "Epoch 237 : Loss = 1.214133893571249\n",
            "Epoch 238 : Loss = 1.2179407421304789\n",
            "Epoch 239 : Loss = 1.2121365871994338\n",
            "Epoch 240 : Loss = 1.1920861097163022\n",
            "Epoch 241 : Loss = 1.1859466448062805\n",
            "Epoch 242 : Loss = 1.1820607771026133\n",
            "Epoch 243 : Loss = 1.1983451290828426\n",
            "Epoch 244 : Loss = 1.182583916478041\n",
            "Epoch 245 : Loss = 1.1919575647194627\n",
            "Epoch 246 : Loss = 1.163543256317697\n",
            "Epoch 247 : Loss = 1.1702897062700384\n",
            "Epoch 248 : Loss = 1.1535261773896965\n",
            "Epoch 249 : Loss = 1.1369711090048014\n",
            "Epoch 250 : Loss = 1.1482565747736224\n",
            "Epoch 251 : Loss = 1.1637464765887644\n",
            "Epoch 252 : Loss = 1.1322903487740492\n",
            "Epoch 253 : Loss = 1.1341912036158066\n",
            "Epoch 254 : Loss = 1.115450781812236\n",
            "Epoch 255 : Loss = 1.0965962007070669\n",
            "Epoch 256 : Loss = 1.089672771895804\n",
            "Epoch 257 : Loss = 1.0895297793145795\n",
            "Epoch 258 : Loss = 1.0886816787387437\n",
            "Epoch 259 : Loss = 1.0972745065489713\n",
            "Epoch 260 : Loss = 1.0639859184570843\n",
            "Epoch 261 : Loss = 1.0752269795962743\n",
            "Epoch 262 : Loss = 1.0599984836079932\n",
            "Epoch 263 : Loss = 1.0382687318615798\n",
            "Epoch 264 : Loss = 1.080560523458474\n",
            "Epoch 265 : Loss = 1.0572939874403153\n",
            "Epoch 266 : Loss = 1.0446396837666476\n",
            "Epoch 267 : Loss = 1.0223810975975276\n",
            "Epoch 268 : Loss = 1.0269975882374036\n",
            "Epoch 269 : Loss = 1.0016940398083332\n",
            "Epoch 270 : Loss = 1.043285569662832\n",
            "Epoch 271 : Loss = 1.0379214361569606\n",
            "Epoch 272 : Loss = 1.0245605935618438\n",
            "Epoch 273 : Loss = 1.0093155918636387\n",
            "Epoch 274 : Loss = 0.9779924474111417\n",
            "Epoch 275 : Loss = 0.9939035530289706\n",
            "Epoch 276 : Loss = 0.9579423396014171\n",
            "Epoch 277 : Loss = 0.9865340642397412\n",
            "Epoch 278 : Loss = 0.988103288391326\n",
            "Epoch 279 : Loss = 0.975360847100979\n",
            "Epoch 280 : Loss = 0.9510958443950693\n",
            "Epoch 281 : Loss = 0.927291606569124\n",
            "Epoch 282 : Loss = 0.937792461923606\n",
            "Epoch 283 : Loss = 0.9317210233585344\n",
            "Epoch 284 : Loss = 0.9419554825028477\n",
            "Epoch 285 : Loss = 0.944180411536519\n",
            "Epoch 286 : Loss = 0.9394850429757547\n",
            "Epoch 287 : Loss = 0.9673254608277246\n",
            "Epoch 288 : Loss = 0.896248078304716\n",
            "Epoch 289 : Loss = 0.9005924126827758\n",
            "Epoch 290 : Loss = 0.8695522077407571\n",
            "Epoch 291 : Loss = 0.8656025119774848\n",
            "Epoch 292 : Loss = 0.8793085404389411\n",
            "Epoch 293 : Loss = 0.8886011568926768\n",
            "Epoch 294 : Loss = 0.8885934280186164\n",
            "Epoch 295 : Loss = 0.8454554807849048\n",
            "Epoch 296 : Loss = 0.8321388945346926\n",
            "Epoch 297 : Loss = 0.8536291041440665\n",
            "Epoch 298 : Loss = 0.8055006074573104\n",
            "Epoch 299 : Loss = 0.8263254105421723\n",
            "Epoch 300 : Loss = 0.8085758613377082\n",
            "Epoch 301 : Loss = 0.8303367601454466\n",
            "Epoch 302 : Loss = 0.8164430912778767\n",
            "Epoch 303 : Loss = 0.7852524317515438\n",
            "Epoch 304 : Loss = 0.7849185927819707\n",
            "Epoch 305 : Loss = 0.7934113630849725\n",
            "Epoch 306 : Loss = 0.7977287918848444\n",
            "Epoch 307 : Loss = 0.7804159156536806\n",
            "Epoch 308 : Loss = 0.7919045028370847\n",
            "Epoch 309 : Loss = 0.7878510376717571\n",
            "Epoch 310 : Loss = 0.7546081277136187\n",
            "Epoch 311 : Loss = 0.7537897615482584\n",
            "Epoch 312 : Loss = 0.7335736599948763\n",
            "Epoch 313 : Loss = 0.7479814357458507\n",
            "Epoch 314 : Loss = 0.7742005106879444\n",
            "Epoch 315 : Loss = 0.7460213976454652\n",
            "Epoch 316 : Loss = 0.70029343399852\n",
            "Epoch 317 : Loss = 0.7099444722880054\n",
            "Epoch 318 : Loss = 0.6836463568102608\n",
            "Epoch 319 : Loss = 0.7010550613187332\n",
            "Epoch 320 : Loss = 0.7010968036767913\n",
            "Epoch 321 : Loss = 0.692012071609497\n",
            "Epoch 322 : Loss = 0.6949641181201469\n",
            "Epoch 323 : Loss = 0.681998949848401\n",
            "Epoch 324 : Loss = 0.6425717738862652\n",
            "Epoch 325 : Loss = 0.6294385794978524\n",
            "Epoch 326 : Loss = 0.6511840105887489\n",
            "Epoch 327 : Loss = 0.643166295327376\n",
            "Epoch 328 : Loss = 0.6438309388293622\n",
            "Epoch 329 : Loss = 0.610070807177846\n",
            "Epoch 330 : Loss = 0.6942905544819318\n",
            "Epoch 331 : Loss = 0.6428474426684894\n",
            "Epoch 332 : Loss = 0.6557813469541198\n",
            "Epoch 333 : Loss = 0.6049705853860968\n",
            "Epoch 334 : Loss = 0.5762568905378468\n",
            "Epoch 335 : Loss = 0.5972974917199138\n",
            "Epoch 336 : Loss = 0.6029168408922201\n",
            "Epoch 337 : Loss = 0.656944196606347\n",
            "Epoch 338 : Loss = 0.6402796835018782\n",
            "Epoch 339 : Loss = 0.6179257045639517\n",
            "Epoch 340 : Loss = 0.5583508952361781\n",
            "Epoch 341 : Loss = 0.5440889636398608\n",
            "Epoch 342 : Loss = 0.5843890012348986\n",
            "Epoch 343 : Loss = 0.5574868020280312\n",
            "Epoch 344 : Loss = 0.5915941963627778\n",
            "Epoch 345 : Loss = 0.5430683811367181\n",
            "Epoch 346 : Loss = 0.5296311901836861\n",
            "Epoch 347 : Loss = 0.5433248275248431\n",
            "Epoch 348 : Loss = 0.527530153557814\n",
            "Epoch 349 : Loss = 0.5622191836194294\n",
            "Epoch 350 : Loss = 0.5660555161250178\n",
            "Epoch 351 : Loss = 0.5491992386791349\n",
            "Epoch 352 : Loss = 0.5229843680451556\n",
            "Epoch 353 : Loss = 0.5311014126817524\n",
            "Epoch 354 : Loss = 0.5154975681770139\n",
            "Epoch 355 : Loss = 0.51721343989987\n",
            "Epoch 356 : Loss = 0.5023463245883636\n",
            "Epoch 357 : Loss = 0.4667678236546001\n",
            "Epoch 358 : Loss = 0.5041804645950371\n",
            "Epoch 359 : Loss = 0.54444850550296\n",
            "Epoch 360 : Loss = 0.539512231997912\n",
            "Epoch 361 : Loss = 0.5274121926428964\n",
            "Epoch 362 : Loss = 0.49500950340194566\n",
            "Epoch 363 : Loss = 0.5119300686108526\n",
            "Epoch 364 : Loss = 0.4888258364557805\n",
            "Epoch 365 : Loss = 0.5225499907852466\n",
            "Epoch 366 : Loss = 0.47094366016703615\n",
            "Epoch 367 : Loss = 0.46579672515599985\n",
            "Epoch 368 : Loss = 0.4564843358478479\n",
            "Epoch 369 : Loss = 0.40679480410619057\n",
            "Epoch 370 : Loss = 0.4257308736910803\n",
            "Epoch 371 : Loss = 0.4299790113645148\n",
            "Epoch 372 : Loss = 0.42303662640707834\n",
            "Epoch 373 : Loss = 0.4255626843366057\n",
            "Epoch 374 : Loss = 0.394255845596566\n",
            "Epoch 375 : Loss = 0.40860047842982755\n",
            "Epoch 376 : Loss = 0.4185934183074207\n",
            "Epoch 377 : Loss = 0.39280291342984514\n",
            "Epoch 378 : Loss = 0.3916854461726411\n",
            "Epoch 379 : Loss = 0.38245227361805345\n",
            "Epoch 380 : Loss = 0.37348780160581607\n",
            "Epoch 381 : Loss = 0.39231201653281156\n",
            "Epoch 382 : Loss = 0.38131260269610306\n",
            "Epoch 383 : Loss = 0.35048954225167994\n",
            "Epoch 384 : Loss = 0.3568873592370063\n",
            "Epoch 385 : Loss = 0.32898384914165585\n",
            "Epoch 386 : Loss = 0.3657814585165695\n",
            "Epoch 387 : Loss = 0.3511504485216706\n",
            "Epoch 388 : Loss = 0.3248704857734853\n",
            "Epoch 389 : Loss = 0.32436372895274007\n",
            "Epoch 390 : Loss = 0.3398112377430919\n",
            "Epoch 391 : Loss = 0.3564128431293606\n",
            "Epoch 392 : Loss = 0.3508485325122128\n",
            "Epoch 393 : Loss = 0.3529411669599885\n",
            "Epoch 394 : Loss = 0.3396880260120285\n",
            "Epoch 395 : Loss = 0.39141056076574826\n",
            "Epoch 396 : Loss = 0.40443235261930405\n",
            "Epoch 397 : Loss = 0.33379200962777755\n",
            "Epoch 398 : Loss = 0.3776924361958321\n",
            "Epoch 399 : Loss = 0.389659152006023\n",
            "Epoch 400 : Loss = 0.3385411231982999\n",
            "Epoch 401 : Loss = 0.3316994316694213\n",
            "Epoch 402 : Loss = 0.34486013391292053\n",
            "Epoch 403 : Loss = 0.3953414156461842\n",
            "Epoch 404 : Loss = 0.35068372390411456\n",
            "Epoch 405 : Loss = 0.33114017499448534\n",
            "Epoch 406 : Loss = 0.31555366973013\n",
            "Epoch 407 : Loss = 0.32544991104029614\n",
            "Epoch 408 : Loss = 0.27686542417944926\n",
            "Converged\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RskMmPeG5RAU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "5f7f7804-27e8-420c-9b2a-d7d3bad8fe78"
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    train_loss = 0.0\n",
        "    y_train = []\n",
        "    y_train_pred = []\n",
        "\n",
        "    for data in trainloader:\n",
        "\n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        y_hat = classifier(X)      \n",
        "        train_loss += criterion(y_hat, y)*len(X)/train_size\n",
        "        \n",
        "        y_train.extend(list(y.detach().cpu().numpy()))\n",
        "        y_train_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
        "\n",
        "print('Train Loss =', train_loss.item())\n",
        "pd.DataFrame(confusion_matrix(y_train, y_train_pred))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Loss = 0.2829172909259796\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    0   1   2   3   4   5   6\n",
              "0  43   0   0   0   0   0   0\n",
              "1   0  30   1   0   0   2   3\n",
              "2   0   0  45   0   0   0   1\n",
              "3   0   0   1  32   0   0   4\n",
              "4   0   0   1   0  36   0   3\n",
              "5   0   0   3   0   0  38   1\n",
              "6   0   0   0   0   0   0  43"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s1_WEeod6Bg2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "146feb9e-b24a-4ea7-d577-27731e9924ca"
      },
      "source": [
        "acc_tr = accuracy_score(y_train, y_train_pred)\n",
        "prec_tr = precision_score(y_train, y_train_pred, average='weighted')\n",
        "f1_tr = f1_score(y_train, y_train_pred, average='weighted')\n",
        "\n",
        "print('Train Accuracy =', acc_tr, 'Train Precision =', prec_tr, 'Train F1 =', f1_tr)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy = 0.9303135888501742 Train Precision = 0.941137341854702 Train F1 = 0.9312989047598114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4eAKVd07iASJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "7f1647d1-862b-4362-d9c5-3dd42f44e14c"
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    test_loss = 0.0\n",
        "    y_test = []\n",
        "    y_test_pred = []\n",
        "\n",
        "    for data in cvloader:\n",
        "\n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        y_hat = classifier(X)      \n",
        "        test_loss += criterion(y_hat, y)*len(X)/test_size\n",
        "        \n",
        "        y_test.extend(list(y.detach().cpu().numpy()))\n",
        "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
        "\n",
        "print('Test Loss =', test_loss.item())\n",
        "pd.DataFrame(confusion_matrix(y_test, y_test_pred))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss = 4.930881977081299\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0  1  2  3  4  5  6\n",
              "0  5  0  1  2  0  1  0\n",
              "1  0  3  2  0  2  0  2\n",
              "2  2  1  4  1  0  0  3\n",
              "3  1  1  4  3  1  0  3\n",
              "4  0  1  5  1  1  5  0\n",
              "5  1  0  4  1  0  3  5\n",
              "6  0  1  2  1  2  3  4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qJK9h5xu4DjA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "016a9a74-5e91-4ee8-af0f-513ce77ca3ab"
      },
      "source": [
        "acc = accuracy_score(y_test, y_test_pred)\n",
        "prec = precision_score(y_test, y_test_pred, average='macro')\n",
        "f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "print('Test Accuracy =', acc, 'Test Precision =', prec, 'Test F1 =', f1)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy = 0.2804878048780488 Test Precision = 0.3073198976560321 Test F1 = 0.292629446576815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_vIZ1K6eE33v"
      },
      "source": [
        "## Question 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jVC5hAh8E33x",
        "colab": {}
      },
      "source": [
        "class CNN3(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_classes, k=4):\n",
        "        super(CNN3, self).__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        # 3x224x224\n",
        "        self.cl1 = nn.Conv2d(3, 4, 3, stride=1, padding=1)\n",
        "        # 4x224x224\n",
        "        self.pl1 = nn.AvgPool2d(2, stride=2)\n",
        "        # 4x112x112\n",
        "        self.cl2 = nn.Conv2d(4, 16, 3, stride=1, padding=1)\n",
        "        # 16x112x112\n",
        "        self.pl2 = nn.AvgPool2d(2, stride=2)\n",
        "\n",
        "        # NetVLAD\n",
        "        self.K = k\n",
        "        self.nv_conv = nn.Conv2d(16, self.K, 1)\n",
        "        self.nv_soft_ass = nn.Softmax2d()\n",
        "\n",
        "        # NetVLAD Parameter\n",
        "        self.c = nn.Parameter(torch.rand(self.K, 16))\n",
        "        \n",
        "        # Flatten to get h\n",
        "        self.flat = nn.Flatten(1, -1)\n",
        "\n",
        "        # Output layer\n",
        "        self.out = nn.Linear(self.K*16, self.n_classes)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        # print(x.shape)\n",
        "        x = self.pl1(F.relu(self.cl1(x)))\n",
        "        # print(x.shape)\n",
        "        x = self.pl2(F.relu(self.cl2(x)))\n",
        "        # print(x.shape)\n",
        "        \n",
        "        # NetVLAD Step 1\n",
        "        a = self.nv_soft_ass(self.nv_conv(x))\n",
        "\n",
        "        # NetVLAD Step 2\n",
        "        for k in range(self.K):\n",
        "            a_k = a[:, k, :, :]\n",
        "            c_k = self.c[k, :]\n",
        "            temp = (x - c_k.reshape(1, -1, 1, 1))*a_k.unsqueeze(1)\n",
        "            z_k = torch.sum(temp, axis=(2, 3))\n",
        "            if k==0:\n",
        "                Z = z_k.unsqueeze(1)\n",
        "            else:\n",
        "                Z = torch.cat((Z, z_k.unsqueeze(1)), 1)\n",
        "        \n",
        "        # Flatten\n",
        "        Z = self.flat(Z)\n",
        "        # print('Z shape', Z.shape)\n",
        "        Z = self.out(Z)\n",
        "\n",
        "        return Z\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "\n",
        "        y_hat = self.forward(x)\n",
        "        y_hat = torch.argmax(y_hat, axis=1)\n",
        "        return y_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2NO7TpyJE330",
        "colab": {}
      },
      "source": [
        "classifier = CNN3(7)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(classifier.parameters(), lr=0.0001, momentum=0.9)\n",
        "classifier = classifier.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmxv4-HM1did",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e3a390f0-8156-403e-daa8-91280233a4fd"
      },
      "source": [
        "old_loss = np.inf\n",
        "from IPython.display import clear_output\n",
        "losses = []\n",
        "max_epoch = 200\n",
        "for epoch in range(max_epoch):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for data in trainloader:\n",
        "            \n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward\n",
        "        y_hat = classifier(X)\n",
        "        \n",
        "        # Calculate Loss (Cross Entropy)\n",
        "        loss = criterion(y_hat, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update Parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()*len(X)/train_size\n",
        "\n",
        "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
        "    losses.append(running_loss)\n",
        "\n",
        "    \n",
        "    if (abs(running_loss-old_loss)/running_loss < 0.2) and epoch>=10 and running_loss<0.01:\n",
        "        print('Converged')\n",
        "        break\n",
        "    \n",
        "    old_loss = running_loss\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 : Loss = 85807623.2677891\n",
            "Epoch 2 : Loss = 28301.72832507622\n",
            "Epoch 3 : Loss = 18408.02954186629\n",
            "Epoch 4 : Loss = 43676.23143510452\n",
            "Epoch 5 : Loss = 43213.82771668119\n",
            "Epoch 6 : Loss = 126600.11846689894\n",
            "Epoch 7 : Loss = 240719.85224303138\n",
            "Epoch 8 : Loss = 215393.4227324695\n",
            "Epoch 9 : Loss = 71398.49858449477\n",
            "Epoch 10 : Loss = 141169.75748584495\n",
            "Epoch 11 : Loss = 207208.68343042248\n",
            "Epoch 12 : Loss = 56377.06017258275\n",
            "Epoch 13 : Loss = 39714.29333623694\n",
            "Epoch 14 : Loss = 69156.5947027439\n",
            "Epoch 15 : Loss = 49089.8818121189\n",
            "Epoch 16 : Loss = 115871.9918336237\n",
            "Epoch 17 : Loss = 96327.01796602787\n",
            "Epoch 18 : Loss = 180568.33177264806\n",
            "Epoch 19 : Loss = 158264.56037674216\n",
            "Epoch 20 : Loss = 189712.8424978223\n",
            "Epoch 21 : Loss = 186683.625\n",
            "Epoch 22 : Loss = 348658.04322735185\n",
            "Epoch 23 : Loss = 444821.3343858886\n",
            "Epoch 24 : Loss = 376555.8679224739\n",
            "Epoch 25 : Loss = 439843.20198170736\n",
            "Epoch 26 : Loss = 271717.7355182927\n",
            "Epoch 27 : Loss = 346034.916104094\n",
            "Epoch 28 : Loss = 582051.9578614983\n",
            "Epoch 29 : Loss = 458804.6652874565\n",
            "Epoch 30 : Loss = 782716.6406794424\n",
            "Epoch 31 : Loss = 737370.8886106273\n",
            "Epoch 32 : Loss = 1109940.2602351916\n",
            "Epoch 33 : Loss = 1259644.2229965155\n",
            "Epoch 34 : Loss = 1218806.8020470385\n",
            "Epoch 35 : Loss = 1253718.5966898955\n",
            "Epoch 36 : Loss = 1410611.5966898955\n",
            "Epoch 37 : Loss = 1033366.2402003484\n",
            "Epoch 38 : Loss = 890387.5984320558\n",
            "Epoch 39 : Loss = 919064.6055095819\n",
            "Epoch 40 : Loss = 1764849.037456446\n",
            "Epoch 41 : Loss = 2898772.5217770035\n",
            "Epoch 42 : Loss = 1614562.4259581878\n",
            "Epoch 43 : Loss = 1797812.1681184666\n",
            "Epoch 44 : Loss = 2174955.413763066\n",
            "Epoch 45 : Loss = 1540044.131533101\n",
            "Epoch 46 : Loss = 1730273.8837108014\n",
            "Epoch 47 : Loss = 1958297.656794425\n",
            "Epoch 48 : Loss = 1609277.43793554\n",
            "Epoch 49 : Loss = 2174077.093858885\n",
            "Epoch 50 : Loss = 2493788.206010453\n",
            "Epoch 51 : Loss = 2400234.141768293\n",
            "Epoch 52 : Loss = 2040736.8732578398\n",
            "Epoch 53 : Loss = 2971595.155923345\n",
            "Epoch 54 : Loss = 4200028.503484321\n",
            "Epoch 55 : Loss = 8876385.809233451\n",
            "Epoch 56 : Loss = 11815059.135888502\n",
            "Epoch 57 : Loss = 5941518.754355401\n",
            "Epoch 58 : Loss = 11030383.993031358\n",
            "Epoch 59 : Loss = 16088955.707317073\n",
            "Epoch 60 : Loss = 16107798.655052265\n",
            "Epoch 61 : Loss = 25727843.770034846\n",
            "Epoch 62 : Loss = 29940467.972125437\n",
            "Epoch 63 : Loss = 47400227.72125436\n",
            "Epoch 64 : Loss = 66212622.773519166\n",
            "Epoch 65 : Loss = 53510676.153310105\n",
            "Epoch 66 : Loss = 33647129.31010453\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAJMMT7Z0_Ri",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "522c15f7-3340-4a72-f00c-8e2ce384dd52"
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    train_loss = 0.0\n",
        "    y_train = []\n",
        "    y_train_pred = []\n",
        "\n",
        "    for data in trainloader:\n",
        "\n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        y_hat = classifier(X)      \n",
        "        train_loss += criterion(y_hat, y)*len(X)/train_size\n",
        "        \n",
        "        y_train.extend(list(y.detach().cpu().numpy()))\n",
        "        y_train_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
        "\n",
        "print('Train Loss =', train_loss.item())\n",
        "plt.imshow(confusion_matrix(y_train, y_train_pred), cmap='Reds')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Loss = 21922.796875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsXfxUyu1OJm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "c3a72c9d-8c46-4da8-87f9-5b876309617a"
      },
      "source": [
        "acc_tr = accuracy_score(y_train, y_train_pred)\n",
        "prec_tr = precision_score(y_train, y_train_pred, average='weighted')\n",
        "f1_tr = f1_score(y_train, y_train_pred, average='weighted')\n",
        "\n",
        "print('Train Accuracy =', acc_tr, 'Train Precision =', prec_tr, 'Train F1 =', f1_tr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy = 0.13240418118466898 Train Precision = 0.017530867195182653 Train F1 = 0.030962208523184133\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lcw20TIlE334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "20e16188-c826-4eed-af1b-6afda2df8118"
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    test_loss = 0.0\n",
        "    y_test = []\n",
        "    y_test_pred = []\n",
        "\n",
        "    for data in testloader:\n",
        "\n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        y_hat = classifier(X)      \n",
        "        test_loss += criterion(y_hat, y)\n",
        "        \n",
        "        y_test.extend(list(y.detach().cpu().numpy()))\n",
        "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
        "\n",
        "print('Test Loss =', test_loss.item())\n",
        "plt.imshow(confusion_matrix(y_test, y_test_pred), cmap='Reds')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss = 57528.66796875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FckLra-d1Ulv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = accuracy_score(y_test, y_test_pred)\n",
        "prec = precision_score(y_test, y_test_pred, average='macro')\n",
        "f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "print('Test Accuracy =', acc, 'Test Precision =', prec, 'Test F1 =', f1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}