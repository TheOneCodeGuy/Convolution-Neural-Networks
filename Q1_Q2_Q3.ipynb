{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_qTq010ijWD"
   },
   "source": [
    "### Run this cell only once (the first ever time you run) to process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "v8PCDpQkAHwN",
    "outputId": "f0a8a37c-61d2-47b2-ebe8-68a68bedd846"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyunpack\n",
      "  Downloading https://files.pythonhosted.org/packages/33/fd/4b64817a1d82df78553ceb1bfc5a2d7ac162da8667be586430fab9db5deb/pyunpack-0.2.1-py2.py3-none-any.whl\n",
      "Collecting entrypoint2 (from pyunpack)\n",
      "  Downloading https://files.pythonhosted.org/packages/46/0a/6156f1bc14a44094cff75bb6ecefe1f8e8a12cfff66379ba3d52d0916c49/entrypoint2-0.2.1-py2.py3-none-any.whl\n",
      "Collecting easyprocess (from pyunpack)\n",
      "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
      "Collecting argparse (from entrypoint2->pyunpack)\n",
      "  Downloading https://files.pythonhosted.org/packages/f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl\n",
      "Installing collected packages: argparse, entrypoint2, easyprocess, pyunpack\n",
      "Successfully installed argparse-1.4.0 easyprocess-0.3 entrypoint2-0.2.1 pyunpack-0.2.1\n",
      "Collecting patool\n",
      "  Downloading https://files.pythonhosted.org/packages/43/94/52243ddff508780dd2d8110964320ab4851134a55ab102285b46e740f76a/patool-1.12-py2.py3-none-any.whl (77kB)\n",
      "Installing collected packages: patool\n",
      "Successfully installed patool-1.12\n"
     ]
    }
   ],
   "source": [
    "# # from google.colab import drive\n",
    "# # drive.mount('/content/drive')\n",
    "\n",
    "# # For extracting\n",
    "# !pip install pyunpack\n",
    "# !pip install patool\n",
    "\n",
    "# from pyunpack import Archive\n",
    "# Archive('CUB_200_2011.tgz').extractall('Assignment3_Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "0Z5GNnFsDPna",
    "outputId": "7c4ef0b1-3665-4550-fb02-e9e8453124fe"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from io import StringIO\n",
    "from PIL import Image\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "J6JD3ycyPuYi",
    "outputId": "6931b221-e6dd-4a22-802e-07cf60890298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DP8iACDGDSWD"
   },
   "outputs": [],
   "source": [
    "class DatasetClass(Dataset):\n",
    "    \n",
    "    def __init__(self, directory, img_size):\n",
    "        \n",
    "        self.directory = directory\n",
    "        classes = os.listdir(directory)\n",
    "        print('Number of Classes =', len(classes))\n",
    "        self.files = []\n",
    "        for class_name in classes:\n",
    "            images = os.listdir(directory + '/' + class_name)\n",
    "            images = [class_name + '/' + image for image in images]\n",
    "            self.files.extend(images)\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.size = len(self.files)\n",
    "        \n",
    "    def __getitem__(self, idx):     \n",
    "        \n",
    "        image_name = self.files[idx]\n",
    "        y = int(image_name[:3])-1\n",
    "        img = Image.open(self.directory + '/' + image_name).convert(mode='RGB').resize(self.img_size)\n",
    "        \n",
    "        trans = transforms.ToTensor()\n",
    "        # return trans(img), torch.Tensor(y, dtype=torch.long)\n",
    "        return trans(img)*255, y        # Multiplying by pixel value\n",
    "      \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TKVGwDDOKAsM"
   },
   "outputs": [],
   "source": [
    "def train_test_loader(directory, img_size, train_fraction=0.8, num_workers=0, batch_size=32):\n",
    "\n",
    "    dataset = DatasetClass(directory, img_size)\n",
    "    \n",
    "    N = dataset.size\n",
    "    train_size = int(N*train_fraction)\n",
    "    test_size = N - train_size\n",
    "\n",
    "    train_data, test_data = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    testloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    return trainloader, testloader, train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "fT66UkOpUDtv",
    "outputId": "2f4d634e-a40a-48e5-a352-a2615e238192"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes = 200\n"
     ]
    }
   ],
   "source": [
    "trainloader, testloader, train_size, test_size = train_test_loader('D:/_SEM8/DL/Assignment 3/Assignment3_Data/CUB_200_2011/images/', (224, 224))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     trainloader, testloader, train_size, test_size = train_test_loader('Assignment3_Data/CUB_200_2011/images/', (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "81_AjruYX-U6",
    "outputId": "9507fecd-15df-4d06-acc7-6b43a9aef2b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 295, 1 / 295, 2 / 295, 3 / 295, 4 / 295, 5 / 295, 6 / 295, 7 / 295, 8 / 295, 9 / 295, 10 / 295, 11 / 295, 12 / 295, 13 / 295, 14 / 295, 15 / 295, 16 / 295, 17 / 295, 18 / 295, 19 / 295, 20 / 295, 21 / 295, 22 / 295, 23 / 295, 24 / 295, 25 / 295, 26 / 295, 27 / 295, 28 / 295, 29 / 295, 30 / 295, 31 / 295, 32 / 295, 33 / 295, 34 / 295, 35 / 295, 36 / 295, 37 / 295, 38 / 295, 39 / 295, 40 / 295, 41 / 295, 42 / 295, 43 / 295, 44 / 295, 45 / 295, 46 / 295, 47 / 295, 48 / 295, 49 / 295, 50 / 295, 51 / 295, 52 / 295, 53 / 295, 54 / 295, 55 / 295, 56 / 295, 57 / 295, 58 / 295, 59 / 295, 60 / 295, 61 / 295, 62 / 295, 63 / 295, 64 / 295, 65 / 295, 66 / 295, 67 / 295, 68 / 295, 69 / 295, 70 / 295, 71 / 295, 72 / 295, 73 / 295, 74 / 295, 75 / 295, 76 / 295, 77 / 295, 78 / 295, 79 / 295, 80 / 295, 81 / 295, 82 / 295, 83 / 295, 84 / 295, 85 / 295, 86 / 295, 87 / 295, 88 / 295, 89 / 295, 90 / 295, 91 / 295, 92 / 295, 93 / 295, 94 / 295, 95 / 295, 96 / 295, 97 / 295, 98 / 295, 99 / 295, 100 / 295, 101 / 295, 102 / 295, 103 / 295, 104 / 295, 105 / 295, 106 / 295, 107 / 295, 108 / 295, 109 / 295, 110 / 295, 111 / 295, 112 / 295, 113 / 295, 114 / 295, 115 / 295, 116 / 295, 117 / 295, 118 / 295, 119 / 295, 120 / 295, 121 / 295, 122 / 295, 123 / 295, 124 / 295, 125 / 295, 126 / 295, 127 / 295, 128 / 295, 129 / 295, 130 / 295, 131 / 295, 132 / 295, 133 / 295, 134 / 295, 135 / 295, 136 / 295, 137 / 295, 138 / 295, 139 / 295, 140 / 295, 141 / 295, 142 / 295, 143 / 295, 144 / 295, 145 / 295, 146 / 295, 147 / 295, 148 / 295, 149 / 295, 150 / 295, 151 / 295, 152 / 295, 153 / 295, 154 / 295, 155 / 295, 156 / 295, 157 / 295, 158 / 295, 159 / 295, 160 / 295, 161 / 295, 162 / 295, 163 / 295, 164 / 295, 165 / 295, 166 / 295, 167 / 295, 168 / 295, 169 / 295, 170 / 295, 171 / 295, 172 / 295, 173 / 295, 174 / 295, 175 / 295, 176 / 295, 177 / 295, 178 / 295, 179 / 295, 180 / 295, 181 / 295, 182 / 295, 183 / 295, 184 / 295, 185 / 295, 186 / 295, 187 / 295, 188 / 295, 189 / 295, 190 / 295, 191 / 295, 192 / 295, 193 / 295, 194 / 295, 195 / 295, 196 / 295, 197 / 295, 198 / 295, 199 / 295, 200 / 295, 201 / 295, 202 / 295, 203 / 295, 204 / 295, 205 / 295, 206 / 295, 207 / 295, 208 / 295, 209 / 295, 210 / 295, 211 / 295, 212 / 295, 213 / 295, 214 / 295, 215 / 295, 216 / 295, 217 / 295, 218 / 295, 219 / 295, 220 / 295, 221 / 295, 222 / 295, 223 / 295, 224 / 295, 225 / 295, 226 / 295, 227 / 295, 228 / 295, 229 / 295, 230 / 295, 231 / 295, 232 / 295, 233 / 295, 234 / 295, 235 / 295, 236 / 295, 237 / 295, 238 / 295, 239 / 295, 240 / 295, 241 / 295, 242 / 295, 243 / 295, 244 / 295, 245 / 295, 246 / 295, 247 / 295, 248 / 295, 249 / 295, 250 / 295, 251 / 295, 252 / 295, 253 / 295, 254 / 295, 255 / 295, 256 / 295, 257 / 295, 258 / 295, 259 / 295, 260 / 295, 261 / 295, 262 / 295, 263 / 295, 264 / 295, 265 / 295, 266 / 295, 267 / 295, 268 / 295, 269 / 295, 270 / 295, 271 / 295, 272 / 295, 273 / 295, 274 / 295, 275 / 295, 276 / 295, 277 / 295, 278 / 295, 279 / 295, 280 / 295, 281 / 295, 282 / 295, 283 / 295, 284 / 295, 285 / 295, 286 / 295, 287 / 295, 288 / 295, 289 / 295, 290 / 295, 291 / 295, 292 / 295, 293 / 295, 294 / 295, "
     ]
    }
   ],
   "source": [
    "RGB_mean = np.zeros(3)\n",
    "i = 0\n",
    "for X, y in trainloader:\n",
    "    print(i, '/', len(trainloader), end=', ')\n",
    "    i += 1\n",
    "    RGB_mean += (X.sum(0).sum(1).sum(1)/(X.shape[2]*X.shape[2])).numpy()/train_size\n",
    "    \n",
    "RGB_mean = torch.from_numpy(RGB_mean).to(device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2902, 0.2941, 0.2745,  ..., 0.3451, 0.4118, 0.2157],\n",
       "          [0.2706, 0.2784, 0.2784,  ..., 0.3255, 0.3098, 0.3216],\n",
       "          [0.2667, 0.2784, 0.2784,  ..., 0.4000, 0.3490, 0.5098],\n",
       "          ...,\n",
       "          [0.6000, 0.6157, 0.6275,  ..., 0.7765, 0.7804, 0.7922],\n",
       "          [0.5961, 0.6039, 0.6118,  ..., 0.7804, 0.7804, 0.7922],\n",
       "          [0.6235, 0.6039, 0.6078,  ..., 0.7765, 0.7804, 0.7882]],\n",
       "\n",
       "         [[0.1725, 0.1765, 0.1647,  ..., 0.3804, 0.4196, 0.2039],\n",
       "          [0.1529, 0.1608, 0.1686,  ..., 0.3804, 0.3765, 0.3765],\n",
       "          [0.1490, 0.1686, 0.1686,  ..., 0.4431, 0.4353, 0.5804],\n",
       "          ...,\n",
       "          [0.6078, 0.6235, 0.6353,  ..., 0.7686, 0.7725, 0.7843],\n",
       "          [0.6039, 0.6118, 0.6196,  ..., 0.7725, 0.7725, 0.7843],\n",
       "          [0.6235, 0.6118, 0.6157,  ..., 0.7686, 0.7725, 0.7804]],\n",
       "\n",
       "         [[0.0235, 0.0275, 0.0118,  ..., 0.1333, 0.2627, 0.0667],\n",
       "          [0.0196, 0.0275, 0.0235,  ..., 0.0706, 0.0980, 0.1451],\n",
       "          [0.0078, 0.0235, 0.0235,  ..., 0.1765, 0.1529, 0.3922],\n",
       "          ...,\n",
       "          [0.4118, 0.4275, 0.4392,  ..., 0.7176, 0.7216, 0.7333],\n",
       "          [0.4078, 0.4157, 0.4235,  ..., 0.7216, 0.7216, 0.7333],\n",
       "          [0.4196, 0.4157, 0.4196,  ..., 0.7176, 0.7216, 0.7294]]],\n",
       "\n",
       "\n",
       "        [[[0.7176, 0.7137, 0.6980,  ..., 0.6510, 0.7333, 0.7804],\n",
       "          [0.7059, 0.6980, 0.6941,  ..., 0.6235, 0.6549, 0.7020],\n",
       "          [0.7020, 0.6980, 0.6941,  ..., 0.6235, 0.6588, 0.6941],\n",
       "          ...,\n",
       "          [0.2471, 0.6196, 0.5843,  ..., 0.4196, 0.4510, 0.4667],\n",
       "          [0.2549, 0.6118, 0.5961,  ..., 0.4235, 0.4510, 0.4706],\n",
       "          [0.2863, 0.6157, 0.5843,  ..., 0.4235, 0.4549, 0.4824]],\n",
       "\n",
       "         [[0.7529, 0.7490, 0.7412,  ..., 0.6627, 0.7294, 0.7686],\n",
       "          [0.7412, 0.7333, 0.7294,  ..., 0.6431, 0.6667, 0.7137],\n",
       "          [0.7373, 0.7333, 0.7216,  ..., 0.6392, 0.6706, 0.7059],\n",
       "          ...,\n",
       "          [0.1843, 0.6549, 0.6510,  ..., 0.4706, 0.5020, 0.5373],\n",
       "          [0.1922, 0.6471, 0.6510,  ..., 0.4745, 0.5137, 0.5333],\n",
       "          [0.2353, 0.6510, 0.6392,  ..., 0.4745, 0.5098, 0.5373]],\n",
       "\n",
       "         [[0.6784, 0.6745, 0.6745,  ..., 0.5804, 0.6510, 0.6941],\n",
       "          [0.6667, 0.6667, 0.6549,  ..., 0.5647, 0.5922, 0.6392],\n",
       "          [0.6627, 0.6667, 0.6510,  ..., 0.5725, 0.6039, 0.6392],\n",
       "          ...,\n",
       "          [0.1216, 0.4863, 0.4471,  ..., 0.2510, 0.2824, 0.3098],\n",
       "          [0.1294, 0.4784, 0.4510,  ..., 0.2627, 0.2706, 0.2902],\n",
       "          [0.1686, 0.4745, 0.4314,  ..., 0.2667, 0.3020, 0.3294]]],\n",
       "\n",
       "\n",
       "        [[[0.4078, 0.3490, 0.4000,  ..., 0.1843, 0.1451, 0.1137],\n",
       "          [0.4510, 0.4314, 0.4980,  ..., 0.2118, 0.2118, 0.1843],\n",
       "          [0.4510, 0.4549, 0.4941,  ..., 0.1725, 0.1882, 0.1725],\n",
       "          ...,\n",
       "          [0.0392, 0.0471, 0.0392,  ..., 0.0941, 0.0941, 0.0745],\n",
       "          [0.0392, 0.0471, 0.0392,  ..., 0.0980, 0.0863, 0.0745],\n",
       "          [0.0392, 0.0471, 0.0392,  ..., 0.1373, 0.1137, 0.1059]],\n",
       "\n",
       "         [[0.2863, 0.2196, 0.2588,  ..., 0.1333, 0.1137, 0.0902],\n",
       "          [0.3216, 0.2941, 0.3451,  ..., 0.1686, 0.1804, 0.1608],\n",
       "          [0.3176, 0.3059, 0.3412,  ..., 0.1412, 0.1569, 0.1490],\n",
       "          ...,\n",
       "          [0.0471, 0.0549, 0.0471,  ..., 0.0745, 0.0784, 0.0706],\n",
       "          [0.0471, 0.0549, 0.0471,  ..., 0.0784, 0.0706, 0.0588],\n",
       "          [0.0471, 0.0549, 0.0471,  ..., 0.1098, 0.0902, 0.0863]],\n",
       "\n",
       "         [[0.1647, 0.0902, 0.1176,  ..., 0.0588, 0.0627, 0.0353],\n",
       "          [0.1843, 0.1451, 0.1843,  ..., 0.0980, 0.1294, 0.1059],\n",
       "          [0.1725, 0.1529, 0.1725,  ..., 0.0667, 0.1059, 0.0941],\n",
       "          ...,\n",
       "          [0.0431, 0.0510, 0.0431,  ..., 0.0510, 0.0745, 0.0549],\n",
       "          [0.0431, 0.0510, 0.0431,  ..., 0.0549, 0.0667, 0.0471],\n",
       "          [0.0431, 0.0510, 0.0431,  ..., 0.0863, 0.0902, 0.0745]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.5490, 0.5529, 0.5569,  ..., 0.5529, 0.5647, 0.5686],\n",
       "          [0.5608, 0.5569, 0.5529,  ..., 0.5490, 0.5647, 0.5647],\n",
       "          [0.5608, 0.5529, 0.5490,  ..., 0.5608, 0.5725, 0.5725],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.5059, 0.5098, 0.5137,  ..., 0.4863, 0.4980, 0.5020],\n",
       "          [0.5176, 0.5137, 0.5098,  ..., 0.4824, 0.4980, 0.4980],\n",
       "          [0.5176, 0.5098, 0.5137,  ..., 0.4941, 0.5059, 0.5059],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.3961, 0.4000, 0.4039,  ..., 0.3765, 0.3882, 0.3922],\n",
       "          [0.4078, 0.4039, 0.4000,  ..., 0.3725, 0.3882, 0.3882],\n",
       "          [0.4000, 0.3922, 0.3922,  ..., 0.3843, 0.3961, 0.3961],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.5804, 0.7098, 0.7059,  ..., 0.2118, 0.2078, 0.4431],\n",
       "          [0.6392, 0.6667, 0.7216,  ..., 0.1647, 0.3686, 0.6510],\n",
       "          [0.4902, 0.5490, 0.5608,  ..., 0.2549, 0.5373, 0.7569],\n",
       "          ...,\n",
       "          [0.4392, 0.4510, 0.3882,  ..., 0.4510, 0.5961, 0.6196],\n",
       "          [0.4863, 0.4627, 0.3529,  ..., 0.4863, 0.6157, 0.6196],\n",
       "          [0.6314, 0.5333, 0.4157,  ..., 0.5255, 0.6235, 0.6431]],\n",
       "\n",
       "         [[0.7686, 0.8157, 0.7843,  ..., 0.2431, 0.2353, 0.5098],\n",
       "          [0.7961, 0.7529, 0.8039,  ..., 0.2118, 0.4510, 0.8039],\n",
       "          [0.5608, 0.5922, 0.6157,  ..., 0.3176, 0.6314, 0.8706],\n",
       "          ...,\n",
       "          [0.5412, 0.5373, 0.4784,  ..., 0.5020, 0.6706, 0.7137],\n",
       "          [0.5725, 0.5412, 0.4275,  ..., 0.5373, 0.6941, 0.7255],\n",
       "          [0.6941, 0.5686, 0.4392,  ..., 0.5804, 0.7098, 0.7373]],\n",
       "\n",
       "         [[0.7137, 0.8902, 0.9843,  ..., 0.2510, 0.2588, 0.4706],\n",
       "          [0.8039, 0.9137, 0.9176,  ..., 0.2118, 0.4157, 0.7843],\n",
       "          [0.7098, 0.8039, 0.8078,  ..., 0.2745, 0.6314, 0.8549],\n",
       "          ...,\n",
       "          [0.6863, 0.6824, 0.5098,  ..., 0.4353, 0.5529, 0.5882],\n",
       "          [0.7647, 0.7373, 0.5569,  ..., 0.4706, 0.5882, 0.5882],\n",
       "          [0.7529, 0.5882, 0.4314,  ..., 0.5294, 0.6275, 0.6510]]],\n",
       "\n",
       "\n",
       "        [[[0.5059, 0.5059, 0.5137,  ..., 0.5059, 0.5059, 0.5059],\n",
       "          [0.5137, 0.5098, 0.5176,  ..., 0.5059, 0.5059, 0.5059],\n",
       "          [0.5216, 0.5216, 0.5176,  ..., 0.5059, 0.5020, 0.5020],\n",
       "          ...,\n",
       "          [0.6078, 0.5529, 0.5843,  ..., 0.6745, 0.5882, 0.6549],\n",
       "          [0.5608, 0.5412, 0.5333,  ..., 0.8549, 0.4392, 0.7804],\n",
       "          [0.4824, 0.5098, 0.4235,  ..., 0.8275, 0.8706, 0.5569]],\n",
       "\n",
       "         [[0.5137, 0.5137, 0.5137,  ..., 0.5059, 0.5059, 0.5059],\n",
       "          [0.5137, 0.5098, 0.5098,  ..., 0.5059, 0.5059, 0.5059],\n",
       "          [0.5137, 0.5137, 0.5098,  ..., 0.5059, 0.5020, 0.5020],\n",
       "          ...,\n",
       "          [0.6039, 0.5333, 0.5451,  ..., 0.6745, 0.6196, 0.5451],\n",
       "          [0.5412, 0.4863, 0.4784,  ..., 0.9059, 0.4000, 0.7373],\n",
       "          [0.4980, 0.4863, 0.4275,  ..., 0.7647, 0.7922, 0.5137]],\n",
       "\n",
       "         [[0.4627, 0.4627, 0.4667,  ..., 0.4588, 0.4588, 0.4588],\n",
       "          [0.4667, 0.4627, 0.4627,  ..., 0.4588, 0.4588, 0.4588],\n",
       "          [0.4667, 0.4667, 0.4627,  ..., 0.4588, 0.4549, 0.4549],\n",
       "          ...,\n",
       "          [0.3529, 0.3059, 0.3373,  ..., 0.5255, 0.4667, 0.4902],\n",
       "          [0.2902, 0.2941, 0.3020,  ..., 0.7882, 0.2941, 0.6196],\n",
       "          [0.2588, 0.2588, 0.2627,  ..., 0.7020, 0.6627, 0.4431]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vc8N6oxxiwx0"
   },
   "source": [
    "### Question 1. a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xzGC9uLa9xum"
   },
   "outputs": [],
   "source": [
    "class VGGNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, RGB_mean):\n",
    "        super(VGGNet, self).__init__()\n",
    "        \n",
    "        self.RGB_mean = RGB_mean \n",
    "\n",
    "        self.c11 = nn.Conv2d(3, 64, 3, stride=1, padding=1)\n",
    "        self.c12 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
    "        self.p1 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        self.c21 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
    "        self.c22 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.p2 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        self.c31 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
    "        self.c32 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
    "        self.c33 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
    "        self.p3 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        self.c41 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n",
    "        self.c42 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.c43 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.p4 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        self.c51 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.c52 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.c53 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.p5 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        self.flat = nn.Flatten(1, -1)\n",
    "        self.fc1 = nn.Linear(7*7*512, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.out = nn.Linear(4096, 200)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(type(x), type(self.RGB_mean))\n",
    "\n",
    "        x = x - self.RGB_mean[None, :, None, None]\n",
    "#         x = x.to(dtype=torch.float)\n",
    "        x = self.p1(F.relu(self.c12(F.relu(self.c11(x)))))\n",
    "        x = self.p1(F.relu(self.c22(F.relu(self.c21(x)))))\n",
    "        x = self.p3(F.relu(self.c33(F.relu(self.c32(F.relu(self.c31(x)))))))\n",
    "        x = self.p4(F.relu(self.c43(F.relu(self.c42(F.relu(self.c41(x)))))))\n",
    "        x = self.p5(F.relu(self.c53(F.relu(self.c52(F.relu(self.c51(x)))))))\n",
    "        x = F.relu(self.fc2(F.relu(self.fc1(self.flat(x)))))\n",
    "        Z = self.out(x)\n",
    "\n",
    "\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EwuOkncsWXVy"
   },
   "outputs": [],
   "source": [
    "VGG_model = VGGNet(RGB_mean)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(VGG_model.parameters(), lr=0.001, momentum=0.9)\n",
    "VGG_model = VGG_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VUTJaWtqSqQj"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-3384e795694a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# Forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVGG_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Calculate Loss (Cross Entropy)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-12aa8a982d06>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc43\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc42\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc41\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc53\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc52\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc51\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1608\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1609\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1610\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1611\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1612\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "old_loss = np.inf\n",
    "\n",
    "max_epoch = 100\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        \n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        y_hat = VGG_model(X)\n",
    "        \n",
    "        # Calculate Loss (Cross Entropy)\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
    "    \n",
    "    if abs(running_loss-old_loss)/running_loss < 1e-5:\n",
    "        print('Converged')\n",
    "        break\n",
    "    \n",
    "    old_loss = running_loss\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "colab_type": "code",
    "id": "Ok3viz8oX4nv",
    "outputId": "97583237-8039-45b6-fa28-176139706f6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 392.4207763671875\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows  200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    ...  193  194  195  196  197  198  199\n",
       "0      0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "1      0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "2      0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "3      0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "4      0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
       "195    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "196    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "197    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "198    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "199    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "\n",
       "[200 rows x 200 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    y_test = []\n",
    "    y_test_pred = []\n",
    "\n",
    "    for data in testloader:\n",
    "\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        y_hat = VGG_model(X)      \n",
    "        test_loss += criterion(y_hat, y)\n",
    "        \n",
    "        y_test.extend(list(y.detach().cpu().numpy()))\n",
    "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
    "\n",
    "print('Test Loss =', test_loss.item())\n",
    "plt.imshow(confusion_matrix(y_test, y_test_pred), cmap='Reds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r_MY7HHT7GYv"
   },
   "source": [
    "### Question 1. b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZEvZrYhn7JuX"
   },
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, RGB_mean):\n",
    "    \n",
    "        super(GoogLeNet, self).__init__()\n",
    "        self.RGB_mean = RGB_mean\n",
    "        self.n_classes = n_classes\n",
    "        # Convolution\n",
    "        # 3x224x224\n",
    "        self.c1 = nn.Conv2d(3, 64, 7, stride=2, padding=3)\n",
    "        # 64x112x112\n",
    "        self.mp1 = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "        # Deep Convolution\n",
    "        # 64x56x56\n",
    "        self.c21 = nn.Conv2d(64, 64, 1, stride=1, padding=0)\n",
    "        # 64x56x56\n",
    "        self.c22 = nn.Conv2d(64, 192, 3, stride=1, padding=1)\n",
    "        # 192x56x56\n",
    "        self.mp2 = nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "        # Inception 3a\n",
    "        # 192x28x28\n",
    "        # P1\n",
    "        self.c3a1 = nn.Conv2d(192, 64, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c3a21 = nn.Conv2d(192, 96, 1, stride=1, padding=0)\n",
    "        self.c3a22 = nn.Conv2d(96, 128, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c3a31 = nn.Conv2d(192, 16, 1, stride=1, padding=0)\n",
    "        self.c3a32 = nn.Conv2d(16, 32, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp3a4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c3a4 = nn.Conv2d(192, 32, 1, stride=1, padding=0)\n",
    "\n",
    "        # Inception 3b\n",
    "        # 256x28x28\n",
    "        # P1\n",
    "        self.c3b1 = nn.Conv2d(256, 128, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c3b21 = nn.Conv2d(256, 128, 1, stride=1, padding=0)\n",
    "        self.c3b22 = nn.Conv2d(128, 192, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c3b31 = nn.Conv2d(256, 32, 1, stride=1, padding=0)\n",
    "        self.c3b32 = nn.Conv2d(32, 96, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp3b4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c3b4 = nn.Conv2d(256, 64, 1, stride=1, padding=0)\n",
    "\n",
    "        # 480x28x28\n",
    "        # MP\n",
    "        self.mp3 = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "        # Inception 4a\n",
    "        # 480x14x14\n",
    "        # P1\n",
    "        self.c4a1 = nn.Conv2d(480, 192, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c4a21 = nn.Conv2d(480, 96, 1, stride=1, padding=0)\n",
    "        self.c4a22 = nn.Conv2d(96, 208, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c4a31 = nn.Conv2d(480, 16, 1, stride=1, padding=0)\n",
    "        self.c4a32 = nn.Conv2d(16, 48, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp4a4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c4a4 = nn.Conv2d(480, 64, 1, stride=1, padding=0)\n",
    "\n",
    "        # Auxiliary 1\n",
    "        # 512x14x14\n",
    "        self.apa1 = nn.AvgPool2d(5, stride=3, padding=0)\n",
    "        # 512x4x4\n",
    "        self.ca1 = nn.Conv2d(512, 128, 1, stride=1)\n",
    "        # 128x4x4\n",
    "        self.flat1 = nn.Flatten(1, -1)\n",
    "        # (128x4x4)x1\n",
    "        self.fca1 = nn.Linear(2048, 1024)\n",
    "        self.a1drop = nn.Dropout(0.7)\n",
    "        self.a1out = nn.Linear(1024, self.n_classes)\n",
    "\n",
    "        # Inception 4b\n",
    "        # 512x14x14\n",
    "        # P1\n",
    "        self.c4b1 = nn.Conv2d(512, 160, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c4b21 = nn.Conv2d(512, 112, 1, stride=1, padding=0)\n",
    "        self.c4b22 = nn.Conv2d(112, 224, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c4b31 = nn.Conv2d(512, 24, 1, stride=1, padding=0)\n",
    "        self.c4b32 = nn.Conv2d(24, 64, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp4b4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c4b4 = nn.Conv2d(512, 64, 1, stride=1, padding=0)\n",
    "\n",
    "        # Inception 4c\n",
    "        # 512x14x14\n",
    "        # P1\n",
    "        self.c4c1 = nn.Conv2d(512, 128, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c4c21 = nn.Conv2d(512, 128, 1, stride=1, padding=0)\n",
    "        self.c4c22 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c4c31 = nn.Conv2d(512, 24, 1, stride=1, padding=0)\n",
    "        self.c4c32 = nn.Conv2d(24, 64, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp4c4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c4c4 = nn.Conv2d(512, 64, 1, stride=1, padding=0)\n",
    "\n",
    "        # Inception 4d\n",
    "        # 512x14x14\n",
    "        # P1\n",
    "        self.c4d1 = nn.Conv2d(512, 112, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c4d21 = nn.Conv2d(512, 144, 1, stride=1, padding=0)\n",
    "        self.c4d22 = nn.Conv2d(144, 288, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c4d31 = nn.Conv2d(512, 32, 1, stride=1, padding=0)\n",
    "        self.c4d32 = nn.Conv2d(32, 64, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp4d4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c4d4 = nn.Conv2d(512, 64, 1, stride=1, padding=0)\n",
    "\n",
    "        # Auxiliary 2\n",
    "        # 528x14x14\n",
    "        self.apa2 = nn.AvgPool2d(5, stride=3, padding=0)\n",
    "        # 528x4x4\n",
    "        self.ca2 = nn.Conv2d(528, 128, 1, stride=1)\n",
    "        # 128x4x4\n",
    "        self.flat1 = nn.Flatten(1, -1)\n",
    "        # (128x4x4)x1\n",
    "        self.fca2 = nn.Linear(2048, 1024)\n",
    "        self.a2drop = nn.Dropout(0.7)\n",
    "        self.a2out = nn.Linear(1024, self.n_classes)\n",
    "\n",
    "        # Inception 4e\n",
    "        # 528x14x14\n",
    "        # P1\n",
    "        self.c4e1 = nn.Conv2d(528, 256, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c4e21 = nn.Conv2d(528, 160, 1, stride=1, padding=0)\n",
    "        self.c4e22 = nn.Conv2d(160, 320, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c4e31 = nn.Conv2d(528, 32, 1, stride=1, padding=0)\n",
    "        self.c4e32 = nn.Conv2d(32, 128, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp4e4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c4e4 = nn.Conv2d(528, 128, 1, stride=1, padding=0)\n",
    "\n",
    "        # 832x14x14\n",
    "        # MP\n",
    "        self.mp4 = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "        # Inception 5a\n",
    "        # 832x7x7\n",
    "        # P1\n",
    "        self.c5a1 = nn.Conv2d(832, 256, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c5a21 = nn.Conv2d(832, 160, 1, stride=1, padding=0)\n",
    "        self.c5a22 = nn.Conv2d(160, 320, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c5a31 = nn.Conv2d(832, 32, 1, stride=1, padding=0)\n",
    "        self.c5a32 = nn.Conv2d(32, 128, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp5a4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c5a4 = nn.Conv2d(832, 128, 1, stride=1, padding=0)\n",
    "\n",
    "        # Inception 5b\n",
    "        # 832x7x7\n",
    "        # P1\n",
    "        self.c5b1 = nn.Conv2d(832, 384, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c5b21 = nn.Conv2d(832, 192, 1, stride=1, padding=0)\n",
    "        self.c5b22 = nn.Conv2d(192, 384, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c5b31 = nn.Conv2d(832, 48, 1, stride=1, padding=0)\n",
    "        self.c5b32 = nn.Conv2d(48, 128, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp5b4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c5b4 = nn.Conv2d(832, 128, 1, stride=1, padding=0)\n",
    "\n",
    "        # 1024x7x7\n",
    "        self.ap = nn.AvgPool2d(7, stride=1)\n",
    "        # 1024x1x1\n",
    "        self.drop = nn.Dropout(0.4)\n",
    "        self.flat = nn.Flatten(1, -1)\n",
    "        self.out = nn.Linear(1024, self.n_classes)\n",
    "\n",
    "    def forward(self, x, auxiliary=True):\n",
    "\n",
    "        x = x - RGB_mean(None, :, None, None)\n",
    "\n",
    "        # Layer 1\n",
    "        x = self.mp1(F.relu(self.c1(x)))\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.mp2(F.relu(self.c22(F.relu(self.c21(x)))))\n",
    "\n",
    "        # Layer 3a\n",
    "        x1 = F.relu(self.c3a1(x))\n",
    "        x2 = F.relu(self.c3a22(F.relu(self.c3a21(x))))\n",
    "        x3 = F.relu(self.c3a32(F.relu(self.c3a31(x))))\n",
    "        x4 = F.relu(self.c3a4(self.mp3a4(x)))\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "\n",
    "        # Layer 3b\n",
    "        x1 = F.relu(self.c3b1(x))\n",
    "        x2 = F.relu(self.c3b22(F.relu(self.c3b21(x))))\n",
    "        x3 = F.relu(self.c3b32(F.relu(self.c3b31(x))))\n",
    "        x4 = F.relu(self.c3b4(self.mp3b4(x)))\n",
    "        x = self.mp3(torch.cat((x1, x2, x3, x4), 1))\n",
    "\n",
    "        # Layer 4a\n",
    "        x1 = F.relu(self.c4a1(x))\n",
    "        x2 = F.relu(self.c4a22(F.relu(self.c4a21(x))))\n",
    "        x3 = F.relu(self.c4a32(F.relu(self.c4a31(x))))\n",
    "        x4 = F.relu(self.c4a4(self.mp4a4(x)))\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "\n",
    "        # Auxiliary 1\n",
    "        if auxiliary == True:\n",
    "            z1 = self.flat1(F.relu(self.ca1(self.apa1(x))))\n",
    "            z1 = self.a1out(self.a1drop(F.relu(self.fca1(z1))))\n",
    "        else:\n",
    "            z1 = None\n",
    "\n",
    "        # Layer 4b\n",
    "        x1 = F.relu(self.c4b1(x))\n",
    "        x2 = F.relu(self.c4b22(F.relu(self.c4b21(x))))\n",
    "        x3 = F.relu(self.c4b32(F.relu(self.c4b31(x))))\n",
    "        x4 = F.relu(self.c4b4(self.mp4b4(x)))\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "\n",
    "        # Layer 4c\n",
    "        x1 = F.relu(self.c4c1(x))\n",
    "        x2 = F.relu(self.c4c22(F.relu(self.c4c21(x))))\n",
    "        x3 = F.relu(self.c4c32(F.relu(self.c4c31(x))))\n",
    "        x4 = F.relu(self.c4c4(self.mp4c4(x)))\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "\n",
    "        # Layer 4d\n",
    "        x1 = F.relu(self.c4d1(x))\n",
    "        x2 = F.relu(self.c4d22(F.relu(self.c4d21(x))))\n",
    "        x3 = F.relu(self.c4d32(F.relu(self.c4d31(x))))\n",
    "        x4 = F.relu(self.c4d4(self.mp4d4(x)))\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "\n",
    "        # Auxiliary 2\n",
    "        if auxiliary == True:\n",
    "            z2 = self.flat(F.relu(self.ca2(self.apa2(x))))\n",
    "            z2 = self.a2out(self.a2drop(F.relu(self.fca2(z2))))\n",
    "        else:\n",
    "            z2 = None\n",
    "\n",
    "        # Layer 4e\n",
    "        x1 = F.relu(self.c4e1(x))\n",
    "        x2 = F.relu(self.c4e22(F.relu(self.c4e21(x))))\n",
    "        x3 = F.relu(self.c4e32(F.relu(self.c4e31(x))))\n",
    "        x4 = F.relu(self.c4e4(self.mp4e4(x)))\n",
    "        x = self.mp4(torch.cat((x1, x2, x3, x4), 1))\n",
    "\n",
    "        # Layer 5a\n",
    "        x1 = F.relu(self.c5a1(x))\n",
    "        x2 = F.relu(self.c5a22(F.relu(self.c5a21(x))))\n",
    "        x3 = F.relu(self.c5a32(F.relu(self.c5a31(x))))\n",
    "        x4 = F.relu(self.c5a4(self.mp5a4(x)))\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "\n",
    "        # Layer 5b\n",
    "        x1 = F.relu(self.c5b1(x))\n",
    "        x2 = F.relu(self.c5b22(F.relu(self.c5b21(x))))\n",
    "        x3 = F.relu(self.c5b32(F.relu(self.c5b31(x))))\n",
    "        x4 = F.relu(self.c5b4(self.mp5b4(x)))\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "\n",
    "        # Final Output\n",
    "        x = self.out(self.flat(self.drop(self.ap(x))))\n",
    "\n",
    "        return x, z1, z2\n",
    "\n",
    "    \n",
    "    def predict(self, x):\n",
    "\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = torch.argmax(y_hat, axis=1)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k-5ltA0Q2kdX"
   },
   "outputs": [],
   "source": [
    "classifier = GoogLeNet(200)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOH2pEviO6nE",
    "outputId": "56cd4478-398e-4a6e-c8d7-97ebd9996aa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Loss = 2500.9888887405396\n",
      "Epoch 2 : Loss = 2500.915831565857\n",
      "Epoch 3 : Loss = 2500.9091567993164\n",
      "Converged\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "old_loss = np.inf\n",
    "\n",
    "max_epoch = 500\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        \n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        y_hat, y_hat1, y_hat2 = classifier(X)\n",
    "        \n",
    "        # Calculate Loss (Cross Entropy)\n",
    "        loss_main = criterion(y_hat, y)\n",
    "        loss1 = criterion(y_hat1, y)\n",
    "        loss2 = criterion(y_hat2, y)\n",
    "\n",
    "        # Weighted Loss\n",
    "        loss = loss_main + 0.3*loss1 + 0.3*loss2\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
    "    \n",
    "    if abs(running_loss-old_loss)/running_loss < 1e-5:\n",
    "        print('Converged')\n",
    "        break\n",
    "    \n",
    "    old_loss = running_loss\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ATw2OTqQiJCq",
    "outputId": "855eda4f-9804-499b-8250-b236225a2ef8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 392.137939453125\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    y_test = []\n",
    "    y_test_pred = []\n",
    "\n",
    "    for data in testloader:\n",
    "\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        y_hat, _, _ = classifier(X)      \n",
    "        test_loss += criterion(y_hat, y)\n",
    "        \n",
    "        y_test.extend(list(y.detach().cpu().numpy()))\n",
    "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
    "\n",
    "print('Test Loss =', test_loss.item())\n",
    "plt.imshow(confusion_matrix(y_test, y_test_pred), cmap='Reds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s1RPoXsreDy4"
   },
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NuqwHPPlQavd"
   },
   "outputs": [],
   "source": [
    "class CNN2(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super(CNN2, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        # 3x224x224\n",
    "        self.cl1 = nn.Conv2d(3, 4, 3, stride=1, padding=1)\n",
    "        # 4x224x224\n",
    "        self.pl1 = nn.AvgPool2d(2, stride=2)\n",
    "        # 4x112x112\n",
    "        self.cl2 = nn.Conv2d(4, 16, 3, stride=1, padding=1)\n",
    "        # 16x112x112\n",
    "        self.pl2 = nn.AvgPool2d(2, stride=2)\n",
    "\n",
    "        # 16x56x56\n",
    "        self.flat = nn.Flatten(1, -1)\n",
    "        # (16x56x56)x1\n",
    "        self.out = nn.Linear(16*56*56, self.n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.pl1(F.relu(self.cl1(x)))\n",
    "        x = self.pl2(F.relu(self.cl2(x)))\n",
    "        x = self.out(self.flat(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = torch.argmax(y_hat, axis=1)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y4dEykLIhv7s"
   },
   "outputs": [],
   "source": [
    "classifier = CNN2(200)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "_3RX6TtEhMFp",
    "outputId": "a496fb8d-dbd5-4154-bd53-62ebac550bd7"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5c0c3068f979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9fc865d241b5>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mimage_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   1871\u001b[0m                 )\n\u001b[1;32m   1872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1873\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "old_loss = np.inf\n",
    "from IPython.display import clear_output\n",
    "\n",
    "max_epoch = 50\n",
    "losses = []\n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        \n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        y_hat = classifier(X)\n",
    "        \n",
    "        # Calculate Loss (Cross Entropy)\n",
    "        loss = criterion(y_hat, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()*len(X)/9430\n",
    "    \n",
    "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
    "    losses.append(running_loss)\n",
    "\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    if (abs(running_loss-old_loss)/running_loss < 5e-2) and epoch>=10 and running_loss<1000:\n",
    "        print('Converged')\n",
    "        break\n",
    "    \n",
    "    old_loss = running_loss\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "RskMmPeG5RAU",
    "outputId": "bccc9f1e-e705-481b-d84a-0017fb6e8340"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 39.00154113769531\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    y_train = []\n",
    "    y_train_pred = []\n",
    "\n",
    "    for data in trainloader:\n",
    "\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        y_hat = classifier(X)      \n",
    "        train_loss += criterion(y_hat, y)\n",
    "        \n",
    "        y_train.extend(list(y.detach().cpu().numpy()))\n",
    "        y_train_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
    "\n",
    "print('Train Loss =', train_loss.item())\n",
    "plt.imshow(confusion_matrix(y_train, y_train_pred), cmap='Reds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "s1_WEeod6Bg2",
    "outputId": "19208fbb-77ea-43fe-80c6-92b7ca64dc57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy = 0.9813361611876988 Train Precision = 0.9839351684742241 Train F1 = 0.9814279047682407\n"
     ]
    }
   ],
   "source": [
    "acc_tr = accuracy_score(y_train, y_train_pred)\n",
    "prec_tr = precision_score(y_train, y_train_pred, average='weighted')\n",
    "f1_tr = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "print('Train Accuracy =', acc_tr, 'Train Precision =', prec_tr, 'Train F1 =', f1_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4eAKVd07iASJ"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    y_test = []\n",
    "    y_test_pred = []\n",
    "\n",
    "    for data in testloader:\n",
    "\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        y_hat = classifier(X)      \n",
    "        test_loss += criterion(y_hat, y)\n",
    "        \n",
    "        y_test.extend(list(y.detach().cpu().numpy()))\n",
    "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
    "\n",
    "print('Test Loss =', test_loss.item())\n",
    "plt.imshow(confusion_matrix(y_test, y_test_pred), cmap='Reds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qJK9h5xu4DjA"
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "prec = precision_score(y_test, y_test_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "print('Test Accuracy =', acc, 'Test Precision =', prec, 'Test F1 =', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_vIZ1K6eE33v"
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jVC5hAh8E33x"
   },
   "outputs": [],
   "source": [
    "class CNN3(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, k=4):\n",
    "        super(CNN3, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        # 3x224x224\n",
    "        self.cl1 = nn.Conv2d(3, 4, 3, stride=1, padding=1)\n",
    "        # 4x224x224\n",
    "        self.pl1 = nn.AvgPool2d(2, stride=2)\n",
    "        # 4x112x112\n",
    "        self.cl2 = nn.Conv2d(4, 16, 3, stride=1, padding=1)\n",
    "        # 16x112x112\n",
    "        self.pl2 = nn.AvgPool2d(2, stride=2)\n",
    "\n",
    "        # NetVLAD\n",
    "        self.K = k\n",
    "        self.nv_conv = nn.Conv2d(256, k, 1)\n",
    "        self.nv_soft_ass = nn.Softmax2d()\n",
    "\n",
    "        # NetVLAD Parameter\n",
    "        self.c = nn.Parameter(torch.Tensor(self.K, 256))\n",
    "        \n",
    "        # Flatten to get h\n",
    "        self.flat = nn.Flatten(1, -1)\n",
    "\n",
    "        # Output layer\n",
    "        # self.out = nn.Linear()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        print(x.shape)\n",
    "        x = self.pl1(F.relu(self.cl1(x)))\n",
    "        print(x.shape)\n",
    "        x = self.pl2(F.relu(self.cl2(x)))\n",
    "        print(x.shape)\n",
    "        \n",
    "        # NetVLAD Step 1\n",
    "        a = self.nv_soft_ass(self.nv_conv(x))\n",
    "\n",
    "        # NetVLAD Step 2\n",
    "        for k in range(self.K):\n",
    "            a_k = a[:, k, :, :]\n",
    "            c_k = self.c[k, :]\n",
    "            temp = (x - c_k.reshape(1, -1, 1, 1))*a_k.unsqueeze(1)\n",
    "            z_k = torch.sum(temp, axis=(2, 3))\n",
    "            if k==0:\n",
    "                Z = z_k.unsqueeze(1)\n",
    "            else:\n",
    "                Z = torch.cat((Z, z_k.unsqueeze(1)), 1)\n",
    "        \n",
    "        # Flatten\n",
    "        Z = self.flatten(Z)\n",
    "        print(Z.shape)\n",
    "\n",
    "        return Z\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = torch.argmax(y_hat, axis=1)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2NO7TpyJE330"
   },
   "outputs": [],
   "source": [
    "classifier = CNN3(200)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "DbpAEbjRE332",
    "outputId": "389e13fd-cf44-4d80-eb35-9bf33196e84d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 4, 112, 112])\n",
      "torch.Size([32, 16, 56, 56])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-d8ed2172ed53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-92f1e63286ba>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# NetVLAD Step 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnv_soft_ass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnv_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# NetVLAD Step 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    348\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    349\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 350\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [4, 256, 1, 1], expected input[32, 16, 56, 56] to have 256 channels, but got 16 channels instead"
     ]
    }
   ],
   "source": [
    "old_loss = np.inf\n",
    "\n",
    "max_epoch = 500\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        \n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        y_hat = classifier(X)\n",
    "        break\n",
    "        \n",
    "        # Calculate Loss (Cross Entropy)\n",
    "        loss = criterion(y_hat, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    break\n",
    "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
    "    \n",
    "    if abs(running_loss-old_loss)/running_loss < 1e-5:\n",
    "        print('Converged')\n",
    "        break\n",
    "    \n",
    "    old_loss = running_loss\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lcw20TIlE334"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    y_test = []\n",
    "    y_test_pred = []\n",
    "\n",
    "    for data in testloader:\n",
    "\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        y_hat = classifier(X)      \n",
    "        test_loss += criterion(y_hat, y)\n",
    "        \n",
    "        y_test.extend(list(y.detach().cpu().numpy()))\n",
    "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
    "\n",
    "print('Test Loss =', test_loss.item())\n",
    "plt.imshow(confusion_matrix(y_test, y_test_pred), cmap='Reds')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Q1_Q2_Q3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
