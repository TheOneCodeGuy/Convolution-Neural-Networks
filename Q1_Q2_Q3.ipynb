{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_qTq010ijWD"
   },
   "source": [
    "### Run this cell only once (the first ever time you run) to process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "v8PCDpQkAHwN",
    "outputId": "f0a8a37c-61d2-47b2-ebe8-68a68bedd846"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyunpack\n",
      "  Downloading https://files.pythonhosted.org/packages/33/fd/4b64817a1d82df78553ceb1bfc5a2d7ac162da8667be586430fab9db5deb/pyunpack-0.2.1-py2.py3-none-any.whl\n",
      "Collecting entrypoint2 (from pyunpack)\n",
      "  Downloading https://files.pythonhosted.org/packages/46/0a/6156f1bc14a44094cff75bb6ecefe1f8e8a12cfff66379ba3d52d0916c49/entrypoint2-0.2.1-py2.py3-none-any.whl\n",
      "Collecting easyprocess (from pyunpack)\n",
      "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
      "Collecting argparse (from entrypoint2->pyunpack)\n",
      "  Downloading https://files.pythonhosted.org/packages/f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl\n",
      "Installing collected packages: argparse, entrypoint2, easyprocess, pyunpack\n",
      "Successfully installed argparse-1.4.0 easyprocess-0.3 entrypoint2-0.2.1 pyunpack-0.2.1\n",
      "Collecting patool\n",
      "  Downloading https://files.pythonhosted.org/packages/43/94/52243ddff508780dd2d8110964320ab4851134a55ab102285b46e740f76a/patool-1.12-py2.py3-none-any.whl (77kB)\n",
      "Installing collected packages: patool\n",
      "Successfully installed patool-1.12\n"
     ]
    }
   ],
   "source": [
    "# # from google.colab import drive\n",
    "# # drive.mount('/content/drive')\n",
    "\n",
    "# # For extracting\n",
    "# !pip install pyunpack\n",
    "# !pip install patool\n",
    "\n",
    "# from pyunpack import Archive\n",
    "# Archive('CUB_200_2011.tgz').extractall('Assignment3_Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "0Z5GNnFsDPna",
    "outputId": "92111e4f-891c-403c-d9d9-11079c7d5dbc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from io import StringIO\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "J6JD3ycyPuYi",
    "outputId": "7c7041f0-bd84-4eb0-ba6c-8a76bca5caa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DP8iACDGDSWD"
   },
   "outputs": [],
   "source": [
    "class DatasetClass(Dataset):\n",
    "    \n",
    "    def __init__(self, directory, img_size):\n",
    "        \n",
    "        self.directory = directory\n",
    "        classes = os.listdir(directory)\n",
    "        print('Number of Classes =', len(classes))\n",
    "        self.files = []\n",
    "        for class_name in classes:\n",
    "            images = os.listdir(directory + '/' + class_name)\n",
    "            images = [class_name + '/' + image for image in images]\n",
    "            self.files.extend(images)\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.size = len(self.files)\n",
    "        \n",
    "    def __getitem__(self, idx):     \n",
    "        \n",
    "        image_name = self.files[idx]\n",
    "        y = int(image_name[:3])-1\n",
    "        img = Image.open(self.directory + '/' + image_name).convert(mode='RGB').resize(self.img_size)\n",
    "        \n",
    "        trans = transforms.ToTensor()\n",
    "        # return trans(img), torch.Tensor(y, dtype=torch.long)\n",
    "        return trans(img), y\n",
    "      \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TKVGwDDOKAsM"
   },
   "outputs": [],
   "source": [
    "def train_test_loader(directory, img_size, train_fraction=0.8, num_workers=0, batch_size=32):\n",
    "\n",
    "    dataset = DatasetClass(directory, img_size)\n",
    "    \n",
    "    N = dataset.size\n",
    "    train_size = int(N*train_fraction)\n",
    "    test_size = N - train_size\n",
    "\n",
    "    train_data, test_data = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    testloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    return trainloader, testloader, train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fT66UkOpUDtv",
    "outputId": "8bdd99b8-5d89-4911-dc70-3bc85ddfb585"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes = 200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trainloader, testloader, train_size, test_size = train_test_loader('Assignment3_Data/CUB_200_2011/images/', (224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vc8N6oxxiwx0"
   },
   "source": [
    "### Question 1. a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xzGC9uLa9xum"
   },
   "outputs": [],
   "source": [
    "class VGGNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VGGNet, self).__init__()\n",
    "        \n",
    "        self.c11 = nn.Conv2d(3, 64, 3, stride=1, padding=1)\n",
    "        self.c12 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
    "        self.p1 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        self.c21 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
    "        self.c22 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.p2 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        self.c31 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
    "        self.c32 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
    "        self.c33 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
    "        self.p3 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        self.c41 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n",
    "        self.c42 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.c43 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.p4 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        self.c51 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.c52 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.c53 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.p5 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        self.flat = nn.Flatten(1, -1)\n",
    "        self.fc1 = nn.Linear(7*7*512, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.out = nn.Linear(4096, 200)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.p1(F.relu(self.c12(F.relu(self.c11(x)))))\n",
    "        x = self.p1(F.relu(self.c22(F.relu(self.c21(x)))))\n",
    "        x = self.p3(F.relu(self.c33(F.relu(self.c32(F.relu(self.c31(x)))))))\n",
    "        x = self.p4(F.relu(self.c43(F.relu(self.c42(F.relu(self.c41(x)))))))\n",
    "        x = self.p5(F.relu(self.c53(F.relu(self.c52(F.relu(self.c51(x)))))))\n",
    "        x = F.relu(self.fc2(F.relu(self.fc1(self.flat(x)))))\n",
    "        Z = self.out(x)\n",
    "\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EwuOkncsWXVy"
   },
   "outputs": [],
   "source": [
    "VGG_model = VGGNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(VGG_model.parameters(), lr=0.001, momentum=0.9)\n",
    "VGG_model = VGG_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "VUTJaWtqSqQj",
    "outputId": "d47217a3-94cd-4cb6-a479-4f6edfcc31fa"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-2c8b01d11f7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# Forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVGG_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Calculate Loss (Cross Entropy)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-355e19c71275>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc43\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc42\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc41\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc53\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc52\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc51\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1608\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1609\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1610\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1611\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1612\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "old_loss = np.inf\n",
    "\n",
    "max_epoch = 500\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        \n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        y_hat = VGG_model(X)\n",
    "        \n",
    "        # Calculate Loss (Cross Entropy)\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
    "    \n",
    "    if abs(running_loss-old_loss)/running_loss < 1e-5:\n",
    "        print('Converged')\n",
    "        break\n",
    "    \n",
    "    old_loss = running_loss\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "colab_type": "code",
    "id": "Ok3viz8oX4nv",
    "outputId": "97583237-8039-45b6-fa28-176139706f6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 392.4207763671875\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    ...  193  194  195  196  197  198  199\n",
       "0      0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "1      0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "2      0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "3      0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "4      0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
       "195    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "196    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "197    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "198    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "199    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "\n",
       "[200 rows x 200 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    y_test = []\n",
    "    y_test_pred = []\n",
    "\n",
    "    for data in testloader:\n",
    "\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        y_hat = VGG_model(X)      \n",
    "        test_loss += criterion(y_hat, y)\n",
    "        \n",
    "        y_test.extend(list(y.detach().cpu().numpy()))\n",
    "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
    "\n",
    "print('Test Loss =', test_loss.item())\n",
    "plt.imshow(confusion_matrix(y_test, y_test_pred), cmap='Reds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r_MY7HHT7GYv"
   },
   "source": [
    "### Question 1. b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZEvZrYhn7JuX"
   },
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "    \n",
    "        super(GoogLeNet, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        # Convolution\n",
    "        # 3x224x224\n",
    "        self.c1 = nn.Conv2d(3, 64, 7, stride=2, padding=3)\n",
    "        # 64x112x112\n",
    "        self.mp1 = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "        # Deep Convolution\n",
    "        # 64x56x56\n",
    "        self.c21 = nn.Conv2d(64, 64, 1, stride=1, padding=0)\n",
    "        # 64x56x56\n",
    "        self.c22 = nn.Conv2d(64, 192, 3, stride=1, padding=1)\n",
    "        # 192x56x56\n",
    "        self.mp2 = nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "        # Inception 3a\n",
    "        # 192x28x28\n",
    "        # P1\n",
    "        self.c3a1 = nn.Conv2d(192, 64, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c3a21 = nn.Conv2d(192, 96, 1, stride=1, padding=0)\n",
    "        self.c3a22 = nn.Conv2d(96, 128, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c3a31 = nn.Conv2d(192, 16, 1, stride=1, padding=0)\n",
    "        self.c3a32 = nn.Conv2d(16, 32, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp3a4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c3a4 = nn.Conv2d(192, 32, 1, stride=1, padding=0)\n",
    "\n",
    "        # Inception 3b\n",
    "        # 256x28x28\n",
    "        # P1\n",
    "        self.c3b1 = nn.Conv2d(256, 128, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c3b21 = nn.Conv2d(256, 128, 1, stride=1, padding=0)\n",
    "        self.c3b22 = nn.Conv2d(128, 192, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c3b31 = nn.Conv2d(256, 32, 1, stride=1, padding=0)\n",
    "        self.c3b32 = nn.Conv2d(32, 96, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp3b4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c3b4 = nn.Conv2d(256, 64, 1, stride=1, padding=0)\n",
    "\n",
    "        # 480x28x28\n",
    "        # MP\n",
    "        self.mp3 = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "        # Inception 4a\n",
    "        # 480x14x14\n",
    "        # P1\n",
    "        self.c4a1 = nn.Conv2d(480, 192, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c4a21 = nn.Conv2d(480, 96, 1, stride=1, padding=0)\n",
    "        self.c4a22 = nn.Conv2d(96, 208, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c4a31 = nn.Conv2d(480, 16, 1, stride=1, padding=0)\n",
    "        self.c4a32 = nn.Conv2d(16, 48, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp4a4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c4a4 = nn.Conv2d(480, 64, 1, stride=1, padding=0)\n",
    "\n",
    "        # Auxiliary 1\n",
    "        # 512x14x14\n",
    "        self.apa1 = nn.AvgPool2d(5, stride=3, padding=0)\n",
    "        # 512x4x4\n",
    "        self.ca1 = nn.Conv2d(512, 128, 1, stride=1)\n",
    "        # 128x4x4\n",
    "        self.flat1 = nn.Flatten(1, -1)\n",
    "        # (128x4x4)x1\n",
    "        self.fca1 = nn.Linear(2048, 1024)\n",
    "        self.a1drop = nn.Dropout(0.7)\n",
    "        self.a1out = nn.Linear(1024, self.n_classes)\n",
    "\n",
    "        # Inception 4b\n",
    "        # 512x14x14\n",
    "        # P1\n",
    "        self.c4b1 = nn.Conv2d(512, 160, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c4b21 = nn.Conv2d(512, 112, 1, stride=1, padding=0)\n",
    "        self.c4b22 = nn.Conv2d(112, 224, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c4b31 = nn.Conv2d(512, 24, 1, stride=1, padding=0)\n",
    "        self.c4b32 = nn.Conv2d(24, 64, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp4b4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c4b4 = nn.Conv2d(512, 64, 1, stride=1, padding=0)\n",
    "\n",
    "        # Inception 4c\n",
    "        # 512x14x14\n",
    "        # P1\n",
    "        self.c4c1 = nn.Conv2d(512, 128, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c4c21 = nn.Conv2d(512, 128, 1, stride=1, padding=0)\n",
    "        self.c4c22 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c4c31 = nn.Conv2d(512, 24, 1, stride=1, padding=0)\n",
    "        self.c4c32 = nn.Conv2d(24, 64, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp4c4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c4c4 = nn.Conv2d(512, 64, 1, stride=1, padding=0)\n",
    "\n",
    "        # Inception 4d\n",
    "        # 512x14x14\n",
    "        # P1\n",
    "        self.c4d1 = nn.Conv2d(512, 112, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c4d21 = nn.Conv2d(512, 144, 1, stride=1, padding=0)\n",
    "        self.c4d22 = nn.Conv2d(144, 288, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c4d31 = nn.Conv2d(512, 32, 1, stride=1, padding=0)\n",
    "        self.c4d32 = nn.Conv2d(32, 64, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp4d4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c4d4 = nn.Conv2d(512, 64, 1, stride=1, padding=0)\n",
    "\n",
    "        # Auxiliary 2\n",
    "        # 528x14x14\n",
    "        self.apa2 = nn.AvgPool2d(5, stride=3, padding=0)\n",
    "        # 528x4x4\n",
    "        self.ca2 = nn.Conv2d(528, 128, 1, stride=1)\n",
    "        # 128x4x4\n",
    "        self.flat1 = nn.Flatten(1, -1)\n",
    "        # (128x4x4)x1\n",
    "        self.fca2 = nn.Linear(2048, 1024)\n",
    "        self.a2drop = nn.Dropout(0.7)\n",
    "        self.a2out = nn.Linear(1024, self.n_classes)\n",
    "\n",
    "        # Inception 4e\n",
    "        # 528x14x14\n",
    "        # P1\n",
    "        self.c4e1 = nn.Conv2d(528, 256, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c4e21 = nn.Conv2d(528, 160, 1, stride=1, padding=0)\n",
    "        self.c4e22 = nn.Conv2d(160, 320, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c4e31 = nn.Conv2d(528, 32, 1, stride=1, padding=0)\n",
    "        self.c4e32 = nn.Conv2d(32, 128, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp4e4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c4e4 = nn.Conv2d(528, 128, 1, stride=1, padding=0)\n",
    "\n",
    "        # 832x14x14\n",
    "        # MP\n",
    "        self.mp4 = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "        # Inception 5a\n",
    "        # 832x7x7\n",
    "        # P1\n",
    "        self.c5a1 = nn.Conv2d(832, 256, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c5a21 = nn.Conv2d(832, 160, 1, stride=1, padding=0)\n",
    "        self.c5a22 = nn.Conv2d(160, 320, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c5a31 = nn.Conv2d(832, 32, 1, stride=1, padding=0)\n",
    "        self.c5a32 = nn.Conv2d(32, 128, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp5a4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c5a4 = nn.Conv2d(832, 128, 1, stride=1, padding=0)\n",
    "\n",
    "        # Inception 5b\n",
    "        # 832x7x7\n",
    "        # P1\n",
    "        self.c5b1 = nn.Conv2d(832, 384, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c5b21 = nn.Conv2d(832, 192, 1, stride=1, padding=0)\n",
    "        self.c5b22 = nn.Conv2d(192, 384, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c5b31 = nn.Conv2d(832, 48, 1, stride=1, padding=0)\n",
    "        self.c5b32 = nn.Conv2d(48, 128, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp5b4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c5b4 = nn.Conv2d(832, 128, 1, stride=1, padding=0)\n",
    "\n",
    "        # 1024x7x7\n",
    "        self.ap = nn.AvgPool2d(7, stride=1)\n",
    "        # 1024x1x1\n",
    "        self.drop = nn.Dropout(0.4)\n",
    "        self.flat = nn.Flatten(1, -1)\n",
    "        self.out = nn.Linear(1024, self.n_classes)\n",
    "\n",
    "    def forward(self, x, auxiliary=True):\n",
    "\n",
    "        # Layer 1\n",
    "        x = self.mp1(F.relu(self.c1(x)))\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.mp2(F.relu(self.c22(F.relu(self.c21(x)))))\n",
    "\n",
    "        # Layer 3a\n",
    "        x1 = F.relu(self.c3a1(x))\n",
    "        x2 = F.relu(self.c3a22(F.relu(self.c3a21(x))))\n",
    "        x3 = F.relu(self.c3a32(F.relu(self.c3a31(x))))\n",
    "        x4 = F.relu(self.c3a4(self.mp3a4(x)))\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "\n",
    "        # Layer 3b\n",
    "        x1 = F.relu(self.c3b1(x))\n",
    "        x2 = F.relu(self.c3b22(F.relu(self.c3b21(x))))\n",
    "        x3 = F.relu(self.c3b32(F.relu(self.c3b31(x))))\n",
    "        x4 = F.relu(self.c3b4(self.mp3b4(x)))\n",
    "        x = self.mp3(torch.cat((x1, x2, x3, x4), 1))\n",
    "\n",
    "        # Layer 4a\n",
    "        x1 = F.relu(self.c4a1(x))\n",
    "        x2 = F.relu(self.c4a22(F.relu(self.c4a21(x))))\n",
    "        x3 = F.relu(self.c4a32(F.relu(self.c4a31(x))))\n",
    "        x4 = F.relu(self.c4a4(self.mp4a4(x)))\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "\n",
    "        # Auxiliary 1\n",
    "        if auxiliary == True:\n",
    "            z1 = self.flat1(F.relu(self.ca1(self.apa1(x))))\n",
    "            z1 = self.a1out(self.a1drop(F.relu(self.fca1(z1))))\n",
    "        else:\n",
    "            z1 = None\n",
    "\n",
    "        # Layer 4b\n",
    "        x1 = F.relu(self.c4b1(x))\n",
    "        x2 = F.relu(self.c4b22(F.relu(self.c4b21(x))))\n",
    "        x3 = F.relu(self.c4b32(F.relu(self.c4b31(x))))\n",
    "        x4 = F.relu(self.c4b4(self.mp4b4(x)))\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "\n",
    "        # Layer 4c\n",
    "        x1 = F.relu(self.c4c1(x))\n",
    "        x2 = F.relu(self.c4c22(F.relu(self.c4c21(x))))\n",
    "        x3 = F.relu(self.c4c32(F.relu(self.c4c31(x))))\n",
    "        x4 = F.relu(self.c4c4(self.mp4c4(x)))\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "\n",
    "        # Layer 4d\n",
    "        x1 = F.relu(self.c4d1(x))\n",
    "        x2 = F.relu(self.c4d22(F.relu(self.c4d21(x))))\n",
    "        x3 = F.relu(self.c4d32(F.relu(self.c4d31(x))))\n",
    "        x4 = F.relu(self.c4d4(self.mp4d4(x)))\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "\n",
    "        # Auxiliary 2\n",
    "        if auxiliary == True:\n",
    "            z2 = self.flat(F.relu(self.ca2(self.apa2(x))))\n",
    "            z2 = self.a2out(self.a2drop(F.relu(self.fca2(z2))))\n",
    "        else:\n",
    "            z2 = None\n",
    "\n",
    "        # Layer 4e\n",
    "        x1 = F.relu(self.c4e1(x))\n",
    "        x2 = F.relu(self.c4e22(F.relu(self.c4e21(x))))\n",
    "        x3 = F.relu(self.c4e32(F.relu(self.c4e31(x))))\n",
    "        x4 = F.relu(self.c4e4(self.mp4e4(x)))\n",
    "        x = self.mp4(torch.cat((x1, x2, x3, x4), 1))\n",
    "\n",
    "        # Layer 5a\n",
    "        x1 = F.relu(self.c5a1(x))\n",
    "        x2 = F.relu(self.c5a22(F.relu(self.c5a21(x))))\n",
    "        x3 = F.relu(self.c5a32(F.relu(self.c5a31(x))))\n",
    "        x4 = F.relu(self.c5a4(self.mp5a4(x)))\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "\n",
    "        # Layer 5b\n",
    "        x1 = F.relu(self.c5b1(x))\n",
    "        x2 = F.relu(self.c5b22(F.relu(self.c5b21(x))))\n",
    "        x3 = F.relu(self.c5b32(F.relu(self.c5b31(x))))\n",
    "        x4 = F.relu(self.c5b4(self.mp5b4(x)))\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "\n",
    "        # Final Output\n",
    "        x = self.out(self.flat(self.drop(self.ap(x))))\n",
    "\n",
    "        return x, z1, z2\n",
    "\n",
    "    \n",
    "    def predict(self, x):\n",
    "\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = torch.argmax(y_hat, axis=1)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k-5ltA0Q2kdX"
   },
   "outputs": [],
   "source": [
    "classifier = GoogLeNet(200)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  12:28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOH2pEviO6nE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Loss = 2500.9888887405396\n",
      "Epoch 2 : Loss = 2500.915831565857\n",
      "Epoch 3 : Loss = 2500.9091567993164\n",
      "Converged\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "old_loss = np.inf\n",
    "\n",
    "max_epoch = 500\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        \n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        y_hat, y_hat1, y_hat2 = classifier(X)\n",
    "        \n",
    "        # Calculate Loss (Cross Entropy)\n",
    "        loss_main = criterion(y_hat, y)\n",
    "        loss1 = criterion(y_hat1, y)\n",
    "        loss2 = criterion(y_hat2, y)\n",
    "\n",
    "        # Weighted Loss\n",
    "        loss = loss_main + 0.3*loss1 + 0.3*loss2\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
    "    \n",
    "    if abs(running_loss-old_loss)/running_loss < 1e-5:\n",
    "        print('Converged')\n",
    "        break\n",
    "    \n",
    "    old_loss = running_loss\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ATw2OTqQiJCq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 392.137939453125\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAefklEQVR4nO2df5AV1ZXHv4cZlVoYl18ygAyBGALBDQ44Qn5amGwSRAWJiQuVUjZrCWxCGVJapZBUNCkTk03w1+4qaqQkKQNhJURUQsIajUWJDDOA+IMfQUUYBwdBVlFjdGbO/tH99M2b9153v3u7+/R951M1NW/6dZ97uu+d0+f+OoeYGYqiVC990lZAUZR0USOgKFWOGgFFqXLUCChKlaNGQFGqHDUCilLlxGYEiGg6Ee0lov1EdF1c5SiKYgbFsU6AiGoA7APwJQBtALYBmMvMz1svTFEUI+LyBKYA2M/MLzLzewBWA5gVU1mKohhQG5Pc0wEcyvu7DcDUUicPGTKYR48aFZMqGaSrE6iJq2pSKvftN4F+pxqJ6Nj5DACgvvGTNjTKBl2d3m8L9dK6Y+dRZj6t8HhcLY2KHOvR7yCi+QDmA8Cohga0bH48JlUUCfCJ10F1g4xkdG3bCACoOWe6DZVSpfvgbvQZ9YnA8/jE6wBg/OwAgPoNeLnY8bi6A20AGvL+HgmgPf8EZr6bmZuYuem0IYNjUiO75CpfMlF05OMd5uXt2ALescVYjgRoYH248+oGWTEA5YjLCGwDMJaIxhDRyQDmAFgfU1lKldDZsgudLbvSVsMKcf9jRyGW7gAzdxLRIgB/BFADYAUzPxdHWRKw4erGKS8uoujIHS8DIdzfcpx00Qyj66uRMG0plinCqDRNnsQ6JmCXJAxJuTLiKP//zj8XADDgD09YlVstUL8BrczcVHhcVwwqSpWTwjyUkgRJdCfKlVH4nQ3PoP/UcUbXK8VRT0BJhO7H1waeEzTbQBdcArrgElsqKT7qCSgVY7vfHziAVf8Ra2VlBZvrBEqhRkCpmEgNc1hD8DkBdK+4GQDQ54Z7jGVlhSS6daK6A1lYIJMUzj2LVw8FnhLYHTh7CujsKbY0ShVJ9SvKCES1epIepG2ysE4gCn2mBfflg+6ZW5vBrc2WNEoXSfUryghE/aeW9CCV8oQZGAyis/0oOtuPWtAmfSS9wJwZE0hiAEUJT+GgYc1FVxrLrB0xxFiG0htRnoDJP3ASGy2UnpR7mxVbJ2DKgfXbcWD9dmM5EgjbVvnE67F7DaKMgJItohjdrlW3BZ4T1NhHNo3CyKbqijuRxMvNme6Aa2RlE1FYaPjI4HMC7vfdQ8cAAH2taKTkUE9AUaocNQJCcckLAGBlsZASD6K6A1FdYNdc5hxhQ0+VI+2txIXYWPJb963LjGUovRHlCURttC4aAAAlDUCUUeK0dxEWwh1Fw9tFYutVt2LrVbcay5GArhNQKkKa0UvaE/jUXx4wliEFPt4hpj5FeQKSrGPWkfYswwbWLEf3xtXo3rjagjZKPhUbASJqIKLHiGg3ET1HRN/xj99ARK8Q0U7/RwPDpYCUt0yO7j3ma/7fWvcY3lr3mAVt0sd0zMcmJt2BTgBXM/N2IqoD0EpEm/zvbmHmX0QVKK3hKuVJur76zz4v0fIk0H1wN4B4jUbFngAzH2bm7f7nEwB2w8s8pFQJUbocfcabbwF2yRMI++z6jPpE7F6DlTEBIhoNYBKArf6hRUS0i4hWENHAsHJyVk9xj65l1xrLOPWuu3DqXXdZ0CZ9bHhRtsZ9jI0AEfUHsBbAYmZ+E8CdAM4A0AjgMIBlJa6bT0QtRNTy2lFvOaiNwSMXSGLTSNLUWogG9N6NS/HejUvNlXEEW90xIyNARCfBMwD3M/PvAICZO5i5i5m7AdwDL0NxL4qlIdMxAQ8XdkQWGrGuh8yNwPZN+7B90z5jORKQ5PWazA4QgHsB7Gbmm/OOD887bTaAZytXT1GUuDGZHfgsgMsAPENEO/1jSwHMJaJGeFmIDwBYYKShUhE2lh4HESnvwOE24/Km3r7YWIYUJHV9KzYCzLwZxVOQb6hcHcUWkuahbfHgv/8cAPBVC1GK0kZSd0/UsmGTTS9JzKcmRVZCpUWpr5q53zGWN/MHc0PrVg3Y2iQmygiY4MI/fw6qG5SJ2YFIG4hCrJUPlNfeHro86dj4B7b1khBlBKS/+ZLEtWdhow/81ta9AIABxpKUfERtIFLchY93BJ8T4P30bRiMvg2DbamUKpKMvChPQMkWkbYSh/AEgmRpyPF4UE9AIJIWkpQjzNs9RxhjEeQJuBRyXNKYjxoBgWRlkDOKnjZWDA6p74ch9f2M5Sg90e6AQFyInVh4D3TW5wKvCbpnl7YS29pAZEOOegJKxSTt0tLwkaHyF2QBG89OxAYiRVGyjygjIGmwJE2y0hWItHdAow33QFIdixoTkPRgFHlMGO/GGoEoJLGEXJQnoHjY8IiS8KqSnsrsP/s8K4ODEjzOsM8uidgSagQEYqPSk/Cqkp7K7GzZhc6WXcZyJHickqaBRRkBCRZaCU+k+nr1kHF5tSOG6KrBGNAxAaG4sFYgnz7TLjEXMmKEuQylF6I8AcUjKwagnI6FXoKN5CMuIcnrFWUEJD0Y14l78LHQQNjIRWhrTCBLJBF5WpQRyMLbLwmoblDsI+9Wlq1G2EDET282Lq+2aSJqmyYay5FA6N2XCcwOGI8JENEBACcAdAHoZOYmIhoE4LcARsMLNnopMx83LauakDR6XIpIOg5rMC4v5wXUzjcWpeRhyxM4j5kbmbnJ//s6AI8y81gAj/p/KwkirWtVc850YxmL79+Gxfdvs6BN+kiqn7i6A7MArPQ/rwRwcUzlOItpI5HWterattFYxjXj6nHNODmhuk2QVD82jAAD+BMRtRJRzlGrZ+bDgJe4FMDQwouKpSFTFCV5bKwT+CwztxPRUACbiGhPmIuY+W4AdwNA0+RJbEEPp5D0pihFlKlM3rEFMOwSjPnhQqPrJSFpGtjYE2Dmdv/3EQDr4OUe7MilI/N/HwklS1A/KQq29c7Kc4jSiPtMn2Nc3vsPbcD7D7mR2ybszEq5KUIRWYmJqB8R1eU+A/gyvNyD6wHM80+bB+DBUPKEWMao2NY7K88hSiPsXnFz8EkBuDRFGHZmpdwUoY24jYB5d6AewDovNylqAfyGmTcS0TYAa4joCgAHAXzdsBwl49DZRZNTR6IapwjLbSUO06UIYyiMjAAzvwjgrCLHjwH4YmR5gvpJil3C7B0Iqv9q3DwUJXBLpegGIqVigvYO5H8fJg1ZYHkWvAmlN6KWDSseWRkYLEevvQMWko9gWIOVlYdZpZJ2EeYaNQJKInStus1YBj+yFvzIWgvaZJNKPKkw14gyAlnJvJMELngD+dgIFd7ZfhSd7UctaJMdkthFKGpMIAubZpJAUmIKa2VYcONPumiGsQwphH12UcZdKkWUJ6B4SEpMUY4oW4n7jA8e1Au672oMOV7OE9DkI4qiWEFUd0DxoLpBmVgzEaX71r2nOXA7cdD9Tl2zLHR5rpBI1OjYS1AqQroBAJIfyOUdW7yNSIpV1AgoFRPFE7ARY1DpiYgNREp1E6UR2shF6FKgUUmDvzomoFRMpEZoIfnIKcvuNZYhBRvLqG3hjCeQxKIKpSflnnfhd3TW54zLe+PSi/HGpW5EqpO0JsYZI5BEaOakyIoxi7LDzUY8gb4Ng9G3ofoyE8eNM0bAJaIswqkm3j10DO8eciMepaQl8jomIBBJrqItbGwD7j91nAVNZCCpjtUIKIlQc9GVxjJym4e00Xro3gElU9jIO+ASTkwREtE4eKnGcnwUwA8ADABwJYDX/ONLmdmNELGK4iAVGwFm3gugEQCIqAbAK/BCjn8TwC3M/IvIMjOwXl5Jj9ygYN+U9bCBpHZuqzvwRQAvMLPRsjBJD0YJJul8C79sPoRfNpsvOnIFW8/f1hjLHACr8v5eRESXA2gBcHWxjMR+yrL5ADCqoXrjxpUiC15RFP3C7B0IkrfoksbQ5blCuZDjYuIJENHJAGYC+B//0J0AzoDXVTgMoOj+T2a+m5mbmLnptCG6AKQQ0wpOYsFRpL0DT282Lu+ki2Y4E10o7LNLYhGcje7A+QC2M3MHADBzBzN3MXM3gHvgpSULRVZWysWNpJHjcnTvaS75XeE9hM07UI5qjCyUBDa6A3OR1xUgouG5jMQAZsNLSxYKSQ9GCaZckJBey4YtBBWZ/KWPh1dOCY2RESCifwDwJQAL8g7/BxE1wktZfqDgOyUELhrDMDEGg2hrOQgA+JixJCUf0zRk7wAYXHDsMiONFHQf3C1qWakNwngCQYyeOdmSNko+zqzALDeKqjjCiBFpa+AkzhgBl/75XfMCADvdARsJTJTeiNo7oLMD7tL9uHn6sJeuX46Xrl9uQZv0kdTWRRkBRVGSR1R3wCWXXumJjfBiQ+r7WdBEBpLauihPQJKLlCZZeQ5Z0VMpjygjIMk6KsFEqa8wg51BRuVox9s42vF26DIlY8OASttApFjEBWNYuAHKxtqHkU2jTNVyitSDiijxkYUdhEHEoX81rhhMYv2LqO6A4pFLSBonktzRsBw5/i6OHH830TLjIuw/dRK7CNUTEErcFW9DfhQZ/PRmIKA7ECRvwnjdch4HagSUionUzx9mHjjmH9f83lhG1tDugCKaSAN9IXIRBnUvulbdhq5Vt4UvUwmFGgElEU7c8evAc7I+GBoHWYksZA1dfJItotTXqXfdFaMmigmijICiKMkjygioO2gPaV4VdxhFowcAdLbsQmfLLgvaKPno7ICjJGFQI5URcmCwnMyTv/+T8OUJR1L0qFCeABGtIKIjRPRs3rFBRLSJiP7q/x7oHyciup2I9hPRLiLSmFARkfYWt0KIKcIgo/LejUvx3o1LbWmk+ITtDtwHoDBA3HUAHmXmsQAe9f8GvBDkY/2f+fDyECgRcLFbFCb5SJDxa2s5+MHS4awjxQsAQhoBZn4CQGENzQKw0v+8EsDFecd/xR5PARhARMNDlePiG1ABEK7RBxm/kU2jnNlEJKmtmwwM1ufyC/i/h/rHTweQ3wFs848F4uIbUPGwkZrcJU9AEnEMDFKRY9zrJM1FmHmiDG7ZCDTqihcgDRNPoCPn5vu/j/jH2wDk/1ePBNBeeLHmIiyNJFexHLb7tUH3Xds0EbVNE62WmRaSvF4TI7AewDz/8zwAD+Ydv9yfJfgUgDfy0pIpIbDRQJIwJN0Hd4c+l493GJfXfNMDaL7pAWM51USYdhCqO0BEqwBMAzCEiNoAXA/gpwDWENEVAA4C+Lp/+gYAMwDsB/AOgG9GUViShUwT02eRxHOM4gnQwPrgcwJ0nnr74tDlSSepth6mDGLu1V1PnKbJk7hl8+Npq6FEJEpDttHo908+GwDwse2tRnKqFeo3oJWZmwqPi1o2rChK8qgRcBRpg4s2MhCNXnghRi+80II26SOpfnTvgFCyMCYQqQwLkYXQ3muSKbNIGvtST8BRJL1plN5Iqh/1BARiY4eZpDcNYGexUGf7UQBuNFpJ9ePC83QOSZtLJPHuoWMAgL4p6+EaoroDklykNHHxOdgYGKz70VLU/ciNrcSS6liUEZDkIqVJEslH4iYO/an+I6G2JGcBSW1duwNKLBQ28j7TLjGW+eaCBQCAAX94wlhW2khaHSvKE1A+JO4GkrSn0bXs2kTLk44UAwAI8wQkWUfXSfo50wXmnsAvm70wFdcYS1LyEWUElGwRZSrTRl9+8U2XG8uQgqQXnnYHFKXKEeUJSLGMLpDEmybM9mCr5U36dKLlVQvqCQjFdOBO2t6B7o2rg88JCFLy0oLv4aUF3wtdpmQkvfBEeQKS+klp4uJzoOEjA88JGl/QGIPxIMoIuNbwKyUrzyGKsaKzPmdcXu2IIcYyskbOI4yzTYjqDmR9lVy1EaVh2hg/6Gw/+sEmoqwjqa0HGoESKch+TkR7/DRj64hogH98NBH9jYh2+j/LoyhjYu34xOuiHqwpLt0LEG7vQNCYwPZN+7B90z5bKqVKaA+qblDsnmEYT+A+9E5BtgnAPzHzRAD7ACzJ++4FZm70fxbaUTOYJB5WkpjeizQjEmbZcNCYwITxgzFhvIant02gESiWgoyZ/8TMnf6fT8HLLaAIIgmDGCXkePeeZuPynt9zDM/vOWYsR+mJjTGBfwPwh7y/xxDRDiL6CxF93oL8qiTuN3kx+VHLLNfP7yUrRGryIKYs+RqmLPmasZx80vKYJHlqRrMDRPQ9AJ0A7vcPHQYwipmPEdHZAH5PRGcy85tFrtU0ZGWIfaFPEflRyyx3fuF3fLgtkuxidLbsAgDUzjcW9QFpdSEldV0r9gSIaB6ACwF8g/3kBcz8d2Y+5n9uBfACgI8Xu17TkJVG0luiHFH07DN9jnF5mpA0HioyAkQ0HcC1AGYy8zt5x08johr/80cBjAXwog1FFUWJh8DuQIkUZEsAnAJgExEBwFP+TMC5AH5ERJ0AugAsZOZsvNYEIclVLIdtPYMWH+mKwXgINALMPLfI4XtLnLsWgHkwOSXzS4cL9eeOl4GAKcCg+82tEfiMuXpKHqKWDSsfkgUDUM5Q9QovZiHk+JR5+u8fB6KWDSv2SGJwMdIuQgvRhumCS6xEKFJ6op6AoyThSSS9gcjGWgOlN5n2BPLfdq7tHVB6s/WqW7H1qlvTVsMKUVZbxk2mPYH8t1AW+tCukfQzd2lMIOmoTOXItCeQj0uegI37SOJZRCmDn95sXF7zyifRvPJJYzkSkPTScsYIuLSL0MZ9SHsW3Gq+gWjowL4YOrC6MhGWe7nZMvTOGAGXcMWjyafm6p+lrYIowtZxuZebLUOf6TEBV5H2Fi9FpCnCPc2oOacwLEU0xtz1Y6PrJWGjjm0tKFNPQFGqHFFGwEU32GWiTHPZyEDkUshxGzjZHeDjHZlxhZXgcGD5hNk7EMTomZONrleKI8oTkDR3qgSTtCfg0hRh17aNxjJsec6iPAHFIys7CCN5Ak9vNvYEpt6+2Oh6SdjYUGWrjcjyBDLQ8JX04MNtVsKUScBG4FVbiDICioeTxnCYeRzJRd+9F4u+WzSUReYwnS61iTPdgVz/NIqLKpmsdAnCYmNM4NZvnGNBExl0H9wtpq2KMgImDd+1QUWXDIAtTv7+T9JWwRpSDABQeRqyG4jolbx0YzPyvltCRPuJaC8RfSWKMiYN36W9A1khyuwAd7xsXB53vGxFjtKTStOQAcAteenGNgAAEU0AMAfAmf41d+SiD4dBFwt5SNprXimFdWmjO8A7toB3bDGWI4GwbT2J3bEVpSErwywAq/38Ay8B2A8g9FyIvsk9JLmK5SinZ2Fd2uiuvbXuMby17rGKrs3qC0ZKQtJSLPKzEq8gooH+sdMB5MeAavOPKRHISoONFE/geIdxeX9+5lX8+ZlXK7pW2gtGkj6VGoE7AZwBoBFe6rFl/nEqci4XE0BE84mohYhaXjuqSSYVJS0qMgLM3MHMXczcDeAefOjytwHInxAeCaC9hIxeaciy8gaMG0lviXJE0dPGgN7EoXWYOLTOWE6WEDEmUAwiGp7352wAuZmD9QDmENEpRDQGXhoyOUujlNTgR8xDjo9sGlV1WYiSGBOoNA3ZNCJqhOfqHwCwAACY+TkiWgPgeXjZir/NzF1hlcnKG1DxiBRyPES+gCB5tU0TQ+smHUmLwaymIfPP/zEAd0LAKCWJsvWbH1kLBCyVDZKVmxkYMP+H4RR0gFxXIE6DIWrFoAlJPKykyMq9RJnKrL3hHuPy6r51mbEMKYT2oCK2gUrajjNGQPo/TFSycD9RXFob7u+JO34NABhw0ZVGclymkmfsjBFwiSwYACBid8BC1Kj+s88zul4SksYEdCuxUjFJr2w8sPxhHFj+cKJlxoUUAwCoJ1C1JP0mshFjcMwPF1rSRslHlBGQ5CK5TuLP2UJGYVeiCklDVHdADYBSLUjaKSrKCCiKkjyijIDuHcgWUd5mNTqt1wNJ28VFGQHtDtgjCYMapSHbiLPf2bILnS27jOUoPRE1MKjYIwmDmvRArkt7B8KSxOpRUZ6AdgeyRZRAIWGSbQTVf/NND6D5pgdCl+kCInYRKkopImUgCrFiMOj7KUu+Fro8JTyiPAEdE3CX7o2rjWWYxBhUSqOegFIxkeIJDB9pXF7/qeOMZSi9EeUJ6JiAPZJ4luUMQGH5NqYI39q6F29t3WssR+mJKCOg3QF7JDU7ELb8zruvNy6v/9Rx6g3EgCgjoNgjbU+g17mTPm1cXvPKJ9G88kljORKQ5PVWmobst3kpyA4Q0U7/+Ggi+lved8ujKCNpPXWa2Ggg0ryqMFOEQUwYPxgTxg+2oE36SKqfMAOD9wH4LwC/yh1g5n/JfSaiZQDeyDv/BWZutKWgoijxYpSGjIgIwKUAVllRRtB66qwjyd0EgL9ffUXgOUE6133rMmfiDEqqH9Mxgc8D6GDmv+YdG0NEO4joL0T0eUP5VYkNV1GSuxmWIJ1P3PHrD+IMKvYwXScwFz29gMMARjHzMSI6G8DviehMZn6z8EIimg9gPgCMamgo/FpxjJO//xNjGc/v8dLVfcZYUvpIMtIVewJEVAvgqwB+mzvmZyM+5n9uBfACgI8Xu75YGjLlQyS5i6VIeiDXpYFBSZh0B/4ZwB5m/iDmExGdRkQ1/uePwktD9qKZiopUoozh2Fg23LdhMPo2qBHIYetFEWaKcBWALQDGEVEbEeVGeOag94DguQB2EdHTAB4AsJCZQ2uahbdfUkhyF0sRpb5q5n7HuLztm/Zh+6Z9xnIkIGkauNI0ZGDmfy1ybC2AijNPZqHhu4KNWABRru9adRtqqyh9WBCS2rpuIBJK3AE7km6ENjYQDR3Y14ImSiFqBIQi6U1hgz7TgrMSB+FS3gFJ4fV174BAbPQXpY2vdC271lgGtzaDW5staJM+UgwAoJ6ASFxcLFRz9c+MZdz6n/8LALjmBmNRSh7qCShKlaOegEAk9Rdt0b2nGTXnTDeSsfimyy1po+QjyhOQ1o9Ni6wYgCj1xTu2mBfY3u79OICkti7KE8hK41ei02f6HHMhI0aYyxCCpLYuyhNQskWkyEID643LO7D8YRxY/rCxHFdIbNmwotjAxhTh6JmTMXrmZAvauEFiy4YVpRRRBjBtTBF2th8FoI3WNuoJKB8Q1b2MEnI8TGShakIHBpVA0pgmjFpe98HdJbcTF8qyEVTkpItmGMtQeqOegFAkjR6XIspgH3e8bFze+w9twPsPbTCWIwFJ9SvKCEhykdImC8/CdlbiINpaDqKt5aCxHAlIql9RRsDEOvKJ10U9WFMkvSlKESWykJXZgYUXYvTCC43lSEBS/YoyAoqiJA8xc9o6oGnyJG7Z/HjaaijC+d1IL2btV9vcCDGWNNRvQCszNxUeV09AIFlJxxZFTxtdtS98chi+8MlhxnKUnugUoUCykokpip58vMO4H6wZieNBlCfg0sCe0hMbIccxYoQzm4gkeXuiPAFJI6Zp4mI8ASuRhh3ZRgzY2VBlCxEDg0T0GoC3ARxNW5cYGAI37wtw995cva+PMPNphQdFGAEAIKKWYiOXWcfV+wLcvTdX76sUosYEFEVJHjUCilLlSDICd6etQEy4el+Au/fm6n0VRcyYgKIo6SDJE1AUJQVSNwJENJ2I9hLRfiK6Lm19TCGiA0T0DBHtJKIW/9ggItpERH/1fw9MW88giGgFER0homfzjhW9D/K43a/DXUQkOhBgiXu7gYhe8ettJxHNyPtuiX9ve4noK+loHR+pGgEiqgHw3wDOBzABwFwimpCmTpY4j5kb86aZrgPwKDOPBfCo/7d07gNQmC2k1H2cD2Cs/zMfwJ0J6Vgp96H3vQHALX69NTLzBgDw2+McAGf619zht1tnSNsTmAJgPzO/yMzvAVgNYFbKOsXBLAAr/c8rAVycoi6hYOYnABSu4y51H7MA/Io9ngIwgIiGJ6NpdErcWylmAVjNzH9n5pcA7IfXbp0hbSNwOoBDeX+3+ceyDAP4ExG1EtF8/1g9Mx8GAP/30NS0M6PUfbhSj4v87syKvC6bK/dWkrSNABU5lvXpis8y82R4LvK3iejctBVKABfq8U4AZwBoBHAYwDL/uAv3Vpa0jUAbgIa8v0cCyPQuEWZu938fAbAOnuvYkXOP/d9H0tPQiFL3kfl6ZOYOZu5i5m4A9+BDlz/z9xZE2kZgG4CxRDSGiE6GNwCzPmWdKoaI+hFRXe4zgC8DeBbePc3zT5sH4MF0NDSm1H2sB3C5P0vwKQBv5LoNWaFgDGM2vHoDvHubQ0SnENEYeIOfzUnrFyepbiVm5k4iWgTgjwBqAKxg5ufS1MmQegDriAjwnu1vmHkjEW0DsIaIrgBwEMDXU9QxFES0CsA0AEOIqA3A9QB+iuL3sQHADHiDZu8A+GbiCkegxL1NI6JGeK7+AQALAICZnyOiNQCeB9AJ4NvM3JWG3nGhKwYVpcpJuzugKErKqBFQlCpHjYCiVDlqBBSlylEjoChVjhoBRaly1AgoSpWjRkBRqpz/B4cWxT+CaaWGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    y_test = []\n",
    "    y_test_pred = []\n",
    "\n",
    "    for data in testloader:\n",
    "\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        y_hat, _, _ = classifier(X)      \n",
    "        test_loss += criterion(y_hat, y)\n",
    "        \n",
    "        y_test.extend(list(y.detach().cpu().numpy()))\n",
    "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
    "\n",
    "print('Test Loss =', test_loss.item())\n",
    "plt.imshow(confusion_matrix(y_test, y_test_pred), cmap='Reds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s1RPoXsreDy4"
   },
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NuqwHPPlQavd"
   },
   "outputs": [],
   "source": [
    "class CNN2(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super(CNN2, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        # 3x224x224\n",
    "        self.cl1 = nn.Conv2d(3, 4, 3, stride=1, padding=1)\n",
    "        # 4x224x224\n",
    "        self.pl1 = nn.AvgPool2d(2, stride=2)\n",
    "        # 4x112x112\n",
    "        self.cl2 = nn.Conv2d(3, 16, 3, stride=1, padding=1)\n",
    "        # 16x112x112\n",
    "        self.pl2 = nn.AvgPool2d(2, stride=2)\n",
    "\n",
    "        # 16x56x56\n",
    "        self.flat = nn.Flatten(1, -1)\n",
    "        # (16x56x56)x1\n",
    "        self.out = nn.Linear(16*56*56, self.n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.pl1(F.relu(self.cl1(x)))\n",
    "        x = self.pl2(F.relu(self.cl2(x)))\n",
    "        x = self.out(self.flat(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = torch.argmax(y_hat, axis=1)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y4dEykLIhv7s"
   },
   "outputs": [],
   "source": [
    "classifier = CNN2(200)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_3RX6TtEhMFp"
   },
   "outputs": [],
   "source": [
    "old_loss = np.inf\n",
    "\n",
    "max_epoch = 500\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        \n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        y_hat = classifier(X)\n",
    "        \n",
    "        # Calculate Loss (Cross Entropy)\n",
    "        loss = criterion(y_hat, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
    "    \n",
    "    if abs(running_loss-old_loss)/running_loss < 1e-5:\n",
    "        print('Converged')\n",
    "        break\n",
    "    \n",
    "    old_loss = running_loss\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4eAKVd07iASJ"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    y_test = []\n",
    "    y_test_pred = []\n",
    "\n",
    "    for data in testloader:\n",
    "\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        y_hat = classifier(X)      \n",
    "        test_loss += criterion(y_hat, y)\n",
    "        \n",
    "        y_test.extend(list(y.detach().cpu().numpy()))\n",
    "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
    "\n",
    "print('Test Loss =', test_loss.item())\n",
    "plt.imshow(confusion_matrix(y_test, y_test_pred), cmap='Reds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN3(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, k=4):\n",
    "        super(CNN2, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        # 3x224x224\n",
    "        self.cl1 = nn.Conv2d(3, 4, 3, stride=1, padding=1)\n",
    "        # 4x224x224\n",
    "        self.pl1 = nn.AvgPool2d(2, stride=2)\n",
    "        # 4x112x112\n",
    "        self.cl2 = nn.Conv2d(3, 16, 3, stride=1, padding=1)\n",
    "        # 16x112x112\n",
    "        self.pl2 = nn.AvgPool2d(2, stride=2)\n",
    "\n",
    "        # NetVLAD\n",
    "        self.K = k\n",
    "        self.nv_conv = nn.Conv2d(256, k, 1)\n",
    "        self.nv_soft_ass = nn.Softmax2d()\n",
    "\n",
    "        # NetVLAD Parameter\n",
    "        self.c = nn.Parameter(torch.Tensor(self.K, 256))\n",
    "        \n",
    "        # Flatten to get h\n",
    "        self.flat = nn.Flatten(1, -1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.pl1(F.relu(self.cl1(x)))\n",
    "        x = self.pl2(F.relu(self.cl2(x)))\n",
    "        \n",
    "        # NetVLAD Step 1\n",
    "        a = self.nv_soft_ass(self.nv_conv(x))\n",
    "\n",
    "        # NetVLAD Step 2\n",
    "        for k in range(self.K):\n",
    "            a_k = a[:, k, :, :]\n",
    "            c_k = self.c[k, :]\n",
    "            temp = (x - c_k.reshape(1, -1, 1, 1))*a_k.unsqueeze(1)\n",
    "            z_k = torch.sum(temp, axis=(2, 3))\n",
    "            if k==0:\n",
    "                Z = z_k.unsqueeze(1)\n",
    "            else:\n",
    "                Z = torch.cat((Z, z_k.unsqueeze(1)), 1)\n",
    "        \n",
    "        # Flatten\n",
    "        Z = self.flatten(Z)\n",
    "\n",
    "        return Z\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = torch.argmax(y_hat, axis=1)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y4dEykLIhv7s"
   },
   "outputs": [],
   "source": [
    "classifier = CNN3(200)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_3RX6TtEhMFp"
   },
   "outputs": [],
   "source": [
    "old_loss = np.inf\n",
    "\n",
    "max_epoch = 500\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        \n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        y_hat = classifier(X)\n",
    "        \n",
    "        # Calculate Loss (Cross Entropy)\n",
    "        loss = criterion(y_hat, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
    "    \n",
    "    if abs(running_loss-old_loss)/running_loss < 1e-5:\n",
    "        print('Converged')\n",
    "        break\n",
    "    \n",
    "    old_loss = running_loss\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4eAKVd07iASJ"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    y_test = []\n",
    "    y_test_pred = []\n",
    "\n",
    "    for data in testloader:\n",
    "\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        y_hat = classifier(X)      \n",
    "        test_loss += criterion(y_hat, y)\n",
    "        \n",
    "        y_test.extend(list(y.detach().cpu().numpy()))\n",
    "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
    "\n",
    "print('Test Loss =', test_loss.item())\n",
    "plt.imshow(confusion_matrix(y_test, y_test_pred), cmap='Reds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Ass3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
