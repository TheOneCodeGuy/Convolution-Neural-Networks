{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_qTq010ijWD"
   },
   "source": [
    "### Run this cell only once (the first ever time you run) to process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "v8PCDpQkAHwN",
    "outputId": "f0a8a37c-61d2-47b2-ebe8-68a68bedd846"
   },
   "outputs": [],
   "source": [
    "# # from google.colab import drive\n",
    "# # drive.mount('/content/drive')\n",
    "\n",
    "# # For extracting\n",
    "# !pip install pyunpack\n",
    "# !pip install patool\n",
    "\n",
    "# from pyunpack import Archive\n",
    "# Archive('CUB_200_2011.tgz').extractall('Assignment3_Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fjjzGKEwFmsN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "0Z5GNnFsDPna",
    "outputId": "3a64d6e8-a233-4dd4-9fd9-df223fbdc386"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from io import StringIO\n",
    "from PIL import Image\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "import pickle\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "J6JD3ycyPuYi",
    "outputId": "f1573012-0754-4e4e-af8a-cea2ad53562e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DP8iACDGDSWD"
   },
   "outputs": [],
   "source": [
    "class DatasetClass(Dataset):\n",
    "    \n",
    "    def __init__(self, directory, img_size):\n",
    "        \n",
    "        self.directory = directory\n",
    "        classes = os.listdir(directory)\n",
    "        print('Number of Classes =', len(classes))\n",
    "        self.files = []\n",
    "        for class_name in classes:\n",
    "            images = os.listdir(directory + '/' + class_name)\n",
    "            images = [class_name + '/' + image for image in images]\n",
    "            self.files.extend(images)\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.size = len(self.files)\n",
    "        \n",
    "    def __getitem__(self, idx):     \n",
    "        \n",
    "        image_name = self.files[idx]\n",
    "        y = int(image_name[:3])-1\n",
    "        img = Image.open(self.directory + '/' + image_name).convert(mode='RGB').resize(self.img_size)\n",
    "        \n",
    "        trans = transforms.ToTensor()\n",
    "        # return trans(img), torch.Tensor(y, dtype=torch.long)\n",
    "        \n",
    "        return trans(img)*255, y        # Multiplying by pixel value\n",
    "      \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TKVGwDDOKAsM"
   },
   "outputs": [],
   "source": [
    "def train_test_loader(directory, img_size, train_fraction=0.8, num_workers=0, batch_size=32):\n",
    "\n",
    "    dataset = DatasetClass(directory, img_size)\n",
    "    \n",
    "    N = dataset.size\n",
    "    train_size = int(N*train_fraction)\n",
    "    test_size = N - train_size\n",
    "\n",
    "    train_data, test_data = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    testloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    return trainloader, testloader, train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fT66UkOpUDtv",
    "outputId": "08feea82-743e-4cc5-c894-3ec62919990c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes = 200\n"
     ]
    }
   ],
   "source": [
    "trainloader, testloader, train_size, test_size = train_test_loader('D:/_SEM8/DL/Assignment 3/Assignment3_Data/CUB_200_2011/images/', (224, 224))\n",
    "# trainloader, testloader, train_size, test_size = train_test_loader('/content/drive/My Drive/Assignment3_Data/CUB_200_2011/images', (224, 224))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     trainloader, testloader, train_size, test_size = train_test_loader('Assignment3_Data/CUB_200_2011/images/', (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "colab_type": "code",
    "id": "81_AjruYX-U6",
    "outputId": "39c34d86-c73e-4943-ab50-2c311b6f051c"
   },
   "outputs": [],
   "source": [
    "# if os.path.exists('/content/drive/My Drive/RGB_mean.sav'):\n",
    "#     RGB_mean = pickle.load(open('/content/drive/My Drive/RGB_mean.sav', 'rb'))\n",
    "\n",
    "# else:\n",
    "#     RGB_mean = torch.zeros(3)\n",
    "#     i = 0\n",
    "#     for X, y in trainloader:\n",
    "#         print(i, '/', len(trainloader), end=', ')\n",
    "#         i += 1\n",
    "#         RGB_mean += (X.sum(0).sum(1).sum(1)/(X.shape[2]*X.shape[2]))/train_size\n",
    "\n",
    "#     #   RGB_mean = torch.from_numpy(RGB_mean).to(device, dtype=torch.float)\n",
    "\n",
    "#     pickle.dump(RGB_mean, open('/content/drive/My Drive/RGB_mean.sav', 'wb'))\n",
    "\n",
    "\n",
    "if os.path.exists('D:/_SEM8/DL/Assignment 3/RGB_mean.sav'):\n",
    "    RGB_mean = pickle.load(open('D:/_SEM8/DL/Assignment 3/RGB_mean.sav', 'rb'))\n",
    "\n",
    "else:\n",
    "    RGB_mean = torch.zeros(3)\n",
    "    i = 0\n",
    "    for X, y in trainloader:\n",
    "        print(i, '/', len(trainloader), end=', ')\n",
    "        i += 1\n",
    "        RGB_mean += (X.sum(0).sum(1).sum(1)/(X.shape[2]*X.shape[2]))/train_size\n",
    "\n",
    "    pickle.dump(RGB_mean, open('D:/_SEM8/DL/Assignment 3/RGB_mean.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vc8N6oxxiwx0"
   },
   "source": [
    "### Question 1. a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xzGC9uLa9xum"
   },
   "outputs": [],
   "source": [
    "class VGGNet(nn.Module):\n",
    "    \n",
    "    def __init__(self): #, RGB_mean):\n",
    "        super(VGGNet, self).__init__()\n",
    "        \n",
    "#         self.RGB_mean = RGB_mean.to(device)\n",
    "\n",
    "        self.c11 = nn.Conv2d(3, 64, 3, stride=1, padding=1)\n",
    "        self.c12 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
    "        self.p1 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        self.c21 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
    "        self.c22 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.p2 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        self.c31 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
    "        self.c32 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
    "        self.c33 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
    "        self.p3 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        self.c41 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n",
    "        self.c42 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.c43 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.p4 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        self.c51 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.c52 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.c53 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.p5 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        self.flat = nn.Flatten(1, -1)\n",
    "        self.fc1 = nn.Linear(7*7*512, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.out = nn.Linear(4096, 200)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x #- self.RGB_mean[None, :, None, None]\n",
    "        x = self.p1(F.relu(self.c12(F.relu(self.c11(x)))))\n",
    "        x = self.p1(F.relu(self.c22(F.relu(self.c21(x)))))\n",
    "        x = self.p3(F.relu(self.c33(F.relu(self.c32(F.relu(self.c31(x)))))))\n",
    "        x = self.p4(F.relu(self.c43(F.relu(self.c42(F.relu(self.c41(x)))))))\n",
    "        x = self.p5(F.relu(self.c53(F.relu(self.c52(F.relu(self.c51(x)))))))\n",
    "        x = F.relu(self.fc2(F.relu(self.fc1(self.flat(x)))))\n",
    "        Z = self.out(x)\n",
    "\n",
    "\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EwuOkncsWXVy"
   },
   "outputs": [],
   "source": [
    "VGG_model = VGGNet() #VGGNet(RGB_mean)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(VGG_model.parameters(), lr=0.01, momentum=0.9)\n",
    "VGG_model = VGG_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "VUTJaWtqSqQj",
    "outputId": "168e7833-abf8-4a3f-ce57-f4e69c9f91f0"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8c8a977bf6d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# Forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVGG_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Calculate Loss (Cross Entropy)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-cd41d718e8a9>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc43\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc42\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc41\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc53\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc52\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc51\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1608\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1609\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1610\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1611\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1612\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "old_loss = np.inf\n",
    "\n",
    "max_epoch = 100\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        \n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        y_hat = VGG_model(X)\n",
    "        \n",
    "        # Calculate Loss (Cross Entropy)\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()*32/train_size\n",
    "    \n",
    "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
    "    \n",
    "    if abs(running_loss-old_loss)/running_loss < 1e-5:\n",
    "        print('Converged')\n",
    "        break\n",
    "    \n",
    "    old_loss = running_loss\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "colab_type": "code",
    "id": "Ok3viz8oX4nv",
    "outputId": "97583237-8039-45b6-fa28-176139706f6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 392.4207763671875\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    ...  193  194  195  196  197  198  199\n",
       "0      0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "1      0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "2      0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "3      0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "4      0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
       "195    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "196    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "197    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "198    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "199    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
       "\n",
       "[200 rows x 200 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    y_test = []\n",
    "    y_test_pred = []\n",
    "\n",
    "    for data in testloader:\n",
    "\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        y_hat = VGG_model(X)      \n",
    "        test_loss += criterion(y_hat, y)\n",
    "        \n",
    "        y_test.extend(list(y.detach().cpu().numpy()))\n",
    "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
    "\n",
    "print('Test Loss =', test_loss.item())\n",
    "plt.imshow(confusion_matrix(y_test, y_test_pred), cmap='Reds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r_MY7HHT7GYv"
   },
   "source": [
    "### Question 1. b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZEvZrYhn7JuX"
   },
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, RGB_mean):\n",
    "    \n",
    "        super(GoogLeNet, self).__init__()\n",
    "        self.RGB_mean = RGB_mean.to(device)\n",
    "        self.n_classes = n_classes\n",
    "        # Convolution\n",
    "        # 3x224x224\n",
    "        self.c1 = nn.Conv2d(3, 64, 7, stride=2, padding=3)\n",
    "        # 64x112x112\n",
    "        self.mp1 = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "        # Deep Convolution\n",
    "        # 64x56x56\n",
    "        self.c21 = nn.Conv2d(64, 64, 1, stride=1, padding=0)\n",
    "        # 64x56x56\n",
    "        self.c22 = nn.Conv2d(64, 192, 3, stride=1, padding=1)\n",
    "        # 192x56x56\n",
    "        self.mp2 = nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "        # Inception 3a\n",
    "        # 192x28x28\n",
    "        # P1\n",
    "        self.c3a1 = nn.Conv2d(192, 64, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c3a21 = nn.Conv2d(192, 96, 1, stride=1, padding=0)\n",
    "        self.c3a22 = nn.Conv2d(96, 128, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c3a31 = nn.Conv2d(192, 16, 1, stride=1, padding=0)\n",
    "        self.c3a32 = nn.Conv2d(16, 32, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp3a4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c3a4 = nn.Conv2d(192, 32, 1, stride=1, padding=0)\n",
    "\n",
    "        # Inception 3b\n",
    "        # 256x28x28\n",
    "        # P1\n",
    "        self.c3b1 = nn.Conv2d(256, 128, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c3b21 = nn.Conv2d(256, 128, 1, stride=1, padding=0)\n",
    "        self.c3b22 = nn.Conv2d(128, 192, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c3b31 = nn.Conv2d(256, 32, 1, stride=1, padding=0)\n",
    "        self.c3b32 = nn.Conv2d(32, 96, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp3b4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c3b4 = nn.Conv2d(256, 64, 1, stride=1, padding=0)\n",
    "\n",
    "        # 480x28x28\n",
    "        # MP\n",
    "        self.mp3 = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "        # Inception 4a\n",
    "        # 480x14x14\n",
    "        # P1\n",
    "        self.c4a1 = nn.Conv2d(480, 192, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c4a21 = nn.Conv2d(480, 96, 1, stride=1, padding=0)\n",
    "        self.c4a22 = nn.Conv2d(96, 208, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c4a31 = nn.Conv2d(480, 16, 1, stride=1, padding=0)\n",
    "        self.c4a32 = nn.Conv2d(16, 48, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp4a4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c4a4 = nn.Conv2d(480, 64, 1, stride=1, padding=0)\n",
    "\n",
    "        # Auxiliary 1\n",
    "        # 512x14x14\n",
    "        self.apa1 = nn.AvgPool2d(5, stride=3, padding=0)\n",
    "        # 512x4x4\n",
    "        self.ca1 = nn.Conv2d(512, 128, 1, stride=1)\n",
    "        # 128x4x4\n",
    "        self.flat1 = nn.Flatten(1, -1)\n",
    "        # (128x4x4)x1\n",
    "        self.fca1 = nn.Linear(2048, 1024)\n",
    "        self.a1drop = nn.Dropout(0.7)\n",
    "        self.a1out = nn.Linear(1024, self.n_classes)\n",
    "\n",
    "        # Inception 4b\n",
    "        # 512x14x14\n",
    "        # P1\n",
    "        self.c4b1 = nn.Conv2d(512, 160, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c4b21 = nn.Conv2d(512, 112, 1, stride=1, padding=0)\n",
    "        self.c4b22 = nn.Conv2d(112, 224, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c4b31 = nn.Conv2d(512, 24, 1, stride=1, padding=0)\n",
    "        self.c4b32 = nn.Conv2d(24, 64, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp4b4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c4b4 = nn.Conv2d(512, 64, 1, stride=1, padding=0)\n",
    "\n",
    "        # Inception 4c\n",
    "        # 512x14x14\n",
    "        # P1\n",
    "        self.c4c1 = nn.Conv2d(512, 128, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c4c21 = nn.Conv2d(512, 128, 1, stride=1, padding=0)\n",
    "        self.c4c22 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c4c31 = nn.Conv2d(512, 24, 1, stride=1, padding=0)\n",
    "        self.c4c32 = nn.Conv2d(24, 64, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp4c4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c4c4 = nn.Conv2d(512, 64, 1, stride=1, padding=0)\n",
    "\n",
    "        # Inception 4d\n",
    "        # 512x14x14\n",
    "        # P1\n",
    "        self.c4d1 = nn.Conv2d(512, 112, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c4d21 = nn.Conv2d(512, 144, 1, stride=1, padding=0)\n",
    "        self.c4d22 = nn.Conv2d(144, 288, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c4d31 = nn.Conv2d(512, 32, 1, stride=1, padding=0)\n",
    "        self.c4d32 = nn.Conv2d(32, 64, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp4d4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c4d4 = nn.Conv2d(512, 64, 1, stride=1, padding=0)\n",
    "\n",
    "        # Auxiliary 2\n",
    "        # 528x14x14\n",
    "        self.apa2 = nn.AvgPool2d(5, stride=3, padding=0)\n",
    "        # 528x4x4\n",
    "        self.ca2 = nn.Conv2d(528, 128, 1, stride=1)\n",
    "        # 128x4x4\n",
    "        self.flat1 = nn.Flatten(1, -1)\n",
    "        # (128x4x4)x1\n",
    "        self.fca2 = nn.Linear(2048, 1024)\n",
    "        self.a2drop = nn.Dropout(0.7)\n",
    "        self.a2out = nn.Linear(1024, self.n_classes)\n",
    "\n",
    "        # Inception 4e\n",
    "        # 528x14x14\n",
    "        # P1\n",
    "        self.c4e1 = nn.Conv2d(528, 256, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c4e21 = nn.Conv2d(528, 160, 1, stride=1, padding=0)\n",
    "        self.c4e22 = nn.Conv2d(160, 320, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c4e31 = nn.Conv2d(528, 32, 1, stride=1, padding=0)\n",
    "        self.c4e32 = nn.Conv2d(32, 128, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp4e4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c4e4 = nn.Conv2d(528, 128, 1, stride=1, padding=0)\n",
    "\n",
    "        # 832x14x14\n",
    "        # MP\n",
    "        self.mp4 = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "        # Inception 5a\n",
    "        # 832x7x7\n",
    "        # P1\n",
    "        self.c5a1 = nn.Conv2d(832, 256, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c5a21 = nn.Conv2d(832, 160, 1, stride=1, padding=0)\n",
    "        self.c5a22 = nn.Conv2d(160, 320, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c5a31 = nn.Conv2d(832, 32, 1, stride=1, padding=0)\n",
    "        self.c5a32 = nn.Conv2d(32, 128, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp5a4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c5a4 = nn.Conv2d(832, 128, 1, stride=1, padding=0)\n",
    "\n",
    "        # Inception 5b\n",
    "        # 832x7x7\n",
    "        # P1\n",
    "        self.c5b1 = nn.Conv2d(832, 384, 1, stride=1, padding=0)\n",
    "        # P2\n",
    "        self.c5b21 = nn.Conv2d(832, 192, 1, stride=1, padding=0)\n",
    "        self.c5b22 = nn.Conv2d(192, 384, 3, stride=1, padding=1)\n",
    "        # P3\n",
    "        self.c5b31 = nn.Conv2d(832, 48, 1, stride=1, padding=0)\n",
    "        self.c5b32 = nn.Conv2d(48, 128, 5, stride=1, padding=2)\n",
    "        # P4\n",
    "        self.mp5b4 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.c5b4 = nn.Conv2d(832, 128, 1, stride=1, padding=0)\n",
    "\n",
    "        # 1024x7x7\n",
    "        self.ap = nn.AvgPool2d(7, stride=1)\n",
    "        # 1024x1x1\n",
    "        self.drop = nn.Dropout(0.4)\n",
    "        self.flat = nn.Flatten(1, -1)\n",
    "        self.out = nn.Linear(1024, self.n_classes)\n",
    "\n",
    "    def forward(self, x, auxiliary=True):\n",
    "\n",
    "        x = x - self.RGB_mean[None, :, None, None]\n",
    "\n",
    "        # Layer 1\n",
    "        x = self.mp1(F.relu(self.c1(x)))\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.mp2(F.relu(self.c22(F.relu(self.c21(x)))))\n",
    "\n",
    "        # Layer 3a\n",
    "        x1 = F.relu(self.c3a1(x))\n",
    "        x2 = F.relu(self.c3a22(F.relu(self.c3a21(x))))\n",
    "        x3 = F.relu(self.c3a32(F.relu(self.c3a31(x))))\n",
    "        x4 = F.relu(self.c3a4(self.mp3a4(x)))\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "\n",
    "        # Layer 3b\n",
    "        x1 = F.relu(self.c3b1(x))\n",
    "        x2 = F.relu(self.c3b22(F.relu(self.c3b21(x))))\n",
    "        x3 = F.relu(self.c3b32(F.relu(self.c3b31(x))))\n",
    "        x4 = F.relu(self.c3b4(self.mp3b4(x)))\n",
    "        x = self.mp3(torch.cat((x1, x2, x3, x4), 1))\n",
    "\n",
    "        # Layer 4a\n",
    "        x1 = F.relu(self.c4a1(x))\n",
    "        x2 = F.relu(self.c4a22(F.relu(self.c4a21(x))))\n",
    "        x3 = F.relu(self.c4a32(F.relu(self.c4a31(x))))\n",
    "        x4 = F.relu(self.c4a4(self.mp4a4(x)))\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "\n",
    "        # Auxiliary 1\n",
    "        if auxiliary == True:\n",
    "            z1 = self.flat1(F.relu(self.ca1(self.apa1(x))))\n",
    "            z1 = self.a1out(self.a1drop(F.relu(self.fca1(z1))))\n",
    "        else:\n",
    "            z1 = None\n",
    "\n",
    "        # Layer 4b\n",
    "        x1 = F.relu(self.c4b1(x))\n",
    "        x2 = F.relu(self.c4b22(F.relu(self.c4b21(x))))\n",
    "        x3 = F.relu(self.c4b32(F.relu(self.c4b31(x))))\n",
    "        x4 = F.relu(self.c4b4(self.mp4b4(x)))\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "\n",
    "        # Layer 4c\n",
    "        x1 = F.relu(self.c4c1(x))\n",
    "        x2 = F.relu(self.c4c22(F.relu(self.c4c21(x))))\n",
    "        x3 = F.relu(self.c4c32(F.relu(self.c4c31(x))))\n",
    "        x4 = F.relu(self.c4c4(self.mp4c4(x)))\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "\n",
    "        # Layer 4d\n",
    "        x1 = F.relu(self.c4d1(x))\n",
    "        x2 = F.relu(self.c4d22(F.relu(self.c4d21(x))))\n",
    "        x3 = F.relu(self.c4d32(F.relu(self.c4d31(x))))\n",
    "        x4 = F.relu(self.c4d4(self.mp4d4(x)))\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "\n",
    "        # Auxiliary 2\n",
    "        if auxiliary == True:\n",
    "            z2 = self.flat(F.relu(self.ca2(self.apa2(x))))\n",
    "            z2 = self.a2out(self.a2drop(F.relu(self.fca2(z2))))\n",
    "        else:\n",
    "            z2 = None\n",
    "\n",
    "        # Layer 4e\n",
    "        x1 = F.relu(self.c4e1(x))\n",
    "        x2 = F.relu(self.c4e22(F.relu(self.c4e21(x))))\n",
    "        x3 = F.relu(self.c4e32(F.relu(self.c4e31(x))))\n",
    "        x4 = F.relu(self.c4e4(self.mp4e4(x)))\n",
    "        x = self.mp4(torch.cat((x1, x2, x3, x4), 1))\n",
    "\n",
    "        # Layer 5a\n",
    "        x1 = F.relu(self.c5a1(x))\n",
    "        x2 = F.relu(self.c5a22(F.relu(self.c5a21(x))))\n",
    "        x3 = F.relu(self.c5a32(F.relu(self.c5a31(x))))\n",
    "        x4 = F.relu(self.c5a4(self.mp5a4(x)))\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "\n",
    "        # Layer 5b\n",
    "        x1 = F.relu(self.c5b1(x))\n",
    "        x2 = F.relu(self.c5b22(F.relu(self.c5b21(x))))\n",
    "        x3 = F.relu(self.c5b32(F.relu(self.c5b31(x))))\n",
    "        x4 = F.relu(self.c5b4(self.mp5b4(x)))\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "\n",
    "        # Final Output\n",
    "        x = self.out(self.flat(self.drop(self.ap(x))))\n",
    "\n",
    "        return x, z1, z2\n",
    "\n",
    "    \n",
    "    def predict(self, x):\n",
    "\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = torch.argmax(y_hat, axis=1)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k-5ltA0Q2kdX"
   },
   "outputs": [],
   "source": [
    "classifier = GoogLeNet(200, RGB_mean)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOH2pEviO6nE",
    "outputId": "56cd4478-398e-4a6e-c8d7-97ebd9996aa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Loss = 2501.153305053711\n",
      "Epoch 2 : Loss = 2500.8983478546143\n",
      "Epoch 3 : Loss = 2500.78324508667\n",
      "Epoch 4 : Loss = 2500.6387271881104\n",
      "Epoch 5 : Loss = 2500.5550413131714\n",
      "Epoch 6 : Loss = 2500.473469734192\n",
      "Epoch 7 : Loss = 2500.244875907898\n",
      "Epoch 8 : Loss = 2500.0989379882812\n",
      "Epoch 9 : Loss = 2499.9257650375366\n",
      "Epoch 10 : Loss = 2499.4732961654663\n",
      "Epoch 11 : Loss = 2498.816925048828\n",
      "Epoch 12 : Loss = 2497.4736337661743\n",
      "Epoch 13 : Loss = 2495.2521324157715\n",
      "Epoch 14 : Loss = 2492.7037239074707\n",
      "Epoch 15 : Loss = 2489.75691986084\n",
      "Epoch 16 : Loss = 2486.009684562683\n",
      "Epoch 17 : Loss = 2482.072012901306\n",
      "Epoch 18 : Loss = 2477.4946279525757\n",
      "Epoch 19 : Loss = 2473.602825164795\n",
      "Epoch 20 : Loss = 2470.310311317444\n",
      "Epoch 21 : Loss = 2468.159963607788\n",
      "Epoch 22 : Loss = 2464.9776096343994\n",
      "Epoch 23 : Loss = 2463.316065788269\n",
      "Epoch 24 : Loss = 2460.352342605591\n",
      "Epoch 25 : Loss = 2457.3610486984253\n",
      "Epoch 26 : Loss = 2453.7332725524902\n",
      "Epoch 27 : Loss = 2448.1358337402344\n",
      "Epoch 28 : Loss = 2439.0641956329346\n",
      "Epoch 29 : Loss = 2416.6256470680237\n",
      "Epoch 30 : Loss = 2393.4474773406982\n",
      "Epoch 31 : Loss = 2379.896737575531\n",
      "Epoch 32 : Loss = 2367.185130596161\n",
      "Epoch 33 : Loss = 2354.908924102783\n",
      "Epoch 34 : Loss = 2344.3095088005066\n",
      "Epoch 35 : Loss = 2336.268772125244\n",
      "Epoch 36 : Loss = 2317.746772289276\n",
      "Epoch 37 : Loss = 2296.2177176475525\n",
      "Epoch 38 : Loss = 2266.92804145813\n",
      "Epoch 39 : Loss = 2246.889352798462\n",
      "Epoch 40 : Loss = 2223.29056596756\n",
      "Epoch 41 : Loss = 2196.317319869995\n",
      "Epoch 42 : Loss = 2166.471405029297\n",
      "Epoch 43 : Loss = 2132.183919906616\n",
      "Epoch 44 : Loss = 2097.2789969444275\n",
      "Epoch 45 : Loss = 2060.02911901474\n",
      "Epoch 46 : Loss = 2033.4333124160767\n",
      "Epoch 47 : Loss = 2008.508572101593\n",
      "Epoch 48 : Loss = 1984.0811409950256\n",
      "Epoch 49 : Loss = 1958.079351902008\n",
      "Epoch 50 : Loss = 1952.0023655891418\n",
      "Epoch 51 : Loss = 1920.9764223098755\n",
      "Epoch 52 : Loss = 1892.0809483528137\n",
      "Epoch 53 : Loss = 1875.8210320472717\n",
      "Epoch 54 : Loss = 1841.29585313797\n",
      "Epoch 55 : Loss = 1818.6211290359497\n",
      "Epoch 56 : Loss = 1784.1773109436035\n",
      "Epoch 57 : Loss = 1769.3177523612976\n",
      "Epoch 58 : Loss = 1746.126317024231\n",
      "Epoch 59 : Loss = 1718.8494048118591\n",
      "Epoch 60 : Loss = 1707.7179441452026\n",
      "Epoch 61 : Loss = 1660.8843369483948\n",
      "Epoch 62 : Loss = 1631.8935117721558\n",
      "Epoch 63 : Loss = 1608.9846906661987\n",
      "Epoch 64 : Loss = 1583.7448329925537\n",
      "Epoch 65 : Loss = 1556.2324061393738\n",
      "Epoch 66 : Loss = 1525.8717947006226\n",
      "Epoch 67 : Loss = 1497.9109075069427\n",
      "Epoch 68 : Loss = 1482.0596373081207\n",
      "Epoch 69 : Loss = 1445.6288464069366\n",
      "Epoch 70 : Loss = 1417.94442653656\n",
      "Epoch 71 : Loss = 1399.5293490886688\n",
      "Epoch 72 : Loss = 1365.8672502040863\n",
      "Epoch 73 : Loss = 1335.2966475486755\n",
      "Epoch 74 : Loss = 1308.7092068195343\n",
      "Epoch 75 : Loss = 1270.4828851222992\n",
      "Epoch 76 : Loss = 1264.855057477951\n",
      "Epoch 77 : Loss = 1223.336505651474\n",
      "Epoch 78 : Loss = 1207.4595038890839\n",
      "Epoch 79 : Loss = 1184.7936913967133\n",
      "Epoch 80 : Loss = 1157.803295135498\n",
      "Epoch 81 : Loss = 1121.707371711731\n",
      "Epoch 82 : Loss = 1093.0173573493958\n",
      "Epoch 83 : Loss = 1073.8072295188904\n",
      "Epoch 84 : Loss = 1060.1586072444916\n",
      "Epoch 85 : Loss = 995.1286418437958\n",
      "Epoch 86 : Loss = 991.2496182918549\n",
      "Epoch 87 : Loss = 976.4587638378143\n",
      "Epoch 88 : Loss = 929.843656539917\n",
      "Epoch 89 : Loss = 925.8636767864227\n",
      "Epoch 90 : Loss = 878.0959422588348\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-d97e437f5564>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-d197a442385c>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mimage_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mimage_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mtrans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    928\u001b[0m         \"\"\"\n\u001b[0;32m    929\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"P\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m                             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m                             \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m                                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "old_loss = np.inf\n",
    "\n",
    "max_epoch = 500\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        \n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        y_hat, y_hat1, y_hat2 = classifier(X)\n",
    "        \n",
    "        # Calculate Loss (Cross Entropy)\n",
    "        loss_main = criterion(y_hat, y)\n",
    "        loss1 = criterion(y_hat1, y)\n",
    "        loss2 = criterion(y_hat2, y)\n",
    "\n",
    "        # Weighted Loss\n",
    "        loss = loss_main + 0.3*loss1 + 0.3*loss2\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
    "    \n",
    "    if abs(running_loss-old_loss)/running_loss < 1e-5:\n",
    "        print('Converged')\n",
    "        break\n",
    "    \n",
    "    old_loss = running_loss\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ATw2OTqQiJCq",
    "outputId": "855eda4f-9804-499b-8250-b236225a2ef8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 242.4600067138672\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    y_test = []\n",
    "    y_test_pred = []\n",
    "\n",
    "    for data in testloader:\n",
    "\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        y_hat, _, _ = classifier(X)      \n",
    "        test_loss += criterion(y_hat, y)\n",
    "        \n",
    "        y_test.extend(list(y.detach().cpu().numpy()))\n",
    "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
    "\n",
    "print('Test Loss =', test_loss.item())\n",
    "plt.imshow(confusion_matrix(y_test, y_test_pred), cmap='Reds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s1RPoXsreDy4"
   },
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NuqwHPPlQavd"
   },
   "outputs": [],
   "source": [
    "class CNN2(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super(CNN2, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        # 3x224x224\n",
    "        self.cl1 = nn.Conv2d(3, 4, 3, stride=1, padding=1)\n",
    "        # 4x224x224\n",
    "        self.pl1 = nn.AvgPool2d(2, stride=2)\n",
    "        # 4x112x112\n",
    "        self.cl2 = nn.Conv2d(4, 16, 3, stride=1, padding=1)\n",
    "        # 16x112x112\n",
    "        self.pl2 = nn.AvgPool2d(2, stride=2)\n",
    "\n",
    "        # 16x56x56\n",
    "        self.flat = nn.Flatten(1, -1)\n",
    "        # (16x56x56)x1\n",
    "        self.out = nn.Linear(16*56*56, self.n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.pl1(F.relu(self.cl1(x)))\n",
    "        x = self.pl2(F.relu(self.cl2(x)))\n",
    "        x = self.out(self.flat(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = torch.argmax(y_hat, axis=1)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y4dEykLIhv7s"
   },
   "outputs": [],
   "source": [
    "classifier = CNN2(200)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "_3RX6TtEhMFp",
    "outputId": "a496fb8d-dbd5-4154-bd53-62ebac550bd7"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5c0c3068f979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9fc865d241b5>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mimage_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   1871\u001b[0m                 )\n\u001b[1;32m   1872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1873\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "old_loss = np.inf\n",
    "from IPython.display import clear_output\n",
    "\n",
    "max_epoch = 50\n",
    "losses = []\n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        \n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        y_hat = classifier(X)\n",
    "        \n",
    "        # Calculate Loss (Cross Entropy)\n",
    "        loss = criterion(y_hat, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()*len(X)/9430\n",
    "    \n",
    "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
    "    losses.append(running_loss)\n",
    "\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    if (abs(running_loss-old_loss)/running_loss < 5e-2) and epoch>=10 and running_loss<1000:\n",
    "        print('Converged')\n",
    "        break\n",
    "    \n",
    "    old_loss = running_loss\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "RskMmPeG5RAU",
    "outputId": "bccc9f1e-e705-481b-d84a-0017fb6e8340"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 39.00154113769531\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    y_train = []\n",
    "    y_train_pred = []\n",
    "\n",
    "    for data in trainloader:\n",
    "\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        y_hat = classifier(X)      \n",
    "        train_loss += criterion(y_hat, y)\n",
    "        \n",
    "        y_train.extend(list(y.detach().cpu().numpy()))\n",
    "        y_train_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
    "\n",
    "print('Train Loss =', train_loss.item())\n",
    "plt.imshow(confusion_matrix(y_train, y_train_pred), cmap='Reds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "s1_WEeod6Bg2",
    "outputId": "19208fbb-77ea-43fe-80c6-92b7ca64dc57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy = 0.9813361611876988 Train Precision = 0.9839351684742241 Train F1 = 0.9814279047682407\n"
     ]
    }
   ],
   "source": [
    "acc_tr = accuracy_score(y_train, y_train_pred)\n",
    "prec_tr = precision_score(y_train, y_train_pred, average='weighted')\n",
    "f1_tr = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "print('Train Accuracy =', acc_tr, 'Train Precision =', prec_tr, 'Train F1 =', f1_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4eAKVd07iASJ"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    y_test = []\n",
    "    y_test_pred = []\n",
    "\n",
    "    for data in testloader:\n",
    "\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        y_hat = classifier(X)      \n",
    "        test_loss += criterion(y_hat, y)\n",
    "        \n",
    "        y_test.extend(list(y.detach().cpu().numpy()))\n",
    "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
    "\n",
    "print('Test Loss =', test_loss.item())\n",
    "plt.imshow(confusion_matrix(y_test, y_test_pred), cmap='Reds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qJK9h5xu4DjA"
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "prec = precision_score(y_test, y_test_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "print('Test Accuracy =', acc, 'Test Precision =', prec, 'Test F1 =', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_vIZ1K6eE33v"
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jVC5hAh8E33x"
   },
   "outputs": [],
   "source": [
    "class CNN3(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, k=4):\n",
    "        super(CNN3, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        # 3x224x224\n",
    "        self.cl1 = nn.Conv2d(3, 4, 3, stride=1, padding=1)\n",
    "        # 4x224x224\n",
    "        self.pl1 = nn.AvgPool2d(2, stride=2)\n",
    "        # 4x112x112\n",
    "        self.cl2 = nn.Conv2d(4, 16, 3, stride=1, padding=1)\n",
    "        # 16x112x112\n",
    "        self.pl2 = nn.AvgPool2d(2, stride=2)\n",
    "\n",
    "        # NetVLAD\n",
    "        self.K = k\n",
    "        self.nv_conv = nn.Conv2d(256, k, 1)\n",
    "        self.nv_soft_ass = nn.Softmax2d()\n",
    "\n",
    "        # NetVLAD Parameter\n",
    "        self.c = nn.Parameter(torch.Tensor(self.K, 256))\n",
    "        \n",
    "        # Flatten to get h\n",
    "        self.flat = nn.Flatten(1, -1)\n",
    "\n",
    "        # Output layer\n",
    "        # self.out = nn.Linear()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        print(x.shape)\n",
    "        x = self.pl1(F.relu(self.cl1(x)))\n",
    "        print(x.shape)\n",
    "        x = self.pl2(F.relu(self.cl2(x)))\n",
    "        print(x.shape)\n",
    "        \n",
    "        # NetVLAD Step 1\n",
    "        a = self.nv_soft_ass(self.nv_conv(x))\n",
    "\n",
    "        # NetVLAD Step 2\n",
    "        for k in range(self.K):\n",
    "            a_k = a[:, k, :, :]\n",
    "            c_k = self.c[k, :]\n",
    "            temp = (x - c_k.reshape(1, -1, 1, 1))*a_k.unsqueeze(1)\n",
    "            z_k = torch.sum(temp, axis=(2, 3))\n",
    "            if k==0:\n",
    "                Z = z_k.unsqueeze(1)\n",
    "            else:\n",
    "                Z = torch.cat((Z, z_k.unsqueeze(1)), 1)\n",
    "        \n",
    "        # Flatten\n",
    "        Z = self.flatten(Z)\n",
    "        print(Z.shape)\n",
    "\n",
    "        return Z\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = torch.argmax(y_hat, axis=1)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2NO7TpyJE330"
   },
   "outputs": [],
   "source": [
    "classifier = CNN3(200)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "DbpAEbjRE332",
    "outputId": "389e13fd-cf44-4d80-eb35-9bf33196e84d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 4, 112, 112])\n",
      "torch.Size([32, 16, 56, 56])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-d8ed2172ed53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-92f1e63286ba>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# NetVLAD Step 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnv_soft_ass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnv_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# NetVLAD Step 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    348\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    349\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 350\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [4, 256, 1, 1], expected input[32, 16, 56, 56] to have 256 channels, but got 16 channels instead"
     ]
    }
   ],
   "source": [
    "old_loss = np.inf\n",
    "\n",
    "max_epoch = 500\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        \n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        y_hat = classifier(X)\n",
    "        break\n",
    "        \n",
    "        # Calculate Loss (Cross Entropy)\n",
    "        loss = criterion(y_hat, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    break\n",
    "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
    "    \n",
    "    if abs(running_loss-old_loss)/running_loss < 1e-5:\n",
    "        print('Converged')\n",
    "        break\n",
    "    \n",
    "    old_loss = running_loss\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lcw20TIlE334"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    y_test = []\n",
    "    y_test_pred = []\n",
    "\n",
    "    for data in testloader:\n",
    "\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        y_hat = classifier(X)      \n",
    "        test_loss += criterion(y_hat, y)\n",
    "        \n",
    "        y_test.extend(list(y.detach().cpu().numpy()))\n",
    "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
    "\n",
    "print('Test Loss =', test_loss.item())\n",
    "plt.imshow(confusion_matrix(y_test, y_test_pred), cmap='Reds')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Q1_Q2_Q3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
